# =============================================================================
# KEISEI DRL SHOGI - COMPREHENSIVE CONFIGURATION
# =============================================================================
# This configuration file contains all available settings with schema defaults and comprehensive documentation.
# Each section corresponds to a configuration class in keisei/config_schema.py
#
# CONFIGURATION VALIDATION:
# - TrainingConfig: 3 validators (learning_rate > 0, lr_schedule_type validation, lr_schedule_step_on validation)
# - EvaluationConfig: 3 validators (evaluation_interval_timesteps > 0, num_games > 0, max_moves_per_game > 0)
# - ParallelConfig: 2 validators (num_workers > 0, batch_size > 0)
# - Other configs: No validators (validated through Pydantic field constraints)
#
# CROSS-CONFIGURATION DEPENDENCIES:
# - Device coordination: env.device used across training and evaluation
# - Seed management: env.seed provides base for worker seed offsets in parallel mode
# - Move limits: env.max_moves_per_game should align with evaluation.max_moves_per_game
# - Timing alignment: training.steps_per_epoch should divide evenly into evaluation.evaluation_interval_timesteps
# - Logging integration: logging.run_name used by wandb for experiment naming
#
# USAGE PATTERNS:
# - Direct access: config.section.field (e.g., config.training.learning_rate)
# - Parameter extraction: getattr() patterns for optional fields
# - Manager integration: Each config section integrates with specific manager classes
# - Validation flow: Schema validation → field validators → cross-field consistency checks
#
# NEW FEATURES (v2024):
# - MODEL EVOLUTION DISPLAY: Configurable layer filtering in training display
#   * Set display.log_layer_keyword_filters to control which layers are shown
#   * Default: ["stem", "policy_head", "value_head"] for ResNet models
#   * Custom examples: ["attention", "feedforward"] for Transformers
#   * Performance tip: Use 3-10 specific keywords for optimal display speed
#
# - DYNAMIC ARCHITECTURE DIAGRAM: Auto-generated based on actual config
#   * Shows real input channels (from env.input_channels)
#   * Displays actual model type (from training.model_type)
#   * Updates automatically when configuration changes
#
# =============================================================================

# =============================================================================
# ENVIRONMENT CONFIGURATION (EnvConfig)
# =============================================================================
# Controls the training environment, device settings, and game parameters.
# Used by: Environment setup, device management, action space validation
env:
  # Random seed for reproducible experiments across training runs
  seed: 42
  
  # Computation device: "cpu" for CPU-only, "cuda" for GPU acceleration
  device: "cpu"
  
  # Neural network input channels for the observation space
  # Standard Shogi board representation uses 46 channels
  input_channels: 46
  
  # Total number of possible actions in the action space
  # Standard Shogi has 13,527 possible moves considering all piece movements and drops
  num_actions_total: 13527
  
  # Maximum moves per game before declaring a draw
  # Prevents infinite games and controls episode length
  max_moves_per_game: 500
# =============================================================================
# TRAINING CONFIGURATION (TrainingConfig)
# =============================================================================
# Core PPO training parameters, model architecture, and optimization settings.
# Used by: TrainingLoopManager, PPO optimizer, model checkpointing, learning rate scheduling
training:
  # --- Core PPO Hyperparameters ---
  # Initial learning rate for the Adam optimizer
  learning_rate: 0.0003
  
  # Discount factor for future rewards (gamma in RL literature)
  gamma: 0.99
  
  # PPO clipping parameter to prevent large policy updates
  clip_epsilon: 0.2
  
  # Number of PPO optimization epochs per collected experience buffer
  ppo_epochs: 10
  
  # Mini-batch size for PPO gradient updates
  minibatch_size: 64
  
  # Coefficient for value function loss in the combined loss
  value_loss_coeff: 0.5
  
  # Entropy regularization coefficient to encourage exploration
  entropy_coef: 0.01
  
  # Number of environment steps to collect per PPO training epoch
  steps_per_epoch: 2048
  
  # Total number of environment steps for the entire training run
  total_timesteps: 500000
  
  # Save model checkpoint every N timesteps
  checkpoint_interval_timesteps: 10000
  
  # --- Advanced PPO Settings ---
  # Lambda parameter for Generalized Advantage Estimation (GAE)
  lambda_gae: 0.95
  
  # Maximum gradient norm for gradient clipping (prevents exploding gradients)
  gradient_clip_max_norm: 0.5
  
  # L2 regularization weight decay for optimizer
  weight_decay: 0.0
  
  # Enable advantage normalization for improved training stability
  normalize_advantages: true
  
  # Enable value function loss clipping for additional stability
  enable_value_clipping: false
  
  # --- Model Architecture Configuration ---
  # Feature set for observation builder ("core46" for standard 46-channel representation)
  input_features: "core46"
  
  # Model architecture type (currently supports "resnet")
  model_type: "resnet"
  
  # Number of residual blocks in the ResNet tower
  tower_depth: 9
  
  # Number of channels/width in the ResNet tower
  tower_width: 256
  
  # Squeeze-and-Excitation block ratio (0.0 disables SE blocks)
  se_ratio: 0.25
  
  # --- Training Optimization ---
  # Enable mixed-precision training for faster computation and reduced memory
  mixed_precision: false
  
  # Enable DistributedDataParallel for multi-GPU training
  ddp: false
  
  # --- Display and Progress Settings ---
  # Update frequency for expensive UI elements to reduce flicker
  render_every_steps: 1
  
  # Rich Live display refresh rate (updates per second)
  refresh_per_second: 2
  
  # Enable animated spinner in progress display
  enable_spinner: true
  
  # --- Learning Rate Scheduling Configuration ---
  # Type of learning rate scheduler: "linear", "cosine", "exponential", "step", or null to disable
  lr_schedule_type: null
  
  # When to step the scheduler: "epoch" (per PPO epoch) or "update" (per minibatch update)
  lr_schedule_step_on: "epoch"
  
  # Scheduler-specific parameters (see examples below)
  lr_schedule_kwargs: null
  
  # Example scheduler configurations (uncomment one to enable):
  # Linear decay (recommended for PPO):
  # lr_schedule_type: "linear"
  # lr_schedule_kwargs:
  #   final_lr_fraction: 0.1      # End at 10% of initial learning rate
  
  # Cosine annealing:
  # lr_schedule_type: "cosine"
  # lr_schedule_kwargs:
  #   eta_min_fraction: 0.05      # Minimum LR as fraction of initial
  
  # Exponential decay:
  # lr_schedule_type: "exponential"
  # lr_schedule_step_on: "update"
  # lr_schedule_kwargs:
  #   gamma: 0.9995               # Decay factor per step
  
  # Step decay:
  # lr_schedule_type: "step"
  # lr_schedule_kwargs:
  #   step_size: 50               # Decay every 50 epochs/updates
  #   gamma: 0.5                  # Multiply by 0.5 at each step
# =============================================================================
# EVALUATION CONFIGURATION (EvaluationConfig)
# =============================================================================
# Periodic evaluation settings for assessing training progress.
# Used by: PeriodicEvaluationCallback, Evaluator class, performance monitoring
evaluation:
  # Enable periodic evaluation during training (recommended for monitoring progress)
  enable_periodic_evaluation: true
  
  # Run evaluation every N timesteps (should be multiple of steps_per_epoch)
  evaluation_interval_timesteps: 50000
  
  # Number of games to play during each evaluation session
  num_games: 20
  
  # Type of opponent for evaluation games
  # Options: "random" (random moves), "heuristic" (rule-based), or other implemented opponents
  opponent_type: "random"
  
  # Maximum moves per evaluation game (prevents infinite games)
  max_moves_per_game: 500
  
  # File path for evaluation game logs and results
  log_file_path_eval: "eval_log.txt"

  # Enable Weights & Biases logging for evaluation metrics
  wandb_log_eval: false
  elo_registry_path: "elo_ratings.json"
  agent_id: null
  opponent_id: null
  # Number of previous checkpoints to keep for Elo evaluation
  previous_model_pool_size: 5
# =============================================================================
# LOGGING CONFIGURATION (LoggingConfig)
# =============================================================================
# File paths and settings for training logs and model storage.
# Used by: TrainingLoopManager, model checkpointing, run identification
logging:
  # Directory for saving model checkpoints and training artifacts
  # Automatically creates subdirectories for each training run
  model_dir: "models/"
  
  # Main training log file path (includes progress, metrics, and debug info)
  log_file: "logs/training_log.txt"
  
  # Optional custom name for this training run
  # If null, auto-generates name based on timestamp and configuration
  run_name: null
# =============================================================================
# WEIGHTS & BIASES CONFIGURATION (WandBConfig)
# =============================================================================
# Integration settings for Weights & Biases experiment tracking.
# Used by: W&B logging, experiment tracking, model artifact storage, hyperparameter sweeps
wandb:
  # Enable Weights & Biases logging and experiment tracking
  enabled: true
  
  # W&B project name for organizing experiments
  project: "keisei-shogi-rl"
  
  # W&B entity (username or team name) - null uses default account
  entity: null
  
  # Prefix for automatically generated run names
  run_name_prefix: "keisei"
  
  # Enable wandb.watch() to log model gradients and parameters
  watch_model: true
  
  # Frequency for wandb.watch() logging (every N training steps)
  watch_log_freq: 1000
  
  # Type of data to log with wandb.watch(): "gradients", "parameters", or "all"
  watch_log_type: "all"
  
  # Enable logging of model checkpoints as W&B artifacts
  log_model_artifact: false

# =============================================================================
# PARALLEL PROCESSING CONFIGURATION (ParallelConfig)
# =============================================================================
# Multi-worker parallel experience collection for faster training.
# Used by: ParallelManager, worker process coordination, distributed training
parallel:
  # Enable parallel experience collection with multiple worker processes
  enabled: false
  
  # Number of parallel worker processes for experience collection
  # Recommended: number of CPU cores - 1 (leave one for main process)
  num_workers: 4
  
  # Batch size for experience transmission from workers to main process
  # Larger batches reduce communication overhead but increase memory usage
  batch_size: 32
  
  # Steps between model weight synchronization across workers
  # Lower values ensure workers have more up-to-date policy, higher values reduce overhead
  sync_interval: 100
  
  # Enable compression for model weight transmission to reduce bandwidth
  compression_enabled: true
  
  # Timeout for worker communication operations (seconds)
  # Prevents hanging if workers become unresponsive
  timeout_seconds: 10.0
  
  # Maximum size of experience queues between workers and main process
  # Prevents memory overflow if main process falls behind in processing
  max_queue_size: 1000
  
  # Random seed offset for worker processes to ensure diverse experience collection
  # Worker i gets seed = base_seed + (i * worker_seed_offset)
  worker_seed_offset: 1000


# =============================================================================
# DISPLAY CONFIGURATION (DisplayConfig)
# =============================================================================
# Settings controlling optional TUI enhancements.
display:
  enable_board_display: true        # Show ASCII board panel
  enable_trend_visualization: true  # Show sparkline metric trends
  enable_elo_ratings: true          # Display Elo rating information
  enable_enhanced_layout: true      # Use multi-panel dashboard layout
  board_unicode_pieces: true        # Use Unicode pieces for board rendering
  board_highlight_last_move: true   # Highlight the last move made
  sparkline_width: 15               # Width of sparkline graphs
  trend_history_length: 100         # Number of data points to keep
  elo_initial_rating: 1500.0        # Starting Elo rating
  elo_k_factor: 32.0                # K-factor for Elo updates
  dashboard_height_ratio: 2         # Relative height for dashboard section
  progress_bar_height: 4            # Height of progress bar section
  display_moves:  true              # Show expanded move log with delays
  turn_tick: 0.5                    # Delay in seconds between turns
  show_text_moves: true             # Display recent moves under board
  move_list_length: 20              # Number of moves to show
  show_moves_trend: true            # Display moves per game trend
  show_completion_rate: true        # Display games per hour
  show_enhanced_win_rates: true     # Show win/loss/draw breakdown
  show_turns_trend: true            # Display average turns per game
  metrics_window_size: 100          # Rolling window size
  trend_smoothing_factor: 0.1       # Smoothing factor for trends
  metrics_panel_height: 6           # Height of metrics panel
  enable_trendlines: true           # Enable trendline rendering
  
  # =============================================================================
  # MODEL EVOLUTION LAYER DISPLAY CONFIGURATION
  # =============================================================================
  # Controls which neural network layers are displayed in the Model Evolution panel
  # during training. The panel shows weight statistics (mean, std, min, max) and
  # trends for layers matching the specified keywords.
  #
  # USAGE EXAMPLES:
  # Default (ResNet): ["stem", "policy_head", "value_head"]
  # Transformer: ["attention", "feedforward", "embedding", "position"]
  # Custom CNN: ["conv", "fc", "output"]
  # All layers: ["weight"]  # Shows all weight parameters
  # Specific layers: ["layer1", "layer2", "final"]
  #
  # HOW IT WORKS:
  # - Filters model parameters containing ".weight" in the name
  # - Shows layers where ANY keyword appears in the parameter name
  # - Case-sensitive matching (use lowercase for standard PyTorch naming)
  # - Updates in real-time during training with trend indicators
  #
  # PERFORMANCE IMPACT:
  # - More layers = more computation per update
  # - Recommended: 3-10 keywords for optimal performance
  # - Use specific keywords to avoid showing too many layers
  log_layer_keyword_filters:
    - "stem"             # First convolutional layers (ResNet stem)
    - "res_blocks.0"     # ResNet block 0
    - "res_blocks.1"     # ResNet block 1  
    - "res_blocks.2"     # ResNet block 2
    - "res_blocks.3"     # ResNet block 3
    - "policy_head"      # Policy network output layers
    - "value_head"       # Value network output layers


# =============================================================================
# CONFIGURATION EXAMPLES FOR MODEL EVOLUTION DISPLAY
# =============================================================================
# The following examples show how to customize the log_layer_keyword_filters
# for different model architectures and debugging scenarios.
#
# TO USE THESE EXAMPLES:
# 1. Copy the desired filter list
# 2. Replace the log_layer_keyword_filters section above
# 3. Restart training to see the new layer selection
#
# EXAMPLE 1: Transformer/Attention Models
# ----------------------------------------
# display:
#   log_layer_keyword_filters:
#     - "attention"      # Multi-head attention layers
#     - "feedforward"    # Feed-forward network layers
#     - "embedding"      # Token/position embedding layers
#     - "norm"           # Layer normalization
#     - "output"         # Final output layers
#
# EXAMPLE 2: Custom CNN Architecture
# -----------------------------------
# display:
#   log_layer_keyword_filters:
#     - "conv1"          # First convolutional block
#     - "conv2"          # Second convolutional block
#     - "fc"             # Fully connected layers
#     - "classifier"     # Classification head
#     - "regressor"      # Regression head
#
# EXAMPLE 3: Standard ResNet with More Detail
# -----------------------------------
# display:
#   log_layer_keyword_filters:
#     - "stem"           # Initial convolution
#     - "layer1"         # ResNet stage 1
#     - "layer2"         # ResNet stage 2
#     - "layer3"         # ResNet stage 3
#     - "layer4"         # ResNet stage 4
#     - "policy_head"    # Policy output
#     - "value_head"     # Value output
#
# EXAMPLE 4: Debugging Mode (Show All Layers)
# ---------------------------------------------
# display:
#   log_layer_keyword_filters:
#     - "weight"         # Shows ALL weight parameters
#
# EXAMPLE 5: Minimal Mode (Performance Focused)
# -----------------------------------------------
# display:
#   log_layer_keyword_filters:
#     - "policy_head"    # Only policy output
#     - "value_head"     # Only value output
#
# EXAMPLE 6: EfficientNet/MobileNet Style
# ----------------------------------------
# display:
#   log_layer_keyword_filters:
#     - "features"       # Feature extraction layers
#     - "se"             # Squeeze-and-excitation blocks
#     - "classifier"     # Classification head
#     - "head"           # Various head layers
#
# PERFORMANCE GUIDELINES:
# - Fewer keywords = faster updates, less memory usage
# - More keywords = detailed monitoring, slower updates
# - Optimal range: 3-10 keywords
# - Use specific names to avoid catching too many layers
# - Test different configurations to find the right balance
#
# TROUBLESHOOTING:
# - No layers shown? Check that your keywords match actual layer names
# - Too many layers? Use more specific keywords
# - Slow performance? Reduce the number of keywords
# - Missing important layers? Add broader keywords like "head" or "output"
