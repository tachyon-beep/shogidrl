# W&B Hyperparameter Sweep Configuration for Keisei Shogi
# This file defines the hyperparameter search space and strategy for automated tuning

# Sweep configuration
program: keisei/training/train_wandb_sweep.py
method: bayes  # Options: grid, random, bayes
metric:
  name: eval/final_win_rate
  goal: maximize
  
# Early termination to save compute
early_terminate:
  type: hyperband
  min_iter: 2
  
# Hyperparameter search space
parameters:
  # Core PPO hyperparameters
  learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-2
    
  gamma:
    distribution: uniform
    min: 0.95
    max: 0.999
    
  clip_epsilon:
    distribution: uniform
    min: 0.1
    max: 0.3
    
  ppo_epochs:
    values: [2, 4, 6, 8, 10]
    
  minibatch_size:
    values: [32, 64, 128, 256]
    
  value_loss_coeff:
    distribution: uniform
    min: 0.1
    max: 1.0
    
  entropy_coef:
    distribution: log_uniform_values
    min: 1e-4
    max: 1e-1
    
  # Model architecture parameters
  tower_depth:
    values: [6, 9, 12, 15]
    
  tower_width:
    values: [128, 256, 512]
    
  se_ratio:
    distribution: uniform
    min: 0.0
    max: 0.5
    
  # Training parameters
  steps_per_epoch:
    values: [1024, 2048, 4096]
    
  gradient_clip_max_norm:
    distribution: uniform
    min: 0.1
    max: 2.0
    
  lambda_gae:
    distribution: uniform
    min: 0.9
    max: 0.99

# Fixed parameters (not swept)
command:
  - ${program}
  - --config=default_config.yaml
  - --total-timesteps=50000  # Shorter runs for sweep efficiency
  - --wandb-enabled  # Ensure W&B logging is enabled
  - ${args}  # This will be replaced by W&B agent with sweep parameters
