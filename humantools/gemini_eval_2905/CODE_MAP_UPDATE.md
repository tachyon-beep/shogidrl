# Shogi DRL / Keisei Code Map – rev 29 May 2025 (Updated with Review Findings)

*(Words of Estimative Probability appear in the “Accuracy” column.)*

---

| #     | Sub-system                      | Sections & Purpose                                                                                                                                      | Core / Supporting Files                                                                                                  | Reviewer Watch-list (issues & risks)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Accuracy       |
|:-----|:--------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------|
| **1** | **Configuration & Overrides**   | Pydantic schema; YAML/JSON/CLI/env loaders.<br/>Consumers: `training` & `evaluation` CLIs, `Trainer`, `SessionManager`, `ModelManager`, `EnvManager`. | `config_schema.py`; `utils/utils.py` (`load_config`, `FLAT_KEY_TO_NESTED`); `training/train*.py`; `evaluation/evaluate.py`. | • `num_actions_total` vs `PolicyOutputMapper` validated by `EnvManager`.<br/>• ***Pydantic `device` field in `EnvConfig` needs a validator (e.g., `Literal['cpu', 'cuda']` or custom validator to check `cuda:0` vs `cuda:O` etc.).***<br/>• Complex override precedence in `load_config` – document plainly.<br/>• Keep `default_config.yaml` ↔ `config_schema.py` aligned.<br/>• ***Redundant CLI argument parsing and W&B sweep config logic between `train.py` and `train_wandb_sweep.py` – centralise.*** | Almost Certain |
| **2** | **Shogi Engine Core**           | `shogi_core_definitions.py`, `shogi_game.py`, rule logic, move exec, I/O & features (duplication noted). *SFEN/KIF support added to `shogi_game.py` and `shogi_game_io.py`.* | All `shogi/*` modules shown.                                                                                            | • `copy.deepcopy` & list scans may bottleneck.<br/>• Replace `DEBUG_*` prints with `logging`.<br/>• Stress-test `uchi-fu-zume`, `sennichite`, mandatory promotion.<br/>• Unit-test SFEN & KIF conversions (KIF basic, SFEN more complex).<br/>• ***Observation builder is in `shogi_game_io.generate_neural_network_observation` and `shogi.features.build_core46`. Consolidate; `features.py` seems the more appropriate location with `FeatureSpec`.***<br/>• `board_history` for repetition uses tuple hashes; if game states are large/complex, memory could be a concern for very long games (consider ring buffer or more compact hash).<br/>• ***SFEN parsing in `ShogiGame.from_sfen` has fragile logic for `+` (promotion) and could be more robust (e.g. regex or stateful char processing).*** | Highly Likely |
| **3** | **Policy ↔ Move Mapping**       | Action-index ⇆ `MoveTuple` ⇆ USI.                                                                                                                       | `utils.utils.PolicyOutputMapper`.                                                                                         | • 13 527-action space must equal `EnvConfig.num_actions_total` (checked by `EnvManager`).<br/>• Heuristic fallback for `PieceType` identity in `shogi_move_to_policy_index` still brittle; ensure `MoveTuple` instances are canonical.<br/>• Consider explicit mapping versioning in checkpoints.<br/>• ***Robustly test handling of all-illegal moves (masked logits → `-∞` → softmax NaN) in `ActorCritic.get_action_and_value` and `evaluate_actions`. Current fallback to uniform is a temporary fix.***<br/>• ***`PolicyOutputMapper.get_legal_mask`: `ValueError` pass on `shogi_move_to_policy_index` can silently ignore valid game moves if mapper has issues, leading to incorrect masks.*** | Almost Certain |
| **4** | **Neural-Net Models**           | Simple `core.ActorCritic` vs ResNet-tower `training.models.*`; factory picks model. *`resnet_tower.py` has `SqueezeExcitation` and `ResidualBlock`.* | `core/neural_network.py`; `training/models/*`.                                                                           | • ***Code duplication: `get_action_and_value` and `evaluate_actions` are nearly identical in `core/neural_network.py` (ActorCritic) and `training/models/resnet_tower.py`. Refactor into a base class or utility.***<br/>• `PPOAgent` defaults to `core.ActorCritic`, but `Trainer` (via `ModelManager`) injects `training.models.resnet_tower.ActorCriticResTower`. Ensure consistent API or make `ModelManager` always provide the model to `PPOAgent`.<br/>• `utils/checkpoint.py`'s `load_checkpoint_with_padding` only handles `stem.weight`; generalise or document limitations for other layer changes.<br/>• BatchNorm stats portability (ensure `eval()` mode used correctly during evaluation/inference).<br/>• Warn if “dummy” model selected in production runs.<br/>• ***`training/models/resnet_tower.py` `ResidualBlock.forward`: `if self.se:` check should encompass the call `out = self.se(out)` to prevent `TypeError` if `self.se` is `None`.*** | Highly Likely |
| **5** | **Reinforcement-Learning Core** | PPO agent, experience buffer.                                                                                                                            | `core/ppo_agent.py`; `core/experience_buffer.py`.                                                                         | • `ExperienceBuffer` stores a list of `torch.Tensor` for `obs` and `legal_masks`. For large buffers and many steps, this could be memory intensive, especially `legal_masks` (13k bools/step). Consider alternatives like pre-allocated tensors, memory-mapped files for huge buffers, or regenerating masks if computationally cheap.<br/>• `ExperienceBuffer.get_batch()`: Runtime error during tensor stacking (e.g. `obs_tensor = torch.stack(self.obs...`) returns an empty dict. `PPOAgent.learn()` checks for this empty dict and returns zeroed metrics, potentially masking the underlying error and leading to silent training failure/stagnation. Raise an error from `get_batch` or handle more explicitly in `learn`.<br/>• Hyper-parameter sanity (clip ε, GAE λ, grad-clip values are present).<br/>• ***`PPOAgent.load_model` error handling: `KeyError` for missing dict keys can occur before `.get()` calls if checkpoint is malformed.*** | Highly Likely |
| **6** | **Environment & Managers**      | *`EnvManager` bootstraps `ShogiGame`, validates action-space, applies seeding; `SessionManager` creates dirs, run-names, W&B; `StepManager` owns per-step orchestration & demo-mode. `ModelManager` handles model lifecycle.* | `training/env_manager.py`; `training/session_manager.py`; `training/step_manager.py`; `training/model_manager.py`.         | • Action-space mismatch raises early fatal error in `EnvManager` – good.<br/>• `StepManager` demo-mode `time.sleep()` is fine for demos; ensure not accidentally enabled for performance runs.<br/>• `SessionManager` `generate_run_name` uses timestamp; collisions are unlikely but possible in very high-frequency automated runs (e.g. CI). Consider adding a random suffix or process ID.<br/>• ***`ModelManager._setup_mixed_precision`: Logs warning if CUDA not available but mixed precision requested; ensure this is visible.*** | Likely         |
| **7** | **Training Orchestration & UI** | Trainer loop, callbacks, Rich TUI. *`Trainer` class orchestrates the managers.*                                                                          | `training/trainer.py`; `training/display.py`; `training/callbacks.py`.                                                    | • ***CRITICAL RESUME GAP: `ModelManager.handle_checkpoint_resume()` calls `agent.load_model()`, which returns checkpoint data (timestep, episode counts, wins/draws). However, `Trainer._handle_checkpoint_resume()` correctly updates `self.global_timestep`, etc., from this data. The initial concern about `Trainer` not copying them was incorrect; it does. Ensure this path is well-tested.***<br/>• Periodic evaluation callback (`EvaluationCallback`) in `training/callbacks.py` blocks the main training loop. For long evaluations, consider running in a separate thread/process or making it asynchronous.<br/>• High-frequency `log_file.flush()` in `TrainingLogger` can degrade performance on network filesystems; buffer writes or flush less frequently.<br/>• ***`Trainer.log_both` uses `wandb.log` without explicit step; W&B might auto-increment but verify it aligns with `global_timestep`. It does seem to pass `step=self.global_timestep`.*** | Highly Likely |
| **8** | **Evaluation Pipeline**         | Evaluator wrapper, core loop, CLI. *`Evaluator` class in `evaluate.py` encapsulates setup and loop.*                                                    | `evaluation/evaluate.py`; `evaluation/loop.py`; `utils/agent_loading.py`.                                                 | • ***CRITICAL EVALUATION FLAW: `evaluation/loop.py`'s `run_evaluation_loop` passes `legal_mask = torch.ones(...)` to agent/opponent. This means the agent is NOT being tested on its ability to select among actual legal moves based on policy output. It picks from its entire action space. This significantly undermines evaluation validity. Must use proper `PolicyOutputMapper.get_legal_mask()`.***<br/>• W&B init (`wandb.init`) in `evaluation/evaluate.py` is guarded by `try-except Exception`, which can mask issues. Log specific exceptions.<br/>• `Evaluator.__init__` has many parameters; consider an `EvaluationRunConfig` Pydantic model.<br/>• `main_cli` in `evaluate.py` creates a `PolicyOutputMapper()` but doesn't seem to configure it beyond defaults; ensure this is adequate for loaded agents. | Highly Likely |
| **9** | **Logging / Checkpoint / W&B**  | Plain + Rich loggers, checkpoint save/load & migration, W&B artefacts. *`SessionManager` handles W&B session init; `ModelManager` handles W&B artifact creation for models.* | `utils/utils.TrainingLogger`/`EvaluationLogger`; `utils/checkpoint.py`; `training/model_manager.py`; `training/session_manager.py`; `training/utils.py` (`find_latest_checkpoint`, `serialize_config`, `setup_directories`, `setup_wandb`). | • Still mixing `print` & `logging` in various places (e.g. `PPOAgent.load_model` prints to `sys.stderr`). Standardise on the `logging` module or the provided loggers.<br/>• `utils/checkpoint.py` only pads/truncates the first conv layer (`stem.weight`) for input channels. More comprehensive migration logic might be needed for other architectural changes.<br/>• File I/O: Consider buffered writes (e.g., `open` with `buffering=-1` or larger) for log files, especially if on network filesystems, though `flush()` is also used.<br/>• ***`training/utils.py::serialize_config`: Recursive `json.loads(serialize_config(v))` for non-Pydantic `__dict__` objects could be problematic if `serialize_config` doesn't always return valid JSON strings for such cases.*** | Almost Certain |
| **10**| **Entry-points & Packaging**    | CLIs, `__init__.py` re-exports, package surface. *Root `__init__.py` re-exports core Shogi types and evaluation entry point.*                            | root & subpackage `__init__.py`; `training/train.py`, `training/train_wandb_sweep.py`, `evaluation/evaluate.py` (CLIs).   | • Static cycle check (e.g. `pylint --enable=cyclic-import`) still advisable, though `TYPE_CHECKING` blocks help.<br/>• Ensure top-level re-exports in `__init__.py` files don't trigger heavy imports unnecessarily at package load time (seems okay currently).<br/>• ***`sys.path.insert` in `shogi/shogi_game_io.py` is bad practice; manage paths via project structure/PYTHONPATH.*** | Likely         |

---

### New/expanded focus areas from previous review, now integrated above:

- **ExperienceBuffer silent-failure path**: Integrated into Row 5. `PPOAgent.learn()` now explicitly checks for empty `batch_data`.
- **Trainer ↔ ModelManager resume handshake**: Integrated into Row 7; this seems to be handled correctly by the `Trainer` using data returned by `agent.load_model()`.
- **EnvManager device-string validation**: Suggested for Row 1.
- **Repetition (`board_history`) GC**: Integrated into Row 2; suggestion for ring buffer if memory becomes an issue.
