<?xml version="1.0" encoding="UTF-8"?>
<codebase>
  <file path="evaluate.py"><![CDATA[
# Thin wrapper for keisei.evaluation.evaluate and core utilities for test patching/mocking
from keisei.evaluation import evaluate as _evaluate_mod
from keisei.evaluation.evaluate import main_cli as evaluate_main

# Assign API symbols for patching/mocking in tests
load_evaluation_agent = _evaluate_mod.load_evaluation_agent
EvaluationLogger = _evaluate_mod.EvaluationLogger
Evaluator = _evaluate_mod.Evaluator
run_evaluation_loop = _evaluate_mod.run_evaluation_loop
initialize_opponent = _evaluate_mod.initialize_opponent
execute_full_evaluation_run = _evaluate_mod.execute_full_evaluation_run

# CLI main
evaluate_main_cli = _evaluate_mod.main_cli

if __name__ == "__main__":
    evaluate_main()

]]></file>
  <file path="config_schema.py"><![CDATA[
"""
Pydantic configuration schema for Keisei DRL Shogi Client.
Defines all configuration sections and their defaults.
"""

from typing import Literal, Optional

from pydantic import BaseModel, Field, validator


class EnvConfig(BaseModel):
    device: str = Field("cpu", description="Device to use: 'cpu' or 'cuda'.")
    input_channels: int = Field(
        46, description="Number of input channels for the neural network."
    )
    num_actions_total: int = Field(
        13527, description="Total number of possible actions."
    )
    seed: int = Field(42, description="Random seed for reproducibility.")


class TrainingConfig(BaseModel):
    total_timesteps: int = Field(
        500_000, description="Total number of environment steps for training."
    )
    steps_per_epoch: int = Field(2048, description="Steps per PPO buffer/epoch.")
    ppo_epochs: int = Field(10, description="Number of PPO update epochs per buffer.")
    minibatch_size: int = Field(64, description="Minibatch size for PPO updates.")
    learning_rate: float = Field(3e-4, description="Learning rate for optimizer.")
    gamma: float = Field(0.99, description="Discount factor.")
    clip_epsilon: float = Field(0.2, description="PPO clip epsilon.")
    value_loss_coeff: float = Field(0.5, description="Value loss coefficient.")
    entropy_coef: float = Field(0.01, description="Entropy regularization coefficient.")
    render_every_steps: int = Field(
        1, description="Update expensive display elements (metrics, logs) every N steps to reduce flicker."
    )
    refresh_per_second: int = Field(
        4, description="Rich Live refresh rate per second."
    )
    enable_spinner: bool = Field(
        True, description="Enable spinner column in progress bar (looks cool!)."
    )
    # --- Model/feature config additions ---
    input_features: str = Field("core46", description="Feature set for observation builder (e.g. 'core46', 'core46+all').")
    tower_depth: int = Field(9, description="Number of residual blocks in ResNet tower.")
    tower_width: int = Field(256, description="Width (channels) of ResNet tower.")
    se_ratio: float = Field(0.25, description="SE block squeeze ratio (0 disables SE blocks).")
    model_type: str = Field("resnet", description="Model type to use (e.g. 'resnet').")
    mixed_precision: bool = Field(False, description="Enable mixed-precision training.")
    ddp: bool = Field(False, description="Enable DistributedDataParallel training.")
    gradient_clip_max_norm: float = Field(0.5, description="Maximum norm for gradient clipping.")
    lambda_gae: float = Field(0.95, description="Lambda for Generalized Advantage Estimation (GAE).")
    checkpoint_interval_timesteps: int = Field(10000, description="Save a model checkpoint every N timesteps.")
    evaluation_interval_timesteps: int = Field(50000, description="Run evaluation every N timesteps.")

    @validator("learning_rate")
    # pylint: disable=no-self-argument
    def lr_positive(cls, v):
        if v <= 0:
            raise ValueError("learning_rate must be positive")
        return v


class EvaluationConfig(BaseModel):
    num_games: int = Field(20, description="Number of games to play during evaluation.")
    opponent_type: str = Field(
        "random", description="Type of opponent: 'random', 'heuristic', etc."
    )
    evaluation_interval_timesteps: int = Field(50000, description="Run evaluation every N timesteps.")


class LoggingConfig(BaseModel):
    log_file: str = Field(
        "logs/training_log.txt", description="Path for the main training log."
    )
    model_dir: str = Field(
        "models/", description="Directory to save model checkpoints."
    )
    run_name: Optional[str] = Field(
        None, description="Optional name for this run (overrides auto-generated name if set)."
    )


class WandBConfig(BaseModel):
    enabled: bool = Field(True, description="Enable Weights & Biases logging.")
    project: Optional[str] = Field("keisei-shogi-rl", description="W&B project name.")
    entity: Optional[str] = Field(None, description="W&B entity (username or team).")
    run_name_prefix: Optional[str] = Field("keisei", description="Prefix for W&B run names.")
    watch_model: bool = Field(True, description="Use wandb.watch() to log model gradients and parameters.")
    watch_log_freq: int = Field(1000, description="Frequency for wandb.watch() logging.")
    watch_log_type: Literal["gradients", "parameters", "all"] = Field(
        "all", description="Type of data to log with wandb.watch()."
    )


class DemoConfig(BaseModel):
    enable_demo_mode: bool = Field(
        False,
        description="If True, enables demo mode with per-move delay and extra logging.",
    )
    demo_mode_delay: float = Field(
        0.5, description="Delay in seconds between moves in demo mode."
    )


class AppConfig(BaseModel):
    env: EnvConfig
    training: TrainingConfig
    evaluation: EvaluationConfig
    logging: LoggingConfig
    wandb: WandBConfig
    demo: DemoConfig

    class Config:
        extra = "forbid"  # Disallow unknown fields for strict validation

]]></file>
  <file path="__init__.py"><![CDATA[
"""
DRL Shogi Client - A reinforcement learning agent for Japanese chess (Shogi).

This package contains:
- Shogi game engine and rules
- Reinforcement learning agents
- Neural network models
- Training utilities
"""

from .evaluation.evaluate import execute_full_evaluation_run

# Re-export the main components for easy access
from .shogi.shogi_core_definitions import Color, MoveTuple, Piece, PieceType
from .shogi.shogi_game import ShogiGame

__all__ = [
    # Shogi core types
    "Color",
    "PieceType",
    "Piece",
    "MoveTuple",
    "ShogiGame",
    "execute_full_evaluation_run",
    # Let the other modules be imported explicitly
]

]]></file>
  <file path="evaluation/evaluate.py"><![CDATA[
"""
evaluate.py: Main script for evaluating PPO Shogi agents.
"""

import os
import random
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union

import numpy as np
import torch
from dotenv import load_dotenv

import wandb
from keisei.config_schema import (
    AppConfig,
    DemoConfig,
    EnvConfig,
    EvaluationConfig,
    LoggingConfig,
    TrainingConfig,
    WandBConfig,
)
from keisei.core.ppo_agent import PPOAgent
from keisei.shogi import shogi_game_io  # For observations
from keisei.shogi.shogi_core_definitions import (  # Added PieceType
    Color,
    MoveTuple,
    PieceType,
)
from keisei.shogi.shogi_game import ShogiGame
from keisei.utils import BaseOpponent, EvaluationLogger, PolicyOutputMapper

if TYPE_CHECKING:
    pass  # torch already imported above

load_dotenv()  # Load environment variables from .env file


class SimpleRandomOpponent(BaseOpponent):
    """An opponent that selects a random legal move."""

    def __init__(self, name: str = "SimpleRandomOpponent"):
        super().__init__(name)

    def select_move(self, game_instance: ShogiGame) -> MoveTuple:
        """Selects a random move from the list of legal moves."""
        legal_moves = game_instance.get_legal_moves()  # Removed current_player argument
        if not legal_moves:
            # This case should ideally be handled by the game loop checking for game_over
            raise ValueError(
                "No legal moves available for SimpleRandomOpponent, game should be over."
            )
        return random.choice(legal_moves)


class SimpleHeuristicOpponent(BaseOpponent):
    """An opponent that uses simple heuristics to select a move."""

    def __init__(self, name: str = "SimpleHeuristicOpponent"):
        super().__init__(name)

    def select_move(self, game_instance: ShogiGame) -> MoveTuple:
        """Selects a move based on simple heuristics."""
        legal_moves = game_instance.get_legal_moves()
        if not legal_moves:
            raise ValueError(
                "No legal moves available for SimpleHeuristicOpponent, game should be over."
            )

        capturing_moves: List[MoveTuple] = []
        non_promoting_pawn_moves: List[MoveTuple] = []
        other_moves: List[MoveTuple] = []

        for move_tuple in legal_moves:
            is_capture = False
            is_pawn_move_no_promo = False

            # Check if it's a BoardMoveTuple: (int, int, int, int, bool)
            if (
                isinstance(move_tuple[0], int)
                and isinstance(move_tuple[1], int)
                and isinstance(move_tuple[2], int)
                and isinstance(move_tuple[3], int)
                and isinstance(move_tuple[4], bool)
            ):

                from_r: int = move_tuple[0]
                from_c: int = move_tuple[1]
                to_r: int = move_tuple[2]
                to_c: int = move_tuple[3]
                promote: bool = move_tuple[4]

                # Heuristic 1: Check for capturing moves.
                destination_piece = game_instance.board[to_r][to_c]
                if (
                    destination_piece is not None
                    and destination_piece.color != game_instance.current_player
                ):
                    is_capture = True  # MODIFIED: Set is_capture to True

                # Heuristic 2: Check for non-promoting pawn moves (only if not a capture).
                if not is_capture:
                    source_piece = game_instance.board[from_r][
                        from_c
                    ]  # MODIFIED: Get source piece
                    if (
                        source_piece
                        and source_piece.type == PieceType.PAWN
                        and not promote
                    ):  # MODIFIED: Check if pawn and not promoting
                        is_pawn_move_no_promo = (
                            True  # MODIFIED: Set is_pawn_move_no_promo to True
                        )
            # Drop moves (Tuple[None, None, int, int, PieceType]) and other types of moves
            # will not pass the isinstance checks above.

            if is_capture:
                capturing_moves.append(move_tuple)
            if is_pawn_move_no_promo:  # Changed to if
                non_promoting_pawn_moves.append(move_tuple)
            else:
                other_moves.append(move_tuple)

        if capturing_moves:
            return random.choice(capturing_moves)
        if non_promoting_pawn_moves:  # Changed to if
            return random.choice(non_promoting_pawn_moves)
        if other_moves:
            return random.choice(other_moves)

        # Fallback, should ideally not be reached if legal_moves is not empty.
        return random.choice(legal_moves)


def load_evaluation_agent(
    checkpoint_path: str,
    device_str: str,
    policy_mapper: PolicyOutputMapper,
    input_channels: int,
    # Add input_features as an optional parameter with a default
    input_features: Optional[str] = "core46", 
) -> PPOAgent:
    """Loads a PPOAgent from a checkpoint for evaluation. Raises FileNotFoundError if checkpoint does not exist."""
    if not os.path.isfile(checkpoint_path):
        print(f"Error: Checkpoint file {checkpoint_path} not found.")
        raise FileNotFoundError(f"Checkpoint file {checkpoint_path} not found.")
    # Use minimal config for evaluation
    config = AppConfig(
        env=EnvConfig(
            device=device_str,
            input_channels=input_channels,
            num_actions_total=policy_mapper.get_total_actions(),
            seed=42,  # Default seed for eval agent loading
        ),
        training=TrainingConfig(  # Dummy TrainingConfig for PPOAgent
            total_timesteps=1,      # Placeholder
            steps_per_epoch=1,      # Placeholder
            ppo_epochs=1,           # Placeholder
            minibatch_size=1,       # Placeholder
            learning_rate=1e-4,     # Placeholder
            gamma=0.99,             # Placeholder
            clip_epsilon=0.2,       # Placeholder
            value_loss_coeff=0.5,   # Placeholder
            entropy_coef=0.01,      # Placeholder
            input_features=input_features if input_features else "core46", # Use provided or default
            model_type="resnet",    # Default, actual arch baked into model
            mixed_precision=False,  # Not relevant for eval agent loading
            ddp=False,              # Not relevant for eval agent loading
            gradient_clip_max_norm=0.5, # Placeholder
            lambda_gae=0.95,            # Placeholder
            checkpoint_interval_timesteps=10000, # Placeholder
            evaluation_interval_timesteps=50000, # Placeholder for TrainingConfig's own field
        ),
        evaluation=EvaluationConfig(
            num_games=1, 
            opponent_type="random", 
            evaluation_interval_timesteps=50000 # Explicitly add, though it has a default
        ),  # Minimal
        logging=LoggingConfig(log_file="/tmp/eval.log", model_dir="/tmp/"),
        wandb=WandBConfig(
            enabled=False, 
            project="eval", 
            entity=None,
            run_name_prefix="eval-run", # Added
            watch_model=False,          # Added
            watch_log_freq=1000,        # Added
            watch_log_type="all"        # Added
        ),
        demo=DemoConfig(enable_demo_mode=False, demo_mode_delay=0.0),
    )
    agent = PPOAgent(config=config, device=torch.device(device_str))
    agent.load_model(checkpoint_path)
    agent.model.eval()  # Set the model to evaluation mode
    print(f"Loaded agent from {checkpoint_path} on device {device_str} for evaluation.")
    return agent


def initialize_opponent(
    opponent_type: str,
    opponent_path: Optional[str],
    device_str: str,
    policy_mapper: PolicyOutputMapper,
    input_channels: int,
) -> Union[PPOAgent, BaseOpponent]:  # Adjusted return type
    """Initializes the opponent based on type."""
    if opponent_type == "random":
        print("Initializing SimpleRandomOpponent.")
        return SimpleRandomOpponent()
    elif opponent_type == "heuristic":
        print("Initializing SimpleHeuristicOpponent.")
        return SimpleHeuristicOpponent()
    elif opponent_type == "ppo":
        if not opponent_path:
            raise ValueError("Opponent path must be provided for PPO opponent type.")
        print(f"Initializing PPO opponent from {opponent_path}.")
        return load_evaluation_agent(
            opponent_path, device_str, policy_mapper, input_channels
        )
    else:
        raise ValueError(f"Unknown opponent type: {opponent_type}")


def run_evaluation_loop(
    agent_to_eval: PPOAgent,
    opponent: Union[PPOAgent, BaseOpponent],  # Adjusted opponent type
    num_games: int,
    logger: EvaluationLogger,
    policy_mapper: PolicyOutputMapper,
    max_moves_per_game: int,
    device_str: str,
    wandb_enabled: bool = False,  # Added for W&B
) -> dict:
    """Runs the evaluation loop for a set number of games."""
    wins = 0
    losses = 0
    draws = 0
    total_game_length = 0
    device = torch.device(device_str)

    # Determine opponent name for logging
    current_opponent_name = (
        opponent.name
        if isinstance(opponent, BaseOpponent)
        else opponent.__class__.__name__
    )
    logger.log(  # MODIFIED: Changed to logger.log
        f"Starting evaluation: {agent_to_eval.name} vs {current_opponent_name}"
    )

    for game_num in range(1, num_games + 1):
        game = ShogiGame(max_moves_per_game=max_moves_per_game)
        # Alternate starting player: agent_to_eval is Black (Sente) in odd games, White (Gote) in even games
        agent_is_black = game_num % 2 == 1

        # Determine who is playing which color for this game
        black_player = agent_to_eval if agent_is_black else opponent
        white_player = opponent if agent_is_black else agent_to_eval

        # Corrected log message format
        logger.log(  # MODIFIED: Changed to logger.log
            f"Starting Game {game_num}/{num_games}. "
            f"Agent to eval ({agent_to_eval.name}) is {'Black' if agent_is_black else 'White'}. "
            f"Opponent ({current_opponent_name}) is {'White' if agent_is_black else 'Black'}."
        )

        while not game.game_over:
            # Determine whose turn it is based on game.current_player
            active_agent = (
                black_player if game.current_player == Color.BLACK else white_player
            )

            legal_moves = game.get_legal_moves()
            if not legal_moves:
                break

            selected_move: Optional[MoveTuple] = None
            if isinstance(active_agent, PPOAgent):
                obs_np = shogi_game_io.generate_neural_network_observation(game)
                # obs_tensor = torch.tensor( # This variable was unused.
                #     obs_np, dtype=torch.float32, device=device
                # ).unsqueeze(0)
                legal_mask = policy_mapper.get_legal_mask(legal_moves, device)

                if not legal_mask.any() and legal_moves:
                    logger.log(  # MODIFIED: Changed to logger.log
                        f"Error: Game {game_num}, Move {game.move_count + 1}: "
                        f"Agent {active_agent.name} ({game.current_player.name}) has legal moves, "
                        f"but legal_mask is all False. Legal moves: {legal_moves}. "
                        f"This indicates an issue with PolicyOutputMapper or move generation."
                    )
                    # PPOAgent.select_action should handle this.
                    selected_shogi_move = active_agent.select_action(
                        obs_np, legal_mask, is_training=False
                    )[0]
                    selected_move = (
                        selected_shogi_move  # MODIFIED: Assign to selected_move
                    )
                else:
                    selected_shogi_move = active_agent.select_action(
                        obs_np, legal_mask, is_training=False
                    )[0]
                    selected_move = (
                        selected_shogi_move  # MODIFIED: Assign to selected_move
                    )
            elif isinstance(
                active_agent, BaseOpponent
            ):  # Opponent is a BaseOpponent (Random, Heuristic)
                selected_move = active_agent.select_move(game)
            else:
                # This case should not be reached if opponent types are correctly handled
                logger.log(
                    f"CRITICAL: Unsupported agent type for active_agent: {type(active_agent)}"
                )  # MODIFIED: Added log and changed to raise TypeError
                raise TypeError(
                    f"Unsupported agent type for active_agent: {type(active_agent)}"
                )

            if selected_move is None:
                logger.log(  # MODIFIED: Changed to logger.log
                    f"Error: Game {game_num}, Move {game.move_count + 1}: Active agent {active_agent.name} failed to select a move despite legal moves being available."
                )
                # Decide how to handle this: break, assign loss, etc. For now, break.
                break

            game.make_move(selected_move)
            # logger.log_custom_message(f"Game {game_num}, Move {game.move_count}: {active_agent.name} ({game.current_player.name}) played {selected_move}")

        # Game ended
        game_length = game.move_count
        total_game_length += game_length
        winner = game.winner

        outcome_str = "Draw"
        if winner is not None:
            if (winner == Color.BLACK and agent_is_black) or (
                winner == Color.WHITE and not agent_is_black
            ):
                wins += 1
                outcome_str = f"{agent_to_eval.name} (Agent) wins"
            else:
                losses += 1
                outcome_str = f"{current_opponent_name} (Opponent) wins"
        else:  # Draw
            draws += 1

        # Log main evaluation results
        # MODIFIED: Changed to logger.log and formatted the message
        logger.log(
            f"Game {game_num} Result: Opponent: {current_opponent_name}, "
            f"WinRate(cum): {wins / game_num if game_num > 0 else 0:.2f}, "
            f"AvgGameLen(cum): {(total_game_length / game_num if game_num > 0 else 0):.1f}, "
            f"Outcome: {outcome_str}"
        )
        # Log additional custom metrics for this game
        logger.log(  # MODIFIED: Changed to logger.log
            f"Game {game_num} Details: Length: {game_length}, Outcome: {outcome_str}, Agent Eval Color: {'Black' if agent_is_black else 'White'}"
        )
        logger.log(  # MODIFIED: Changed to logger.log
            f"Game {game_num} ended. Winner: {winner if winner else 'Draw'}"
        )

    avg_game_length = total_game_length / num_games if num_games > 0 else 0
    win_rate = wins / num_games if num_games > 0 else 0
    loss_rate = losses / num_games if num_games > 0 else 0
    draw_rate = draws / num_games if num_games > 0 else 0

    results = {
        "num_games": num_games,
        "wins": wins,
        "losses": losses,
        "draws": draws,
        "win_rate": win_rate,
        "loss_rate": loss_rate,
        "draw_rate": draw_rate,
        "avg_game_length": avg_game_length,
        "opponent_name": current_opponent_name,
        "agent_name": agent_to_eval.name,
    }

    logger.log(
        f"Evaluation finished. Results: {results}"
    )  # MODIFIED: Changed to logger.log
    if wandb_enabled:
        wandb.log(
            {
                "eval/total_games": num_games,
                "eval/wins": wins,
                "eval/losses": losses,
                "eval/draws": draws,
                "eval/win_rate": win_rate,
                "eval/loss_rate": loss_rate,
                "eval/draw_rate": draw_rate,
                "eval/avg_game_length": avg_game_length,
                # Log opponent name if needed, though it's in config
            }
        )

    return results


class Evaluator:
    """
    Evaluator class encapsulates the evaluation logic for PPO Shogi agents.
    It manages agent/opponent loading, logging, W&B integration, and runs the evaluation loop.
    """

    def __init__(
        self,
        agent_checkpoint_path: str,
        opponent_type: str,
        opponent_checkpoint_path: Optional[str],
        num_games: int,
        max_moves_per_game: int,
        device_str: str,
        log_file_path_eval: str,
        policy_mapper: PolicyOutputMapper,
        seed: Optional[int] = None,
        wandb_log_eval: bool = False,
        wandb_project_eval: Optional[str] = None,
        wandb_entity_eval: Optional[str] = None,
        wandb_run_name_eval: Optional[str] = None,
        logger_also_stdout: bool = True,
        wandb_extra_config: Optional[dict] = None,
        wandb_reinit: Optional[bool] = None,
        wandb_group: Optional[str] = None,
    ):
        """
        Initialize the Evaluator with all configuration and dependencies.
        """
        self.agent_checkpoint_path = agent_checkpoint_path
        self.opponent_type = opponent_type
        self.opponent_checkpoint_path = opponent_checkpoint_path
        self.num_games = num_games
        self.max_moves_per_game = max_moves_per_game
        self.device_str = device_str
        self.log_file_path_eval = log_file_path_eval
        self.policy_mapper = policy_mapper
        self.seed = seed
        self.wandb_log_eval = wandb_log_eval
        self.wandb_project_eval = wandb_project_eval
        self.wandb_entity_eval = wandb_entity_eval
        self.wandb_run_name_eval = wandb_run_name_eval
        self.logger_also_stdout = logger_also_stdout
        self.wandb_extra_config = wandb_extra_config
        self.wandb_reinit = wandb_reinit
        self.wandb_group = wandb_group
        self._wandb_active: bool = False
        self._wandb_run: Optional[Any] = None
        self._logger: Optional[EvaluationLogger] = None
        self._agent: Optional[PPOAgent] = None
        self._opponent: Optional[Union[PPOAgent, BaseOpponent]] = None

    def _setup(self) -> None:
        """
        Set up seeds, W&B, logger, agent, and opponent.
        Raises RuntimeError if any required component fails to initialize.
        """
        if self.seed is not None:
            try:
                random.seed(self.seed)
                torch.manual_seed(self.seed)
                np.random.seed(self.seed)
                if self.device_str == "cuda":
                    torch.cuda.manual_seed_all(self.seed)
                print(f"[Evaluator] Set random seed to: {self.seed}")
            except Exception as e:
                raise RuntimeError("Failed to set random seed") from e

        # W&B Initialization
        if self.wandb_log_eval:
            try:
                wandb_config = {
                    "agent_checkpoint": self.agent_checkpoint_path,
                    "opponent_type": self.opponent_type,
                    "opponent_checkpoint": self.opponent_checkpoint_path,
                    "num_games": self.num_games,
                    "max_moves_per_game": self.max_moves_per_game,
                    "device": self.device_str,
                    "seed": self.seed,
                }
                if self.wandb_extra_config:
                    wandb_config.update(self.wandb_extra_config)
                wandb_kwargs: Dict[str, Any] = {
                    "project": self.wandb_project_eval or "keisei-evaluation-runs",
                    "entity": self.wandb_entity_eval,
                    "name": self.wandb_run_name_eval,
                    "config": wandb_config,
                }
                if self.wandb_reinit is not None:
                    wandb_kwargs["reinit"] = self.wandb_reinit
                if self.wandb_group is not None:
                    wandb_kwargs["group"] = self.wandb_group
                try:
                    self._wandb_run = wandb.init(**wandb_kwargs)
                    print(
                        f"[Evaluator] W&B logging enabled: {self._wandb_run.name if self._wandb_run else ''}"
                    )
                    self._wandb_active = True
                except (OSError, RuntimeError, ValueError) as e:
                    print(
                        f"[Evaluator] Error initializing W&B: {e}. W&B logging disabled."
                    )
                    self._wandb_active = False
            except (OSError, RuntimeError, ValueError) as e:
                print(f"[Evaluator] Unexpected error during W&B initialization: {e}")
                self._wandb_active = False
            except Exception as e:
                print(f"[Evaluator] Critical error during W&B initialization: {e}")
                self._wandb_active = False

        # Ensure log directory exists
        log_dir_eval = os.path.dirname(self.log_file_path_eval)
        if log_dir_eval and not os.path.exists(log_dir_eval):
            try:
                os.makedirs(log_dir_eval)
            except Exception as e:
                raise RuntimeError(
                    f"Failed to create log directory '{log_dir_eval}': {e}"
                ) from e

        # Logger
        try:
            self._logger = EvaluationLogger(
                self.log_file_path_eval, also_stdout=self.logger_also_stdout
            )
        except Exception as e:
            raise RuntimeError(f"Failed to initialize EvaluationLogger: {e}") from e
        # Agent and opponent
        # Use the correct input_channels from self.policy_mapper if available, else default to 46
        input_channels = getattr(self.policy_mapper, "input_channels", 46) # TODO: Remove this hardcoded value when possible, generate exception if not set

        try:
            self._agent = load_evaluation_agent(
                self.agent_checkpoint_path,
                self.device_str,
                self.policy_mapper,
                input_channels,
            )
        except Exception as e:
            raise RuntimeError(f"Failed to load evaluation agent: {e}") from e
        try:
            self._opponent = initialize_opponent(
                self.opponent_type,
                self.opponent_checkpoint_path,
                self.device_str,
                self.policy_mapper,
                input_channels,
            )
        except Exception as e: # pylint: disable=broad-except
            print(f"Error initializing opponent {opponent_type}: {e}")
            return None
        if self._logger is None or self._agent is None or self._opponent is None:
            raise RuntimeError(
                "Evaluator setup failed: logger, agent, or opponent is None."
            )

    def evaluate(self) -> Optional[dict]:
        """
        Run the evaluation and return the results dictionary.
        Raises RuntimeError if the Evaluator is not properly initialized or if evaluation fails.
        """
        self._setup()
        results_summary = None
        if self._logger is None or self._agent is None or self._opponent is None:
            raise RuntimeError("Evaluator not properly initialized.")
        try:
            with self._logger as logger:
                logger.log("Starting Shogi Agent Evaluation (Evaluator class call).")
                logger.log(
                    f"Parameters: agent_ckpt='{self.agent_checkpoint_path}', opponent='{self.opponent_type}', num_games={self.num_games}"
                )
                results_summary = run_evaluation_loop(
                    self._agent,
                    self._opponent,
                    self.num_games,
                    logger,
                    self.policy_mapper,
                    self.max_moves_per_game,
                    self.device_str,
                    wandb_enabled=self._wandb_active,
                )
                logger.log(f"[Evaluator] Evaluation Summary: {results_summary}")
        except (RuntimeError, ValueError, OSError) as e:
            print(f"[Evaluator] Error during evaluation run: {e}")
            results_summary = None
        except Exception as e: # pylint: disable=broad-except
            print(f"Error during evaluation run: {e}")
            results_summary = None
        # Final W&B logging
        if self._wandb_active and results_summary is not None:
            try:
                wandb.log(
                    {
                        "eval/final_win_rate": results_summary["win_rate"],
                        "eval/final_loss_rate": results_summary["loss_rate"],
                        "eval/final_draw_rate": results_summary["draw_rate"],
                        "eval/final_avg_game_length": results_summary[
                            "avg_game_length"
                        ],
                    }
                )
                print("[Evaluator] Final W&B metrics logged.")
            except (OSError, RuntimeError, ValueError) as e:
                print(f"[Evaluator] Error logging final metrics to W&B: {e}")
            except Exception as e:
                print(f"[Evaluator] Unhandled error logging to W&B: {e}")
        if self._wandb_active:
            try:
                wandb.finish()
                print("[Evaluator] W&B run finished.")
            except (OSError, RuntimeError, ValueError) as e:
                print(f"[Evaluator] Error finishing W&B run: {e}")
            except Exception as e: # pylint: disable=broad-except
                print(f"Error finalizing evaluation run: {e}")
        return results_summary

# --- Backward-compatible wrapper function ---
def execute_full_evaluation_run(
    agent_checkpoint_path: str,
    opponent_type: str,
    opponent_checkpoint_path: Optional[str],
    num_games: int,
    max_moves_per_game: int,
    device_str: str,
    log_file_path_eval: str,
    policy_mapper: PolicyOutputMapper,
    seed: Optional[int] = None,
    wandb_log_eval: bool = False,
    wandb_project_eval: Optional[str] = None,
    wandb_entity_eval: Optional[str] = None,
    wandb_run_name_eval: Optional[str] = None,
    logger_also_stdout: bool = True,
    wandb_extra_config: Optional[dict] = None,
    wandb_reinit: Optional[bool] = None,
    wandb_group: Optional[str] = None,
    _called_from_cli: bool = False,
) -> Optional[dict]:
    """
    Backward-compatible wrapper for programmatic evaluation. Calls the Evaluator class.
    """
    evaluator = Evaluator(
        agent_checkpoint_path=agent_checkpoint_path,
        opponent_type=opponent_type,
        opponent_checkpoint_path=opponent_checkpoint_path,
        num_games=num_games,
        max_moves_per_game=max_moves_per_game,
        device_str=device_str,
        log_file_path_eval=log_file_path_eval,
        policy_mapper=policy_mapper,
        seed=seed,
        wandb_log_eval=wandb_log_eval,
        wandb_project_eval=wandb_project_eval,
        wandb_entity_eval=wandb_entity_eval,
        wandb_run_name_eval=wandb_run_name_eval,
        logger_also_stdout=logger_also_stdout,
        wandb_extra_config=wandb_extra_config,
        wandb_reinit=wandb_reinit,
        wandb_group=wandb_group,
    )
    return evaluator.evaluate()


# --- CLI entry point ---
def main_cli(): # Renamed from main to main_cli to avoid conflict if this file is imported
    """
    CLI entry point for evaluation. Parses arguments and runs evaluation using Evaluator.
    """
    import argparse # Moved import here

    parser = argparse.ArgumentParser(description="Evaluate a PPO Shogi agent.")
    parser.add_argument(
        "--agent_checkpoint",
        type=str,
        required=True,
        help="Path to agent checkpoint file.",
    )
    parser.add_argument(
        "--opponent_type",
        type=str,
        required=True,
        choices=["random", "heuristic", "ppo"],
        help="Type of opponent.",
    )
    parser.add_argument(
        "--opponent_checkpoint",
        type=str,
        default=None,
        help="Path to opponent checkpoint (if PPO).",
    )
    parser.add_argument(
        "--num_games", type=int, default=10, help="Number of games to play."
    )
    parser.add_argument(
        "--max_moves_per_game", type=int, default=200, help="Maximum moves per game."
    )
    parser.add_argument(
        "--device", type=str, default="cpu", help="Device for evaluation (cpu/cuda)."
    )
    parser.add_argument(
        "--log_file",
        type=str,
        default="eval_log.txt",
        help="Path to evaluation log file.",
    )
    parser.add_argument("--seed", type=int, default=None, help="Random seed.")
    parser.add_argument(
        "--wandb_log_eval", action="store_true", help="Enable W&B logging."
    )
    parser.add_argument(
        "--wandb_project_eval", type=str, default=None, help="W&B project name."
    )
    parser.add_argument(
        "--wandb_entity_eval", type=str, default=None, help="W&B entity."
    )
    parser.add_argument(
        "--wandb_run_name_eval", type=str, default=None, help="W&B run name."
    )
    parser.add_argument("--wandb_group", type=str, default=None, help="W&B group name.")
    parser.add_argument("--wandb_reinit", action="store_true", help="W&B reinit flag.")
    args = parser.parse_args()

    policy_mapper = PolicyOutputMapper()
    evaluator = Evaluator(
        agent_checkpoint_path=args.agent_checkpoint,
        opponent_type=args.opponent_type,
        opponent_checkpoint_path=args.opponent_checkpoint,
        num_games=args.num_games,
        max_moves_per_game=args.max_moves_per_game,
        device_str=args.device,
        log_file_path_eval=args.log_file,
        policy_mapper=policy_mapper,
        seed=args.seed,
        wandb_log_eval=args.wandb_log_eval,
        wandb_project_eval=args.wandb_project_eval,
        wandb_entity_eval=args.wandb_entity_eval,
        wandb_run_name_eval=args.wandb_run_name_eval,
        logger_also_stdout=True,
        wandb_extra_config=None,
        wandb_reinit=args.wandb_reinit,
        wandb_group=args.wandb_group,
    )
    results = evaluator.evaluate()
    print("Evaluation Results:")
    print(results)

if __name__ == "__main__":
    main_cli()

]]></file>
  <file path="evaluation/__init__.py"><![CDATA[
# keisei/evaluation/__init__.py

]]></file>
  <file path="training/display.py"><![CDATA[
"""
training/display.py: Rich UI management for the Shogi RL trainer.
"""
from rich.console import Console, Group
from rich.layout import Layout
from rich.live import Live
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    ProgressColumn,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from rich.text import Text
from typing import Any, Dict, List, Union

class TrainingDisplay:
    def __init__(self, config, trainer, rich_console: Console):
        self.config = config
        self.trainer = trainer
        self.rich_console = rich_console
        self.rich_log_messages = trainer.rich_log_messages
        self.progress_bar, self.training_task, self.layout, self.log_panel = self._setup_rich_progress_display()

    def _setup_rich_progress_display(self):
        progress_columns: List[Union[str, ProgressColumn]]
        base_columns: List[Union[str, ProgressColumn]] = [
            "[progress.description]{task.description}",
            BarColumn(),
            TaskProgressColumn(),
            TextColumn("•"),
            TimeElapsedColumn(),
            TextColumn("•"),
            TimeRemainingColumn(),
            TextColumn(
                "• Steps: {task.completed}/{task.total} ({task.fields[speed]:.1f} it/s)"
            ),
            TextColumn("• {task.fields[ep_metrics]}", style="bright_cyan"),
            TextColumn("• {task.fields[ppo_metrics]}", style="bright_yellow"),
            TextColumn(
                "• Wins B:{task.fields[black_wins_cum]} W:{task.fields[white_wins_cum]} D:{task.fields[draws_cum]}",
                style="bright_green",
            ),
            TextColumn(
                "• Rates B:{task.fields[black_win_rate]:.1%} W:{task.fields[white_win_rate]:.1%} D:{task.fields[draw_rate]:.1%}",
                style="bright_blue",
            ),
        ]
        enable_spinner = getattr(self.config.training, "enable_spinner", True)
        if enable_spinner:
            progress_columns = [SpinnerColumn()] + base_columns
        else:
            progress_columns = base_columns
        progress_bar = Progress(
            *progress_columns,
            console=self.rich_console,
            transient=False,
        )
        initial_black_win_rate = (
            self.trainer.black_wins / self.trainer.total_episodes_completed
            if self.trainer.total_episodes_completed > 0
            else 0.0
        )
        initial_white_win_rate = (
            self.trainer.white_wins / self.trainer.total_episodes_completed
            if self.trainer.total_episodes_completed > 0
            else 0.0
        )
        initial_draw_rate = (
            self.trainer.draws / self.trainer.total_episodes_completed
            if self.trainer.total_episodes_completed > 0
            else 0.0
        )
        training_task = progress_bar.add_task(
            "Training",
            total=self.config.training.total_timesteps,
            completed=self.trainer.global_timestep,
            ep_metrics="Ep L:0 R:0.0",
            ppo_metrics="",
            black_wins_cum=self.trainer.black_wins,
            white_wins_cum=self.trainer.white_wins,
            draws_cum=self.trainer.draws,
            black_win_rate=initial_black_win_rate,
            white_win_rate=initial_white_win_rate,
            draw_rate=initial_draw_rate,
            speed=0.0,
            start=(self.trainer.global_timestep < self.config.training.total_timesteps),
        )
        log_panel = Panel(
            Text(""),
            title="[b]Live Training Log[/b]",
            border_style="bright_green",
            expand=True,
        )
        layout = Layout(name="root")
        layout.split_column(
            Layout(name="main_log", ratio=1),
            Layout(name="progress_display", size=4),
        )
        layout["main_log"].update(log_panel)
        layout["progress_display"].update(progress_bar)
        return progress_bar, training_task, layout, log_panel

    def update_progress(self, trainer, speed, pending_updates):
        update_data = {"completed": trainer.global_timestep, "speed": speed}
        update_data.update(pending_updates)
        self.progress_bar.update(self.training_task, **update_data)

    def update_log_panel(self, trainer):
        visible_rows = max(0, self.rich_console.size.height - 6)
        if trainer.rich_log_messages:
            display_messages = trainer.rich_log_messages[-visible_rows:]
            updated_panel_content = Group(*display_messages)
            self.log_panel.renderable = updated_panel_content
        else:
            self.log_panel.renderable = Text("")

    def start(self):
        return Live(
            self.layout,
            console=self.rich_console,
            refresh_per_second=self.config.training.refresh_per_second,
            transient=False,
        )

]]></file>
  <file path="training/train.py"><![CDATA[
"""
Main training script for Keisei Shogi RL agent.
Refactored to use the Trainer class for better modularity.
"""

import argparse
import multiprocessing
import sys
from datetime import datetime

from keisei.config_schema import AppConfig

# Import config module and related components
from keisei.utils import load_config

from .trainer import Trainer


def main():
    """Main entry point for the training script (Pydantic config version)."""
    parser = argparse.ArgumentParser(
        description="Train PPO agent for Shogi with Rich TUI (Pydantic config)."
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to a YAML or JSON configuration file.",
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="Path to a checkpoint file to resume training from, or 'latest' to auto-detect.",
    )
    parser.add_argument(
        "--seed", type=int, default=None, help="Random seed for reproducibility."
    )
    # --- Model/feature CLI flags ---
    parser.add_argument("--model", type=str, default=None, help="Model type (e.g. 'resnet').")
    parser.add_argument("--input_features", type=str, default=None, help="Feature set for observation builder.")
    parser.add_argument("--tower_depth", type=int, default=None, help="ResNet tower depth.")
    parser.add_argument("--tower_width", type=int, default=None, help="ResNet tower width.")
    parser.add_argument("--se_ratio", type=float, default=None, help="SE block squeeze ratio.")
    parser.add_argument("--mixed_precision", action="store_true", help="Enable mixed-precision training.")
    parser.add_argument("--ddp", action="store_true", help="Enable DistributedDataParallel training.")
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device to use for training (e.g., 'cpu', 'cuda').",
    )
    parser.add_argument(
        "--total-timesteps",
        type=int,
        default=None,
        help="Total timesteps to train for. Overrides config value.",
    )
    parser.add_argument(
        "--savedir",
        type=str,
        default=None,
        help="Directory to save models and logs. Overrides config MODEL_DIR.",
    )
    parser.add_argument(
        "--override",
        action="append",
        default=[],
        help="Override config values via KEY.SUBKEY=VALUE format.",
    )
    parser.add_argument(
        "--render-every",
        type=int,
        default=None,
        help="Update display every N steps to reduce flicker. Overrides config value.",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        default=None,
        help="Optional name for this run (overrides config and auto-generated name).",
    )
    args = parser.parse_args()

    # Build CLI overrides dict (dot notation)
    cli_overrides = {}
    for override in args.override:
        if "=" in override:
            k, v = override.split("=", 1)
            cli_overrides[k] = v

    # Add direct CLI args as overrides if set
    if args.seed is not None:
        cli_overrides["env.seed"] = args.seed
    if args.device is not None:
        cli_overrides["env.device"] = args.device
    if args.total_timesteps is not None:
        cli_overrides["training.total_timesteps"] = args.total_timesteps
    if args.savedir is not None:
        cli_overrides["logging.model_dir"] = args.savedir
    if args.render_every is not None:
        cli_overrides["training.render_every_steps"] = args.render_every
    # Do NOT generate run_name here; let Trainer handle it for correct CLI/config/auto priority

    # Load config (YAML/JSON + CLI overrides)
    config: AppConfig = load_config(args.config, cli_overrides)

    # Initialize and run the trainer (Trainer will determine run_name)
    trainer = Trainer(config=config, args=args)
    trainer.run_training_loop()


if __name__ == "__main__":
    # Set multiprocessing start method for safety, especially with CUDA
    try:
        if multiprocessing.get_start_method(allow_none=True) != "spawn":
            multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError as e:
        print(
            f"Warning: Could not set multiprocessing start method to 'spawn': {e}. Using default: {multiprocessing.get_start_method(allow_none=True)}.",
            file=sys.stderr,
        )
    except Exception as e:
        print(f"Error setting multiprocessing start_method: {e}", file=sys.stderr)

    main()

]]></file>
  <file path="training/utils.py"><![CDATA[
"""
training/utils.py: Helper functions for setup and configuration in the Shogi RL trainer.
"""
import glob
import json
import os
import random
import sys
from typing import Any
import numpy as np
import torch
import wandb

def find_latest_checkpoint(model_dir_path):
    try:
        checkpoints = glob.glob(os.path.join(model_dir_path, "*.pth"))
        if not checkpoints:
            checkpoints = glob.glob(os.path.join(model_dir_path, "*.pt"))
        if not checkpoints:
            return None
        checkpoints.sort(key=os.path.getmtime, reverse=True)
        return checkpoints[0]
    except (OSError, FileNotFoundError) as e:
        print(f"Error in find_latest_checkpoint: {e}", file=sys.stderr)
        return None

def serialize_config(config_obj: Any) -> str:
    if hasattr(config_obj, "dict"):
        conf_dict = config_obj.dict()
    else:
        conf_dict = {}
        if hasattr(config_obj, "__dict__"):
            source_dict = config_obj.__dict__
        elif isinstance(config_obj, dict):
            source_dict = config_obj
        else:
            source_dict = {
                key: getattr(config_obj, key)
                for key in dir(config_obj)
                if not key.startswith("__") and not callable(getattr(config_obj, key))
            }
        for k, v in source_dict.items():
            if isinstance(v, (int, float, str, bool, list, dict, tuple)) or v is None:
                conf_dict[k] = v
            elif hasattr(v, "dict"):
                conf_dict[k] = v.dict()
            elif hasattr(v, "__dict__"):
                conf_dict[k] = json.loads(serialize_config(v))
    try:
        return json.dumps(conf_dict, indent=4, sort_keys=True)
    except TypeError as e:
        print(f"Error serializing config: {e}", file=sys.stderr)
        return "{}"

def setup_directories(config, run_name):
    model_dir = config.logging.model_dir
    log_file = config.logging.log_file
    run_artifact_dir = os.path.join(model_dir, run_name)
    model_dir_path = run_artifact_dir
    log_file_path = os.path.join(run_artifact_dir, os.path.basename(log_file))
    eval_log_file_path = os.path.join(run_artifact_dir, "rich_periodic_eval_log.txt")
    os.makedirs(run_artifact_dir, exist_ok=True)
    return {
        "run_artifact_dir": run_artifact_dir,
        "model_dir": model_dir_path,
        "log_file_path": log_file_path,
        "eval_log_file_path": eval_log_file_path,
    }

def setup_seeding(config):
    seed = config.env.seed
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

def setup_wandb(config, run_name, run_artifact_dir):
    wandb_cfg = config.wandb
    is_active = wandb_cfg.enabled
    if is_active:
        try:
            config_dict_for_wandb = (
                json.loads(serialize_config(config))
                if serialize_config(config)
                else {}
            )
            wandb.init(
                project=wandb_cfg.project,
                entity=wandb_cfg.entity,
                name=run_name,
                config=config_dict_for_wandb,
                mode="online" if wandb_cfg.enabled else "disabled",
                dir=run_artifact_dir,
                resume="allow",
                id=run_name,
            )
        except (TypeError, ValueError, OSError) as e:
            print(f"Error initializing W&B: {e}. W&B logging disabled.", file=sys.stderr)
            is_active = False
    if not is_active:
        print("Weights & Biases logging is disabled or failed to initialize.", file=sys.stderr)
    return is_active

]]></file>
  <file path="training/trainer.py"><![CDATA[
"""
trainer.py: Contains the Trainer class for managing the Shogi RL training loop (refactored).
"""

import json
import os
import sys
import time
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union # pylint: disable=unused-import

import numpy as np
import torch # Add torch import
import wandb
from rich.console import Console, Text
from torch.cuda.amp import GradScaler, autocast # For mixed precision

from keisei.config_schema import AppConfig
from keisei.core.experience_buffer import ExperienceBuffer
from keisei.core.ppo_agent import PPOAgent
from keisei.evaluation.evaluate import execute_full_evaluation_run
from keisei.shogi import Color, ShogiGame
from keisei.utils import (
    PolicyOutputMapper,
    TrainingLogger,
    format_move_with_description_enhanced,
)
from keisei.utils.utils import generate_run_name # ADDED: Import generate_run_name from correct location
from . import utils, display, callbacks


class Trainer:
    """
    Manages the training process for the PPO Shogi agent.
    This class orchestrates setup, training loop, evaluation, and logging.
    """

    # Mixed-precision training
    use_mixed_precision: bool = False
    scaler: Optional[torch.cuda.amp.GradScaler] = None

    # Statistics for tracking and logging
    global_timestep: int = 0
    total_episodes_completed: int = 0
    black_wins: int = 0
    white_wins: int = 0
    draws: int = 0

    def __init__(self, config: AppConfig, args: Any):
        """
        Initializes the Trainer with configuration and command-line arguments.

        Args:
            config: An AppConfig Pydantic object containing all configuration.
            args: Parsed command-line arguments.
        """
        self.config = config
        self.args = args
        # Determine run_name: CLI > config > auto-generate
        run_name = None
        if hasattr(args, "run_name") and args.run_name:
            run_name = args.run_name
        elif hasattr(config, "logging") and hasattr(config.logging, "run_name") and config.logging.run_name:
            run_name = config.logging.run_name
        self.run_name = generate_run_name(config, run_name)

        # Initialize statistics
        self.global_timestep = 0
        self.total_episodes_completed = 0
        self.black_wins = 0
        self.white_wins = 0
        self.draws = 0

        self.device = torch.device(config.env.device)
        self.console = Console()

        # Setup directories
        dirs = utils.setup_directories(self.config, self.run_name)
        self.run_artifact_dir = dirs["run_artifact_dir"]
        self.model_dir = dirs["model_dir"]
        self.log_file_path = dirs["log_file_path"]
        self.eval_log_file_path = dirs["eval_log_file_path"]

        self.logger = TrainingLogger(self.log_file_path, self.console)

        # Mixed Precision Setup
        self.use_mixed_precision = (
            self.config.training.mixed_precision and self.device.type == "cuda"
        )
        if self.use_mixed_precision:
            self.scaler = GradScaler()
            self.logger.log(
                str(Text("Mixed precision training enabled (CUDA).", style="green")) # Convert Text to str
            )
        elif self.config.training.mixed_precision and self.device.type != "cuda":
            self.logger.log(
                str(Text( # Convert Text to str
                    "Mixed precision training requested but CUDA is not available/selected. Proceeding without mixed precision.",
                    style="yellow",
                ))
            )
            self.use_mixed_precision = False

        # --- Model/feature config integration ---
        self.input_features = getattr(args, 'input_features', None) or config.training.input_features
        self.model_type = getattr(args, 'model', None) or config.training.model_type
        self.tower_depth = getattr(args, 'tower_depth', None) or config.training.tower_depth
        self.tower_width = getattr(args, 'tower_width', None) or config.training.tower_width
        self.se_ratio = getattr(args, 'se_ratio', None) or config.training.se_ratio
        # Feature builder
        from keisei.shogi import features
        self.feature_spec = features.FEATURE_SPECS[self.input_features]
        self.obs_shape = (self.feature_spec.num_planes, 9, 9)
        # Model factory
        from keisei.training.models import model_factory # Corrected import
        # from keisei.training.models.resnet_tower import ActorCriticResTower # Old direct import
        # if self.model_type == "resnet": # Old direct instantiation
        self.model = model_factory(
            model_type=self.model_type,
            obs_shape=self.obs_shape, 
            num_actions=config.env.num_actions_total, # Added num_actions
            tower_depth=self.tower_depth,
            tower_width=self.tower_width,
            se_ratio=self.se_ratio if self.se_ratio > 0 else None,
            # Add any other kwargs your model_factory or models might need, e.g.:
            # num_actions_total=config.env.num_actions_total # Already passed as num_actions
        )
        # else:
        #     raise ValueError(f"Unknown model_type: {self.model_type}")

        # Save effective config
        try:
            effective_config_str = utils.serialize_config(self.config)
            config_path = os.path.join(self.run_artifact_dir, "effective_config.json")
            with open(config_path, "w", encoding="utf-8") as f:
                f.write(effective_config_str)
        except (OSError, TypeError) as e:
            print(f"Error saving effective_config.json: {e}", file=sys.stderr)

        # Setup seeding
        utils.setup_seeding(self.config)

        # Initialize Rich TUI
        self.rich_console = Console(file=sys.stderr, record=True)
        self.rich_log_messages: List[Text] = []

        # WP-2: Store pending progress bar updates to consolidate them
        self.pending_progress_updates: Dict[str, Any] = {}

        # Initialize WandB
        self.is_train_wandb_active = utils.setup_wandb(
            self.config, self.run_name, self.run_artifact_dir
        )

        # Initialize game and components
        self._setup_game_components()

        # Setup training components
        self._setup_training_components()

        # Handle checkpoint resuming
        self._handle_checkpoint_resume()

        # Display and callbacks
        self.display = display.TrainingDisplay(self.config, self, self.rich_console)
        eval_cfg = getattr(self.config, "evaluation", None)
        checkpoint_interval = self.config.training.checkpoint_interval_timesteps # Use config value
        eval_interval = (
            eval_cfg.evaluation_interval_timesteps if eval_cfg else self.config.training.evaluation_interval_timesteps # Use config value
        )
        self.callbacks = [
            callbacks.CheckpointCallback(checkpoint_interval, self.model_dir),
            callbacks.EvaluationCallback(eval_cfg, eval_interval),
        ]

    def _setup_game_components(self):
        """Initialize game environment and policy mapper."""
        try:
            self.game = ShogiGame()
            if hasattr(self.game, "seed") and self.config.env.seed is not None:
                self.game.seed(self.config.env.seed)
            self.obs_space_shape = (self.config.env.input_channels, 9, 9)
        except (RuntimeError, ValueError, OSError) as e:
            self.rich_console.print(
                f"[bold red]Error initializing ShogiGame: {e}. Aborting.[/bold red]"
            )
            raise RuntimeError(f"Failed to initialize ShogiGame: {e}") from e
        self.policy_output_mapper = PolicyOutputMapper()
        self.action_space_size = self.policy_output_mapper.get_total_actions()

    def _setup_training_components(self):
        """Initialize PPO agent and experience buffer."""
        self.agent = PPOAgent(
            config=self.config,
            device=torch.device(self.config.env.device),
        )
        self.experience_buffer = ExperienceBuffer(
            buffer_size=self.config.training.steps_per_epoch,
            gamma=self.config.training.gamma,
            lambda_gae=self.config.training.lambda_gae, # Use config value
            device=self.config.env.device,
        )

    def _log_event(self, message: str):
        """Log important events to the main training log file."""
        # Always log to the main log file
        try:
            with open(self.log_file_path, "a", encoding="utf-8") as f:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{timestamp}] {message}\n")
        except Exception as e:
            print(f"[Trainer] Failed to log event: {e}", file=sys.stderr)
        # No longer print to stderr for test compatibility

    def _handle_checkpoint_resume(self):
        """Handle resuming from checkpoint if specified or auto-detected."""
        import shutil
        resume_path = self.args.resume
        def find_ckpt_in_dir(directory):
            return utils.find_latest_checkpoint(directory)
        if resume_path == "latest" or resume_path is None:
            # Try to find latest checkpoint in the run's model_dir
            latest_ckpt = find_ckpt_in_dir(self.model_dir)
            # If not found, try the parent directory (savedir)
            if not latest_ckpt:
                parent_dir = os.path.dirname(self.model_dir.rstrip(os.sep))
                parent_ckpt = find_ckpt_in_dir(parent_dir)
                if parent_ckpt:
                    # Copy the checkpoint into the run's model_dir for consistency
                    dest_ckpt = os.path.join(self.model_dir, os.path.basename(parent_ckpt))
                    shutil.copy2(parent_ckpt, dest_ckpt)
                    latest_ckpt = dest_ckpt
            if latest_ckpt:
                self.agent.load_model(latest_ckpt)
                self.resumed_from_checkpoint = latest_ckpt
                msg = f"Resumed training from checkpoint: {latest_ckpt}"
                self._log_event(msg)
                if hasattr(self, "rich_console"):
                    self.rich_console.print(f"[yellow]{msg}[/yellow]")
            else:
                self.resumed_from_checkpoint = None
        elif resume_path:
            self.agent.load_model(resume_path)
            self.resumed_from_checkpoint = resume_path
            msg = f"Resumed training from checkpoint: {resume_path}"
            self._log_event(msg)
            if hasattr(self, "rich_console"):
                self.rich_console.print(f"[yellow]{msg}[/yellow]")
        else:
            self.resumed_from_checkpoint = None

    def _log_run_info(self, log_both):
        """Log run information at the start of training."""
        run_title = f"Keisei Training Run: {self.run_name}"
        if self.is_train_wandb_active and wandb.run and hasattr(wandb.run, "url"):
            run_title += f" (W&B: {wandb.run.url})"

        log_both(run_title)
        self._log_event(run_title)
        log_both(f"Run directory: {self.run_artifact_dir}")
        self._log_event(f"Run directory: {self.run_artifact_dir}")
        log_both(
            f"Effective config saved to: {os.path.join(self.run_artifact_dir, 'effective_config.json')}"
        )
        self._log_event(f"Effective config saved to: {os.path.join(self.run_artifact_dir, 'effective_config.json')}")

        if self.config.env.seed is not None:
            log_both(f"Random seed: {self.config.env.seed}")
            self._log_event(f"Random seed: {self.config.env.seed}")

        log_both(f"Device: {self.config.env.device}")
        self._log_event(f"Device: {self.config.env.device}")
        log_both(f"Agent: {type(self.agent).__name__} ({self.agent.name})")
        self._log_event(f"Agent: {type(self.agent).__name__} ({self.agent.name})")
        log_both(
            f"Total timesteps: {self.config.training.total_timesteps}, Steps per PPO epoch: {self.config.training.steps_per_epoch}"
        )
        self._log_event(
            f"Total timesteps: {self.config.training.total_timesteps}, Steps per PPO epoch: {self.config.training.steps_per_epoch}"
        )

        if self.global_timestep > 0:
            if self.resumed_from_checkpoint:
                log_both(
                    f"[green]Resumed training from checkpoint: {self.resumed_from_checkpoint}[/green]"
                )
                self._log_event(f"Resumed training from checkpoint: {self.resumed_from_checkpoint}")
            log_both(
                f"Resuming from timestep {self.global_timestep}, {self.total_episodes_completed} episodes completed."
            )
            self._log_event(
                f"Resuming from timestep {self.global_timestep}, {self.total_episodes_completed} episodes completed."
            )
        else:
            log_both("Starting fresh training.")
            self._log_event("Starting fresh training.")

        log_both(f"Model Structure:\n{self.agent.model}", also_to_wandb=False)
        self._log_event(f"Model Structure:\n{self.agent.model}")

    def _initialize_game_state(self, log_both):
        """Initialize the game state for training."""
        try:
            reset_result = self.game.reset()
            if not isinstance(reset_result, np.ndarray):
                if self.is_train_wandb_active and wandb.run:
                    wandb.finish(exit_code=1)
                raise RuntimeError("Game reset failed")
            return reset_result
        except (RuntimeError, ValueError, OSError) as e:
            log_both(
                f"CRITICAL: Error during initial game.reset(): {e}. Aborting.",
                also_to_wandb=True,
            )
            if self.is_train_wandb_active and wandb.run:
                wandb.finish(exit_code=1)
            raise RuntimeError(f"Game initialization error: {e}") from e

    def _execute_training_step(
        self,
        current_obs_np,
        current_obs_tensor,
        current_episode_reward,
        current_episode_length,
        log_both,
    ):
        """Execute a single training step."""
        # Get legal moves
        legal_shogi_moves = self.game.get_legal_moves()
        legal_mask_tensor = self.policy_output_mapper.get_legal_mask(
            legal_shogi_moves, device=torch.device(self.config.env.device)
        )

        # For demo mode - capture piece info before the move
        piece_info_for_demo = None
        if (
            self.config.demo.enable_demo_mode
            and len(legal_shogi_moves) > 0
            and legal_shogi_moves[0] is not None
        ):
            try:
                sample_move = legal_shogi_moves[0]
                if (
                    len(sample_move) == 5
                    and sample_move[0] is not None
                    and sample_move[1] is not None
                ):
                    from_r, from_c = sample_move[0], sample_move[1]
                    piece_info_for_demo = self.game.get_piece(from_r, from_c)
            except (AttributeError, IndexError, ValueError):
                pass  # Silently ignore errors in demo mode preparation

        # Agent action selection
        selected_shogi_move, policy_index, log_prob, value_pred = (
            self.agent.select_action(
                current_obs_np, legal_mask_tensor, is_training=True
            )
        )

        if selected_shogi_move is None:
            log_both(
                f"CRITICAL: Agent failed to select a move at timestep {self.global_timestep}. Resetting episode.",
                also_to_wandb=True,
            )
            current_obs_np = self.game.reset()
            current_obs_tensor = torch.tensor(
                current_obs_np,
                dtype=torch.float32,
                device=torch.device(self.config.env.device),
            ).unsqueeze(0)
            return current_obs_np, current_obs_tensor, 0.0, 0

        # Demo mode per-move logging and delay
        if self.config.demo.enable_demo_mode:
            current_player_name = (
                getattr(
                    self.game.current_player,
                    "name",
                    str(self.game.current_player),
                )
                if hasattr(self.game, "current_player")
                else "Unknown"
            )
            move_str = format_move_with_description_enhanced(
                selected_shogi_move,
                self.policy_output_mapper,
                piece_info_for_demo,
            )
            log_both(
                f"Move {current_episode_length + 1}: {current_player_name} played {move_str}"
            )

            # Add delay for easier observation
            demo_delay = self.config.demo.demo_mode_delay
            if demo_delay > 0:
                time.sleep(demo_delay)

        # Environment step
        try:
            move_result = self.game.make_move(selected_shogi_move)
            if not (isinstance(move_result, tuple) and len(move_result) == 4):
                raise ValueError(f"Invalid move result: {type(move_result)}")
            next_obs_np, reward, done, info = move_result
            current_episode_reward += reward
            current_episode_length += 1

            # Add experience to buffer
            self.experience_buffer.add(
                current_obs_tensor.squeeze(0),
                policy_index,
                reward,
                log_prob,
                value_pred,
                done,
                legal_mask_tensor,
            )

            # Update observations
            current_obs_np = next_obs_np
            current_obs_tensor = torch.tensor(
                current_obs_np,
                dtype=torch.float32,
                device=torch.device(self.config.env.device),
            ).unsqueeze(0)

            if done:
                (
                    current_obs_np,
                    current_obs_tensor,
                    current_episode_reward,
                    current_episode_length,
                ) = self._handle_episode_end(
                    current_episode_reward,
                    current_episode_length,
                    info,
                    log_both,
                )

            # PPO Update
            if (
                (self.global_timestep + 1) % self.config.training.steps_per_epoch == 0
                and self.experience_buffer.ptr == self.config.training.steps_per_epoch
            ):
                self._perform_ppo_update(
                    current_obs_np, log_both
                )

        except ValueError as e:
            log_both(
                f"CRITICAL: Error during training step: {e}. Resetting episode.",
                also_to_wandb=True,
            )
            current_obs_np = self.game.reset()
            current_obs_tensor = torch.tensor(
                current_obs_np,
                dtype=torch.float32,
                device=torch.device(self.config.env.device),
            ).unsqueeze(0)
            current_episode_reward = 0.0
            current_episode_length = 0

        return (
            current_obs_np,
            current_obs_tensor,
            current_episode_reward,
            current_episode_length,
        )

    def _handle_episode_end(
        self,
        current_episode_reward,
        current_episode_length,
        info,
        log_both,
    ):
        """Handle the end of an episode."""
        self.total_episodes_completed += 1
        ep_metrics_str = f"Ep L:{current_episode_length} R:{current_episode_reward:.2f}"

        # Determine game outcome
        game_outcome_message = "Game outcome: Unknown"
        winner_color = None

        if "winner" in info:
            winner = info["winner"]
            if winner is not None:
                game_outcome_message = f"Game outcome: {winner.name} won."
                winner_color = winner
            else:
                game_outcome_message = "Game outcome: Draw."
        elif self.game.winner is not None:
            winner = self.game.winner
            game_outcome_message = f"Game outcome: {winner.name} won."
            winner_color = winner
        elif self.game.game_over and self.game.winner is None:
            game_outcome_message = "Game outcome: Draw (max moves or stalemate)."

        # Update win/loss/draw counts
        if winner_color == Color.BLACK:
            self.black_wins += 1
        elif winner_color == Color.WHITE:
            self.white_wins += 1
        else:
            self.draws += 1

        # Calculate rates
        current_black_win_rate = (
            self.black_wins / self.total_episodes_completed
            if self.total_episodes_completed > 0
            else 0.0
        )
        current_white_win_rate = (
            self.white_wins / self.total_episodes_completed
            if self.total_episodes_completed > 0
            else 0.0
        )
        current_draw_rate = (
            self.draws / self.total_episodes_completed
            if self.total_episodes_completed > 0
            else 0.0
        )

        # Store episode metrics for next throttled update (WP-2)
        self.pending_progress_updates.update(
            {
                "ep_metrics": ep_metrics_str,
                "black_wins_cum": self.black_wins,
                "white_wins_cum": self.white_wins,
                "draws_cum": self.draws,
                "black_win_rate": current_black_win_rate,
                "white_win_rate": current_white_win_rate,
                "draw_rate": current_draw_rate,
            }
        )

        # Log episode completion
        log_both(
            f"Episode {self.total_episodes_completed} finished. Length: {current_episode_length}, Reward: {current_episode_reward:.2f}. {game_outcome_message}",
            also_to_wandb=True,
            wandb_data={
                "episode_reward": current_episode_reward,
                "episode_length": current_episode_length,
                "total_episodes": self.total_episodes_completed,
                "black_wins_cumulative": self.black_wins,
                "white_wins_cumulative": self.white_wins,
                "draws_cumulative": self.draws,
                "black_win_rate": current_black_win_rate,
                "white_win_rate": current_white_win_rate,
                "draw_rate": current_draw_rate,
            },
        )

        # Reset game
        reset_result = self.game.reset()
        if not isinstance(reset_result, np.ndarray):
            log_both(
                f"CRITICAL: game.reset() after episode done did not return ndarray. Got {type(reset_result)}. Aborting.",
                also_to_wandb=True,
            )
            if self.is_train_wandb_active and wandb.run:
                wandb.finish(exit_code=1)
            raise RuntimeError("Game reset failed after episode end")

        current_obs_np = reset_result
        current_obs_tensor = torch.tensor(
            current_obs_np,
            dtype=torch.float32,
            device=torch.device(self.config.env.device),
        ).unsqueeze(0)

        return current_obs_np, current_obs_tensor, 0.0, 0

    def _perform_ppo_update(
        self, current_obs_np, log_both
    ):
        """Perform a PPO update."""
        with torch.no_grad():
            last_value_pred_for_gae = self.agent.get_value(current_obs_np)

        self.experience_buffer.compute_advantages_and_returns(last_value_pred_for_gae)
        learn_metrics = self.agent.learn(self.experience_buffer)
        self.experience_buffer.clear()

        # Format PPO metrics for display
        ppo_metrics_str_parts = []
        if "ppo/kl_divergence_approx" in learn_metrics:
            ppo_metrics_str_parts.append(
                f"KL:{learn_metrics['ppo/kl_divergence_approx']:.4f}"
            )
        if "ppo/policy_loss" in learn_metrics:
            ppo_metrics_str_parts.append(f"PolL:{learn_metrics['ppo/policy_loss']:.4f}")
        if "ppo/value_loss" in learn_metrics:
            ppo_metrics_str_parts.append(f"ValL:{learn_metrics['ppo/value_loss']:.4f}")
        if "ppo/entropy" in learn_metrics:
            ppo_metrics_str_parts.append(f"Ent:{learn_metrics['ppo/entropy']:.4f}")

        ppo_metrics_display = " ".join(ppo_metrics_str_parts)
        # Store PPO metrics for next throttled update (WP-2)
        self.pending_progress_updates["ppo_metrics"] = ppo_metrics_display

        log_both(
            f"PPO Update @ ts {self.global_timestep+1}. Metrics: {json.dumps({k: f'{v:.4f}' for k,v in learn_metrics.items()})}",
            also_to_wandb=True,
            wandb_data=learn_metrics,
        )

    def _finalize_training(self, log_both):
        """Finalize training and save final model."""
        log_both(
            f"Training loop finished at timestep {self.global_timestep}. Total episodes: {self.total_episodes_completed}.",
            also_to_wandb=True,
        )

        if self.global_timestep >= self.config.training.total_timesteps:
            log_both(
                "Training successfully completed all timesteps.", also_to_wandb=True
            )
            final_model_path = os.path.join(self.model_dir, "final_model.pth")
            try:
                self.agent.save_model(
                    final_model_path,
                    self.global_timestep,
                    self.total_episodes_completed,
                )
                log_both(f"Final model saved to {final_model_path}", also_to_wandb=True)

                if self.is_train_wandb_active and wandb.run:
                    wandb.finish()
            except (OSError, RuntimeError) as e:
                log_both(
                    f"Error saving final model {final_model_path}: {e}",
                    log_level="error",
                    also_to_wandb=True,
                )
        else:
            log_both(
                f"Training interrupted at timestep {self.global_timestep} (before {self.config.training.total_timesteps} total).",
                log_level="warning",
                also_to_wandb=True,
            )

        # Always save a final checkpoint if one was not just saved at the last step
        checkpoint_interval = self.config.training.checkpoint_interval_timesteps # Use config value
        last_ckpt_filename = os.path.join(
            self.model_dir, f"checkpoint_ts{self.global_timestep}.pth"
        )
        if self.global_timestep > 0 and not os.path.exists(last_ckpt_filename):
            self.agent.save_model(
                last_ckpt_filename,
                self.global_timestep,
                self.total_episodes_completed,
                stats_to_save={
                    "black_wins": self.black_wins,
                    "white_wins": self.white_wins,
                    "draws": self.draws,
                },
            )
            log_both(
                f"Final checkpoint saved to {last_ckpt_filename}", also_to_wandb=True
            )

        if self.is_train_wandb_active and wandb.run:
            wandb.finish()
            log_both("Weights & Biases run finished.")

        # Save the full console log from Rich
        console_log_path = os.path.join(
            self.run_artifact_dir, "full_console_output_rich.html"
        )
        try:
            self.rich_console.save_html(console_log_path)
            print(
                f"Full Rich console output saved to {console_log_path}", file=sys.stderr
            )
        except OSError as e:
            print(f"Error saving Rich console log: {e}", file=sys.stderr)

        # Final messages
        self.rich_console.rule("[bold green]Run Finished[/bold green]")
        self.rich_console.print(
            f"[bold green]Run '{self.run_name}' processing finished.[/bold green]"
        )
        self.rich_console.print(f"Output and logs are in: {self.run_artifact_dir}")

    def run_training_loop(self):
        """Executes the main training loop."""
        # Always log a session start event to ensure log file is created
        self._log_event(f"--- SESSION START: {self.run_name} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---")
        # TrainingLogger context manager
        with TrainingLogger(
            self.log_file_path,
            rich_console=self.rich_console,
            rich_log_panel=self.rich_log_messages,
        ) as logger:

            def log_both(
                message: str,
                also_to_wandb: bool = False,
                wandb_data: Optional[Dict] = None,
                log_level: str = "info",
            ):
                logger.log(message)
                if self.is_train_wandb_active and also_to_wandb and wandb.run:
                    log_payload = {"train_message": message}
                    if wandb_data:
                        log_payload.update(wandb_data)
                    wandb.log(log_payload, step=self.global_timestep)

            self.log_both = log_both  # Expose for callbacks
            self.execute_full_evaluation_run = execute_full_evaluation_run  # Expose for callbacks

            # Log session start
            session_start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_both(f"--- SESSION START: {self.run_name} at {session_start_time} ---")

            # Setup run information logging
            self._log_run_info(log_both)

            last_time = time.time()
            steps_since_last_time = 0
            current_obs_np = self._initialize_game_state(log_both)
            current_episode_reward = 0.0
            current_episode_length = 0

            current_obs_tensor = torch.tensor(
                current_obs_np,
                dtype=torch.float32,
                device=torch.device(self.config.env.device),
            ).unsqueeze(0)
            with self.display.start() as live:
                while self.global_timestep < self.config.training.total_timesteps:
                    (
                        current_obs_np,
                        current_obs_tensor,
                        current_episode_reward,
                        current_episode_length,
                    ) = self._execute_training_step(
                        current_obs_np,
                        current_obs_tensor,
                        current_episode_reward,
                        current_episode_length,
                        log_both,
                    )

                    # Update step counters
                    self.global_timestep += 1
                    steps_since_last_time += 1

                    # Display updates
                    if (self.global_timestep % self.config.training.render_every_steps) == 0:
                        self.display.update_log_panel(self)

                    current_time = time.time()
                    time_delta = current_time - last_time
                    current_speed = steps_since_last_time / time_delta if time_delta > 0 else 0.0

                    if time_delta > 0.1:  # Update speed roughly every 100ms
                        last_time = current_time
                        steps_since_last_time = 0

                    self.display.update_progress(
                        self, current_speed, self.pending_progress_updates
                    )
                    self.pending_progress_updates.clear()

                    # Callbacks
                    for callback in self.callbacks:
                        callback.on_step_end(self)

                # End of training loop
                self._finalize_training(log_both)

]]></file>
  <file path="training/callbacks.py"><![CDATA[
"""
training/callbacks.py: Periodic task callbacks for the Shogi RL trainer.
"""
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from .trainer import Trainer

class Callback(ABC):
    def on_step_end(self, trainer: "Trainer"):
        pass

class CheckpointCallback(Callback):
    def __init__(self, interval: int, model_dir: str):
        self.interval = interval
        self.model_dir = model_dir
    def on_step_end(self, trainer: "Trainer"):
        if (trainer.global_timestep + 1) % self.interval == 0:
            ckpt_save_path = os.path.join(
                self.model_dir, f"checkpoint_ts{trainer.global_timestep+1}.pth"
            )
            try:
                trainer.agent.save_model(
                    ckpt_save_path,
                    trainer.global_timestep + 1,
                    trainer.total_episodes_completed,
                    stats_to_save={
                        "black_wins": trainer.black_wins,
                        "white_wins": trainer.white_wins,
                        "draws": trainer.draws,
                    },
                )
                trainer.log_both(f"Checkpoint saved to {ckpt_save_path}", also_to_wandb=True)
            except (OSError, RuntimeError) as e:
                trainer.log_both(
                    f"Error saving checkpoint {ckpt_save_path}: {e}",
                    log_level="error",
                    also_to_wandb=True,
                )

import os
class EvaluationCallback(Callback):
    def __init__(self, eval_cfg, interval: int):
        self.eval_cfg = eval_cfg
        self.interval = interval
    def on_step_end(self, trainer: "Trainer"):
        if not getattr(self.eval_cfg, "enable_periodic_evaluation", False):
            return
        if (trainer.global_timestep + 1) % self.interval == 0:
            eval_ckpt_path = os.path.join(
                trainer.model_dir, f"eval_checkpoint_ts{trainer.global_timestep+1}.pth"
            )
            trainer.agent.save_model(
                eval_ckpt_path, trainer.global_timestep + 1, trainer.total_episodes_completed
            )
            trainer.log_both(
                f"Starting periodic evaluation at timestep {trainer.global_timestep + 1}...",
                also_to_wandb=True,
            )
            trainer.agent.model.eval()
            eval_results = trainer.execute_full_evaluation_run(
                agent_checkpoint_path=eval_ckpt_path,
                opponent_type=getattr(self.eval_cfg, "opponent_type", "random"),
                opponent_checkpoint_path=getattr(self.eval_cfg, "opponent_checkpoint_path", None),
                num_games=getattr(self.eval_cfg, "num_games", 20),
                max_moves_per_game=getattr(self.eval_cfg, "max_moves_per_game", 256),
                device_str=trainer.config.env.device,
                log_file_path_eval=getattr(self.eval_cfg, "log_file_path_eval", ""),
                policy_mapper=trainer.policy_output_mapper,
                seed=trainer.config.env.seed,
                wandb_log_eval=getattr(self.eval_cfg, "wandb_log_eval", False),
                wandb_project_eval=getattr(self.eval_cfg, "wandb_project_eval", None),
                wandb_entity_eval=getattr(self.eval_cfg, "wandb_entity_eval", None),
                wandb_run_name_eval=f"periodic_eval_{trainer.run_name}_ts{trainer.global_timestep+1}",
                wandb_group=trainer.run_name,
                wandb_reinit=True,
                logger_also_stdout=False,
            )
            trainer.agent.model.train()
            trainer.log_both(
                f"Periodic evaluation finished. Results: {eval_results}",
                also_to_wandb=True,
                wandb_data=(
                    eval_results if isinstance(eval_results, dict) else {"eval_summary": str(eval_results)}
                ),
            )

]]></file>
  <file path="training/__init__.py"><![CDATA[
# keisei/training/__init__.py

]]></file>
  <file path="training/models/resnet_tower.py"><![CDATA[
"""
resnet_tower.py: ActorCriticResTower model for Keisei Shogi with SE block support.
"""
from typing import Optional
import torch
import torch.nn as nn
import torch.nn.functional as F

class SqueezeExcitation(nn.Module):
    def __init__(self, channels: int, se_ratio: float = 0.25):
        super().__init__()
        hidden = max(1, int(channels * se_ratio))
        self.fc1 = nn.Conv2d(channels, hidden, 1)
        self.fc2 = nn.Conv2d(hidden, channels, 1)
    def forward(self, x):
        s = F.adaptive_avg_pool2d(x, 1)
        s = F.relu(self.fc1(s))
        s = torch.sigmoid(self.fc2(s))
        return x * s

class ResidualBlock(nn.Module):
    def __init__(self, channels: int, se_ratio: Optional[float] = None):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.se = SqueezeExcitation(channels, se_ratio) if se_ratio else None
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.se:
            out = self.se(out)  # pylint: disable=not-callable
        out += x
        return F.relu(out)

class ActorCriticResTower(nn.Module):
    def __init__(self, input_channels: int, num_actions_total: int, tower_depth: int = 9, tower_width: int = 256, se_ratio: Optional[float] = None):
        super().__init__()
        self.stem = nn.Conv2d(input_channels, tower_width, 3, padding=1)
        self.bn_stem = nn.BatchNorm2d(tower_width)
        self.res_blocks = nn.Sequential(*[
            ResidualBlock(tower_width, se_ratio) for _ in range(tower_depth)
        ])
        # Slim policy head: 2 planes, then flatten, then linear
        self.policy_head = nn.Sequential(
            nn.Conv2d(tower_width, 2, 1),
            nn.BatchNorm2d(2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(2 * 9 * 9, num_actions_total)
        )
        # Slim value head: 2 planes, then flatten, then linear
        self.value_head = nn.Sequential(
            nn.Conv2d(tower_width, 2, 1),
            nn.BatchNorm2d(2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(2 * 9 * 9, 1)
        )
    def forward(self, x):
        x = F.relu(self.bn_stem(self.stem(x)))
        x = self.res_blocks(x)
        policy = self.policy_head(x)
        value = self.value_head(x).squeeze(-1)
        return policy, value

]]></file>
  <file path="training/models/__init__.py"><![CDATA[
from .resnet_tower import ActorCriticResTower


def model_factory(model_type, obs_shape, num_actions, tower_depth, tower_width, se_ratio, **kwargs):
    if model_type == "resnet":
        return ActorCriticResTower(
            input_channels=obs_shape[0],
            num_actions_total=num_actions,
            tower_depth=tower_depth,
            tower_width=tower_width,
            se_ratio=se_ratio,
            **kwargs
        )
    # Add dummy/test models for testing
    elif model_type in ["dummy", "testmodel", "resumemodel"]:
        # Use a simple version of ActorCriticResTower or a mock for testing
        # For now, let's use ActorCriticResTower with minimal fixed params
        # Ensure these params are sensible for a minimal test model
        return ActorCriticResTower(
            input_channels=obs_shape[0],  # Should come from feature_spec.num_planes
            num_actions_total=num_actions,
            tower_depth=1,  # Minimal depth
            tower_width=16,  # Minimal width
            se_ratio=None,  # No SE block for simplicity
            **kwargs
        )
    raise ValueError(f"Unknown model_type: {model_type}")

]]></file>
  <file path="shogi/shogi_engine.py"><![CDATA[
"""
This module re-exports the main classes from the refactored Shogi engine components
for backward compatibility.
"""

from .shogi_core_definitions import Color, MoveTuple, Piece, PieceType
from .shogi_game import ShogiGame

# These exports allow code that previously imported from shogi_engine.py
# to continue working without changes
__all__ = ["Color", "PieceType", "Piece", "ShogiGame", "MoveTuple"]

]]></file>
  <file path="shogi/features.py"><![CDATA[
"""
features.py: FeatureSpec registry and core46 observation builder for Keisei Shogi.
"""
from typing import Callable, Dict, List
import numpy as np

# Registry for feature builders
FEATURE_REGISTRY: Dict[str, Callable] = {}

def register_feature(name: str):
    def decorator(fn: Callable):
        FEATURE_REGISTRY[name] = fn
        return fn
    return decorator

class FeatureSpec:
    """
    Describes a set of feature planes for Shogi observation tensors.
    """
    def __init__(self, name: str, builder: Callable, num_planes: int):
        self.name = name
        self.builder = builder
        self.num_planes = num_planes

    def build(self, game) -> np.ndarray:
        return self.builder(game)

# --- Constants for extra planes ---
EXTRA_PLANES = {
    "check": 0,
    "repetition": 1,
    "prom_zone": 2,
    "last2ply": 3,
    "hand_onehot": 4,
}

# --- Core46 Feature Builder ---

@register_feature("core46")
def build_core46(game) -> np.ndarray:
    """
    Build the standard 46-plane observation tensor for the given game state.
    Args:
        game: ShogiGame instance
    Returns:
        obs: np.ndarray of shape (46, 9, 9)
    """
    # This implementation mirrors generate_neural_network_observation in shogi_game_io.py
    obs = np.zeros((46, 9, 9), dtype=np.float32)
    # --- Board pieces: current player POV ---
    for r in range(9):
        for c in range(9):
            piece = game.board[r][c]
            if piece is None:
                continue
            # Determine if piece belongs to current player or opponent
            is_curr = piece.color == game.current_player
            # Unpromoted
            if not piece.is_promoted():
                if is_curr:
                    idx = game.OBS_CURR_PLAYER_UNPROMOTED_START + game.OBS_UNPROMOTED_ORDER.index(piece.piece_type)
                else:
                    idx = game.OBS_OPP_PLAYER_UNPROMOTED_START + game.OBS_UNPROMOTED_ORDER.index(piece.piece_type)
            else:
                # Promoted
                if is_curr:
                    idx = game.OBS_CURR_PLAYER_PROMOTED_START + game.OBS_PROMOTED_ORDER.index(piece.piece_type)
                else:
                    idx = game.OBS_OPP_PLAYER_PROMOTED_START + game.OBS_PROMOTED_ORDER.index(piece.piece_type)
            obs[idx, r, c] = 1.0
    # --- Hand pieces ---
    for i, pt in enumerate(game.OBS_UNPROMOTED_ORDER):
        obs[game.OBS_CURR_PLAYER_HAND_START + i, :, :] = game.hands[game.current_player].get(pt, 0)
        opp = game.current_player.opponent() if hasattr(game.current_player, 'opponent') else (1 - game.current_player)
        obs[game.OBS_OPP_PLAYER_HAND_START + i, :, :] = game.hands[opp].get(pt, 0)
    # --- Meta planes ---
    obs[game.OBS_CURR_PLAYER_INDICATOR, :, :] = 1.0 if game.current_player == game.Color.BLACK else 0.0
    obs[game.OBS_MOVE_COUNT, :, :] = game.move_count / 512.0  # Normalize by max moves
    obs[game.OBS_RESERVED_1, :, :] = 0.0  # Always zero-filled
    obs[game.OBS_RESERVED_2, :, :] = 0.0  # Always zero-filled
    return obs

# --- Optional Feature Planes (T-2, refactored) ---

def add_check_plane(obs: np.ndarray, game, base_planes=46) -> None:
    idx = base_planes + EXTRA_PLANES["check"]
    if hasattr(game, 'is_in_check'):
        obs[idx, :, :] = 1.0 if game.is_in_check(game.current_player) else 0.0
    else:
        obs[idx, :, :] = 0.0

def add_repetition_plane(obs: np.ndarray, game, base_planes=46) -> None:
    idx = base_planes + EXTRA_PLANES["repetition"]
    # If game has a repetition count or sennichite detection, use it; else zeros
    if hasattr(game, 'repetition_count'):
        obs[idx, :, :] = min(game.repetition_count / 4.0, 1.0)  # Normalize to [0,1]
    elif hasattr(game, 'is_sennichite') and game.is_sennichite():
        obs[idx, :, :] = 1.0
    else:
        obs[idx, :, :] = 0.0

def add_prom_zone_plane(obs: np.ndarray, game, base_planes=46) -> None:
    idx = base_planes + EXTRA_PLANES["prom_zone"]
    # Mark promotion zone squares for current player (1.0 in zone, 0.0 elsewhere)
    zone_rows = [0, 1, 2] if game.current_player == game.Color.BLACK else [6, 7, 8]
    for r in zone_rows:
        obs[idx, r, :] = 1.0

def add_last2ply_plane(obs: np.ndarray, game, base_planes=46) -> None:
    idx = base_planes + EXTRA_PLANES["last2ply"]
    # Mark the destination squares of the last two moves (if available)
    if hasattr(game, 'move_history') and len(game.move_history) >= 1:
        for move in game.move_history[-2:]:
            if hasattr(move, 'to_square'):
                r, c = move.to_square
                obs[idx, r, c] = 1.0


def add_hand_onehot_plane(obs: np.ndarray, game, base_planes=46) -> None:
    idx = base_planes + EXTRA_PLANES["hand_onehot"]
    # Mark only [0,0] as 1 if any hand piece present for current player
    for pt in game.OBS_UNPROMOTED_ORDER:
        if game.hands[game.current_player].get(pt, 0) > 0:
            obs[idx, 0, 0] = 1.0
            break

@register_feature("core46+all")
def build_core46_all(game) -> np.ndarray:
    base_planes = 46
    obs = build_core46(game)
    obs = np.concatenate([obs, np.zeros((5, 9, 9), dtype=np.float32)], axis=0)
    add_check_plane(obs, game, base_planes)
    add_repetition_plane(obs, game, base_planes)
    add_prom_zone_plane(obs, game, base_planes)
    add_last2ply_plane(obs, game, base_planes)
    add_hand_onehot_plane(obs, game, base_planes)
    return obs

# Register FeatureSpec for all feature sets
CORE46_SPEC = FeatureSpec("core46", build_core46, 46)
CORE46_ALL_SPEC = FeatureSpec("core46+all", build_core46_all, 51)

# Dummy specs for testing
DUMMY_FEATS_SPEC = FeatureSpec("dummyfeats", build_core46, 46)
TEST_FEATS_SPEC = FeatureSpec("testfeats", build_core46, 46)
RESUME_FEATS_SPEC = FeatureSpec("resumefeats", build_core46, 46) # Add resumefeats

FEATURE_SPECS = {
    "core46": CORE46_SPEC,
    "core46+all": CORE46_ALL_SPEC,
    "dummyfeats": DUMMY_FEATS_SPEC,
    "testfeats": TEST_FEATS_SPEC,
    "resumefeats": RESUME_FEATS_SPEC, # Add resumefeats
}

]]></file>
  <file path="shogi/shogi_game.py"><![CDATA[
"""
shogi_game.py: Main ShogiGame class for DRL Shogi Client.
Orchestrates game state and delegates complex logic to helper modules.
"""

# pylint: disable=too-many-lines

import copy  # Added for __deepcopy__
import re  # Added for SFEN parsing
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union

import numpy as np

# Import helper modules
from . import shogi_game_io, shogi_move_execution, shogi_rules_logic

# Import types and fundamental classes from shogi_core_definitions
from .shogi_core_definitions import BASE_TO_PROMOTED_TYPE  # For SFEN deserialization
from .shogi_core_definitions import PIECE_TYPE_TO_HAND_TYPE  # Used in add_to_hand
from .shogi_core_definitions import PROMOTED_TYPES_SET  # For SFEN serialization
from .shogi_core_definitions import SYMBOL_TO_PIECE_TYPE  # Added for SFEN parsing
from .shogi_core_definitions import MoveTuple  # Already imported above
from .shogi_core_definitions import (
    Color,
    Piece,
    PieceType,
    get_unpromoted_types,
)

if TYPE_CHECKING:
    # from .shogi_core_definitions import MoveTuple # Already imported above
    pass


class ShogiGame:
    """
    Represents the Shogi game state, board, and operations.
    Delegates complex rule logic, I/O, and move execution to helper modules.
    """

    _SFEN_BOARD_CHARS: Dict[PieceType, str] = {
        PieceType.PAWN: "P",
        PieceType.LANCE: "L",
        PieceType.KNIGHT: "N",
        PieceType.SILVER: "S",
        PieceType.GOLD: "G",
        PieceType.BISHOP: "B",
        PieceType.ROOK: "R",
        PieceType.KING: "K",
        PieceType.PROMOTED_PAWN: "P",  # Note: SFEN uses the base piece char for promoted pieces on board, promotion is indicated by '+'
        PieceType.PROMOTED_LANCE: "L",
        PieceType.PROMOTED_KNIGHT: "N",
        PieceType.PROMOTED_SILVER: "S",
        PieceType.PROMOTED_BISHOP: "B",
        PieceType.PROMOTED_ROOK: "R",
    }

    def __init__(
        self, max_moves_per_game: int = 500
    ) -> None:  # Added default for max_moves_per_game
        self.board: List[List[Optional[Piece]]]
        self.hands: Dict[int, Dict[PieceType, int]]
        self.current_player: Color = Color.BLACK
        self.move_count: int = 0
        self.game_over: bool = False
        self.winner: Optional[Color] = None
        self.termination_reason: Optional[str] = None
        self.move_history: List[Dict[str, Any]] = []
        self.board_history: List[Tuple] = []  # Added board_history
        self._max_moves_this_game = max_moves_per_game
        self._initial_board_setup_done = False
        self.reset()

    @property
    def max_moves_per_game(self) -> int:
        return self._max_moves_this_game

    def _setup_initial_board(self):
        """Sets up the board to the standard initial Shogi position."""
        self.board = [[None for _ in range(9)] for _ in range(9)]

        for i in range(9):
            self.board[2][i] = Piece(PieceType.PAWN, Color.WHITE)
            self.board[6][i] = Piece(PieceType.PAWN, Color.BLACK)

        self.board[0][0] = Piece(PieceType.LANCE, Color.WHITE)
        self.board[0][1] = Piece(PieceType.KNIGHT, Color.WHITE)
        self.board[0][2] = Piece(PieceType.SILVER, Color.WHITE)
        self.board[0][3] = Piece(PieceType.GOLD, Color.WHITE)
        self.board[0][4] = Piece(PieceType.KING, Color.WHITE)
        self.board[0][5] = Piece(PieceType.GOLD, Color.WHITE)
        self.board[0][6] = Piece(PieceType.SILVER, Color.WHITE)
        self.board[0][7] = Piece(PieceType.KNIGHT, Color.WHITE)
        self.board[0][8] = Piece(PieceType.LANCE, Color.WHITE)

        self.board[1][1] = Piece(PieceType.ROOK, Color.WHITE)
        self.board[1][7] = Piece(PieceType.BISHOP, Color.WHITE)

        self.board[8][0] = Piece(PieceType.LANCE, Color.BLACK)
        self.board[8][1] = Piece(PieceType.KNIGHT, Color.BLACK)
        self.board[8][2] = Piece(PieceType.SILVER, Color.BLACK)
        self.board[8][3] = Piece(PieceType.GOLD, Color.BLACK)
        self.board[8][4] = Piece(PieceType.KING, Color.BLACK)
        self.board[8][5] = Piece(PieceType.GOLD, Color.BLACK)
        self.board[8][6] = Piece(PieceType.SILVER, Color.BLACK)
        self.board[8][7] = Piece(PieceType.KNIGHT, Color.BLACK)
        self.board[8][8] = Piece(PieceType.LANCE, Color.BLACK)

        self.board[7][1] = Piece(PieceType.BISHOP, Color.BLACK)
        self.board[7][7] = Piece(PieceType.ROOK, Color.BLACK)

    def reset(self) -> np.ndarray:  # MODIFIED: Return np.ndarray
        """Resets the game to the initial state and returns the observation."""  # MODIFIED: Docstring
        self._setup_initial_board()
        self.hands = {
            Color.BLACK.value: {ptype: 0 for ptype in get_unpromoted_types()},
            Color.WHITE.value: {ptype: 0 for ptype in get_unpromoted_types()},
        }
        self.current_player = Color.BLACK
        self.move_count = 0
        self.game_over = False
        self.winner = None
        self.termination_reason = None
        self.move_history = []
        self.board_history = [
            self._board_state_hash()
        ]  # Initialize with starting position hash
        self._initial_board_setup_done = True
        return self.get_observation()  # MODIFIED: Return observation

    def get_piece(self, row: int, col: int) -> Optional[Piece]:
        """Returns the piece at the specified position, or None if empty or out of bounds."""
        if self.is_on_board(row, col):
            return self.board[row][col]
        return None

    def set_piece(self, row: int, col: int, piece: Optional[Piece]) -> None:
        """Sets or removes a piece at the specified position on the board."""
        if self.is_on_board(row, col):
            self.board[row][col] = piece

    def to_string(self) -> str:
        """Returns a string representation of the current board state."""
        return shogi_game_io.convert_game_to_text_representation(self)

    def is_on_board(self, row: int, col: int) -> bool:
        return 0 <= row < 9 and 0 <= col < 9

    def __deepcopy__(self, memo: Dict[int, Any]):  # Added type hint for memo
        if id(self) in memo:
            return memo[id(self)]

        cls = self.__class__
        result = cls.__new__(cls)
        memo[id(self)] = result

        result.board = copy.deepcopy(self.board, memo)
        result.hands = copy.deepcopy(self.hands, memo)
        result.current_player = self.current_player
        result.move_count = self.move_count
        result.game_over = self.game_over
        result.winner = self.winner
        result.termination_reason = self.termination_reason
        result.move_history = []
        result._max_moves_this_game = self._max_moves_this_game
        result._initial_board_setup_done = self._initial_board_setup_done
        # Ensure all attributes are set on result before calling _board_state_hash
        result.board_history = [result._board_state_hash()]

        return result

    def _is_sliding_piece_type(self, piece_type: PieceType) -> bool:
        return shogi_rules_logic.is_piece_type_sliding(piece_type)

    def get_individual_piece_moves(
        self, piece: Piece, r_from: int, c_from: int
    ) -> List[Tuple[int, int]]:  # Changed from list[tuple[int, int]]
        return shogi_rules_logic.generate_piece_potential_moves(
            self, piece, r_from, c_from
        )

    def get_observation(self) -> np.ndarray:
        """
        Generates the neural network observation for the current game state.

        The observation is a multi-channel NumPy array representing the board,
        hands, and other game metadata from the current player's perspective.
        For detailed structure, see `shogi_game_io.generate_neural_network_observation`.

        Returns:
            np.ndarray: The observation array.
        """
        return shogi_game_io.generate_neural_network_observation(self)

    def is_nifu(self, color: Color, col: int) -> bool:
        return shogi_rules_logic.check_for_nifu(self, color, col)

    def is_uchi_fu_zume(self, drop_row: int, drop_col: int, color: Color) -> bool:
        return shogi_rules_logic.check_for_uchi_fu_zume(self, drop_row, drop_col, color)

    def _is_square_attacked(self, row: int, col: int, attacker_color: Color) -> bool:
        return shogi_rules_logic.check_if_square_is_attacked(
            self, row, col, attacker_color
        )

    def get_legal_moves(self) -> List[MoveTuple]:  # Changed from List["MoveTuple"]
        moves = shogi_rules_logic.generate_all_legal_moves(self)
        return moves

    def _king_in_check_after_move(self, player_color: Color) -> bool:
        return shogi_rules_logic.is_king_in_check_after_simulated_move(
            self, player_color
        )

    def find_king(self, color: Color) -> Optional[Tuple[int, int]]:
        """Finds the king of the specified color on the board."""
        # Delegate to the rules logic function
        return shogi_rules_logic.find_king(self, color)

    def is_in_check(
        self, color: Color, debug_recursion: bool = False
    ) -> bool:  # Added debug_recursion
        """Checks if the specified player is in check."""
        # Delegate to the rules logic function
        return shogi_rules_logic.is_in_check(
            self, color, debug_recursion=debug_recursion
        )  # Pass debug flag

    # --- SFEN Encoding Helper Methods ---
    def _sfen_sq(self, r: int, c: int) -> str:
        """Converts 0-indexed (row, col) to SFEN square string (e.g., (0,0) -> "9a")."""
        if not (0 <= r <= 8 and 0 <= c <= 8):
            raise ValueError(f"Invalid Shogi coordinate for SFEN: row {r}, col {c}")
        file = str(9 - c)
        rank = chr(ord("a") + r)
        return file + rank

    def _get_sfen_drop_char(self, piece_type: PieceType) -> str:
        """Helper to get the uppercase SFEN character for a droppable piece type."""
        # Using a mapping for clarity and directness
        sfen_char_map: Dict[PieceType, str] = {
            PieceType.PAWN: "P",
            PieceType.LANCE: "L",
            PieceType.KNIGHT: "N",
            PieceType.SILVER: "S",
            PieceType.GOLD: "G",
            PieceType.BISHOP: "B",
            PieceType.ROOK: "R",
        }
        char = sfen_char_map.get(piece_type)
        if char is None:
            raise ValueError(
                f"PieceType {piece_type.name if hasattr(piece_type, 'name') else piece_type} "
                f"is not a standard droppable piece for SFEN notation or is invalid."
            )
        return char

    def sfen_encode_move(
        self, move_tuple: MoveTuple
    ) -> str:  # Changed from "MoveTuple"
        """
        Encodes a move in SFEN (Shogi Forsyth-Edwards Notation) format.
        Board move: (from_r, from_c, to_r, to_c, promote_bool) -> e.g., "7g7f" or "2b3a+"
        Drop move: (None, None, to_r, to_c, piece_type) -> e.g., "P*5e"
        """
        if (
            len(move_tuple) == 5
            and isinstance(move_tuple[0], int)
            and isinstance(move_tuple[1], int)
            and isinstance(move_tuple[2], int)
            and isinstance(move_tuple[3], int)
            and isinstance(move_tuple[4], bool)
        ):
            from_r, from_c, to_r, to_c, promote = (
                move_tuple[0],
                move_tuple[1],
                move_tuple[2],
                move_tuple[3],
                move_tuple[4],
            )
            from_sq_str = self._sfen_sq(from_r, from_c)
            to_sq_str = self._sfen_sq(to_r, to_c)
            promo_char = "+" if promote else ""
            return f"{from_sq_str}{to_sq_str}{promo_char}"
        elif (
            len(move_tuple) == 5
            and move_tuple[0] is None
            and move_tuple[1] is None
            and isinstance(move_tuple[2], int)
            and isinstance(move_tuple[3], int)
            and isinstance(move_tuple[4], PieceType)
        ):
            _, _, to_r, to_c, piece_to_drop = (
                move_tuple[0],
                move_tuple[1],
                move_tuple[2],
                move_tuple[3],
                move_tuple[4],
            )
            piece_char = self._get_sfen_drop_char(piece_to_drop)
            to_sq_str = self._sfen_sq(to_r, to_c)
            return f"{piece_char}*{to_sq_str}"
        else:
            element_types = [type(el).__name__ for el in move_tuple]
            element_values = [str(el) for el in move_tuple]
            raise ValueError(
                f"Invalid MoveTuple format for SFEN conversion: {move_tuple}. "
                f"Types: {element_types}. Values: {element_values}."
            )

    # --- SFEN Game State Serialization ---
    def _get_sfen_board_char(self, piece: Piece) -> str:
        """Helper to get the SFEN character for a piece on the board."""
        if not isinstance(piece, Piece):
            raise TypeError(f"Expected a Piece object, got {type(piece)}")

        # Get the base character (e.g., 'P' for PAWN and PROMOTED_PAWN)
        base_char = self._SFEN_BOARD_CHARS.get(piece.type)
        if base_char is None:
            # This handles cases like KING which don't have a separate entry for a promoted type
            # but are in _SFEN_BOARD_CHARS. It mainly catches truly unknown types.
            raise ValueError(
                f"Unknown piece type for SFEN board character: {piece.type}"
            )

        sfen_char = ""
        if piece.type in PROMOTED_TYPES_SET:
            sfen_char += "+"
        sfen_char += base_char

        if piece.color == Color.WHITE:
            return sfen_char.lower()
        return sfen_char

    def to_sfen_string(self) -> str:
        """
        Serializes the current game state to an SFEN string.
        Format: <board> <turn> <hands> <move_number>
        Example: lnsgkgsnl/1r5b1/ppppppppp/9/9/9/PPPPPPPPP/1B5R1/LNSGKGSNL b - 1
        """
        sfen_ranks = []
        # Board: ranks 1-9 (board[0] to board[8]), files 1-9 (col[8] to col[0])
        for r in range(9):  # board row 0 (SFEN rank 1) to board row 8 (SFEN rank 9)
            empty_squares_count = 0
            sfen_rank_str = ""
            # Iterate from file 9 (board column 0) down to file 1 (board column 8) for SFEN
            # Our internal board is board[row][col] where col 0 is file 9 (leftmost from Black's view)
            # So we iterate columns from 0 to 8 for our board, which corresponds to SFEN files 9 to 1.
            for c in range(9):  # Iterate through columns 0 to 8 (SFEN files 9 to 1)
                piece = self.board[r][c]
                if piece:
                    if empty_squares_count > 0:
                        sfen_rank_str += str(empty_squares_count)
                        empty_squares_count = 0
                    sfen_rank_str += self._get_sfen_board_char(piece)
                else:
                    empty_squares_count += 1

            if empty_squares_count > 0:  # Append any trailing empty count for the rank
                sfen_rank_str += str(empty_squares_count)
            sfen_ranks.append(sfen_rank_str)
        board_sfen = "/".join(sfen_ranks)

        # Player turn
        turn_sfen = "b" if self.current_player == Color.BLACK else "w"

        # Hands
        # SFEN standard hand piece order: R, B, G, S, N, L, P.
        # Uppercase for Black, lowercase for White.
        # If a player has multiple pieces of the same type, the number precedes the piece character (e.g., 2P).
        # If no pieces in hand, it's "-".

        # Canonical order for pieces in hand as per many SFEN implementations/expectations.
        # Though the spec might be flexible, tests often expect this order.
        SFEN_HAND_PIECE_CANONICAL_ORDER = [
            PieceType.ROOK,
            PieceType.BISHOP,
            PieceType.GOLD,
            PieceType.SILVER,
            PieceType.KNIGHT,
            PieceType.LANCE,
            PieceType.PAWN,
        ]

        hand_sfen_parts = []
        has_black_pieces = False
        # Black's hand (uppercase)
        for piece_type in SFEN_HAND_PIECE_CANONICAL_ORDER:
            count = self.hands[Color.BLACK.value].get(piece_type, 0)
            if count > 0:
                has_black_pieces = True
                sfen_char = self._SFEN_BOARD_CHARS[
                    piece_type
                ]  # Should be uppercase by convention from _SFEN_BOARD_CHARS
                if count > 1:
                    hand_sfen_parts.append(str(count))
                hand_sfen_parts.append(sfen_char)

        has_white_pieces = False
        # White's hand (lowercase)
        for piece_type in SFEN_HAND_PIECE_CANONICAL_ORDER:
            count = self.hands[Color.WHITE.value].get(piece_type, 0)
            if count > 0:
                has_white_pieces = True
                sfen_char_upper = self._SFEN_BOARD_CHARS[piece_type]
                sfen_char = sfen_char_upper.lower()  # Ensure lowercase for White
                if count > 1:
                    hand_sfen_parts.append(str(count))
                hand_sfen_parts.append(sfen_char)

        hands_sfen = (
            "".join(hand_sfen_parts) if (has_black_pieces or has_white_pieces) else "-"
        )

        # Move number (1-indexed)
        # self.move_count is 0 for the first move to be made, so SFEN move number is move_count + 1
        move_num_sfen = str(self.move_count + 1)

        return f"{board_sfen} {turn_sfen} {hands_sfen} {move_num_sfen}"

    # --- SFEN Game State Deserialization ---

    @staticmethod
    def _parse_sfen_board_piece(
        sfen_char_on_board: str, is_promoted_sfen_token: bool
    ) -> Tuple[PieceType, Color]:
        """
        Parses an SFEN board piece character (e.g., 'P', 'l', 'R') and promotion status
        into (PieceType, Color).
        `sfen_char_on_board` is the actual piece letter (e.g., 'P' from "+P", or 'K').
        `is_promoted_sfen_token` is True if a '+' preceded this character in SFEN.
        Returns (PieceType, Color)
        """
        color = Color.BLACK if "A" <= sfen_char_on_board <= "Z" else Color.WHITE

        base_char_upper = sfen_char_on_board.upper()
        base_piece_type = SYMBOL_TO_PIECE_TYPE.get(base_char_upper)

        if base_piece_type is None:
            raise ValueError(
                f"Invalid SFEN piece character for board: {sfen_char_on_board}"
            )

        if is_promoted_sfen_token:
            if base_piece_type in BASE_TO_PROMOTED_TYPE:
                final_piece_type = BASE_TO_PROMOTED_TYPE[base_piece_type]
            elif (
                base_piece_type in PROMOTED_TYPES_SET
            ):  # Already a promoted type, e.g. if SYMBOL_TO_PIECE_TYPE mapped +P directly
                final_piece_type = base_piece_type
            else:
                # '+' was applied to a non-promotable piece like King or Gold
                raise ValueError(
                    f"Invalid promotion: SFEN token '+' applied to non-promotable piece type {base_piece_type.name} (from char '{sfen_char_on_board}')"
                )
        else:  # Not a promoted token
            # If the base_piece_type itself is a promoted type (e.g. if _SFEN_BOARD_CHARS had +P: P and no plain P:P)
            # this would be an issue, but SYMBOL_TO_PIECE_TYPE should map to base types.
            if base_piece_type in PROMOTED_TYPES_SET:
                raise ValueError(
                    f"Invalid SFEN: Character '{sfen_char_on_board}' (mapped to {base_piece_type.name}) implies promotion, but no '+' prefix found."
                )
            final_piece_type = base_piece_type

        return final_piece_type, color

    @classmethod
    def from_sfen(
        cls, sfen_str: str, max_moves_for_game_instance: int = 500
    ) -> "ShogiGame":
        """Loads a game state from an SFEN string."""
        sfen_pattern = re.compile(r"^\s*([^ ]+)\s+([bw])\s+([^ ]+)\s+(\d+)\s*$")
        match = sfen_pattern.match(sfen_str.strip())
        if not match:
            raise ValueError(f"Invalid SFEN string structure: '{sfen_str}'")

        board_sfen, turn_sfen, hands_sfen, move_number_sfen = match.groups()
        current_player = Color.BLACK if turn_sfen == "b" else Color.WHITE

        try:
            move_number = int(move_number_sfen)
            if move_number < 1:
                raise ValueError("SFEN move number must be positive")
        except ValueError as e:
            if "SFEN move number must be positive" in str(e):
                raise
            raise ValueError(
                f"Invalid move number in SFEN: '{move_number_sfen}'"
            ) from e

        game = cls()
        game.current_player = current_player
        game._max_moves_this_game = max_moves_for_game_instance
        game.move_count = move_number - 1
        game.board = [[None for _ in range(9)] for _ in range(9)]
        game.hands = {
            Color.BLACK.value: {ptype: 0 for ptype in get_unpromoted_types()},
            Color.WHITE.value: {ptype: 0 for ptype in get_unpromoted_types()},
        }
        game.move_history = []
        game.board_history = []

        # Parse board
        rows = board_sfen.split("/")
        if len(rows) != 9:
            raise ValueError("Expected 9 ranks")

        for r, row_str in enumerate(rows):
            c = 0
            promoted_flag_active = False
            while c < 9 and row_str:
                char_sfen = row_str[0]
                row_str = row_str[1:]
                if char_sfen == "+":
                    if promoted_flag_active:
                        raise ValueError(
                            "Invalid piece character sequence starting with '+'"
                        )
                    promoted_flag_active = True
                    continue
                elif char_sfen.isdigit():
                    if promoted_flag_active:
                        raise ValueError(
                            f"Invalid SFEN: Digit ('{char_sfen}') cannot immediately follow a promotion token ('+')."
                        )
                    if char_sfen == "0":
                        raise ValueError("Invalid SFEN piece character for board: 0")
                    empty_squares = int(char_sfen)
                    if not 1 <= empty_squares <= 9 - c:
                        raise ValueError(
                            f"Row {r+1} ('{row_str}') describes {c+empty_squares} columns, expected 9"
                        )
                    c += empty_squares
                else:
                    piece_char_upper = char_sfen.upper()
                    base_piece_type = SYMBOL_TO_PIECE_TYPE.get(piece_char_upper)
                    if base_piece_type is None:
                        raise ValueError(
                            f"Invalid SFEN piece character for board: {char_sfen}"
                        )
                    piece_color = Color.BLACK if char_sfen.isupper() else Color.WHITE
                    final_piece_type = base_piece_type
                    if promoted_flag_active:
                        if base_piece_type in BASE_TO_PROMOTED_TYPE:
                            final_piece_type = BASE_TO_PROMOTED_TYPE[base_piece_type]
                            # Removed unused variable 'is_actually_promoted'
                        elif base_piece_type in PROMOTED_TYPES_SET:
                            raise ValueError(
                                f"Invalid promotion: SFEN token '+' applied to non-promotable piece type {base_piece_type.name}"
                            )
                        else:
                            raise ValueError(
                                f"Invalid promotion: SFEN token '+' applied to non-promotable piece type {base_piece_type.name}"
                            )
                    elif (
                        not promoted_flag_active
                        and base_piece_type in PROMOTED_TYPES_SET
                    ):
                        raise ValueError(
                            f"Invalid SFEN piece character for board: {char_sfen}"
                        )
                    game.board[r][c] = Piece(final_piece_type, piece_color)
                    # is_promoted is automatically handled by the Piece constructor
                    # based on final_piece_type. No explicit setting needed here.
                    c += 1
                    promoted_flag_active = False
            if c != 9:
                raise ValueError(
                    f"Row {r+1} ('{rows[r]}') describes {c} columns, expected 9"
                )

        # Parse hands
        if hands_sfen != "-":
            hand_segment_pattern = re.compile(r"(\d*)([PLNSGBRplnsgbr])")
            pos = 0
            parsing_white_hand_pieces = False
            while pos < len(hands_sfen):
                match_hand = hand_segment_pattern.match(hands_sfen, pos)
                if not match_hand:
                    if hands_sfen[pos:].startswith("K") or hands_sfen[pos:].startswith(
                        "k"
                    ):
                        raise ValueError(
                            "Invalid piece character 'K' or non-droppable piece type in SFEN hands"
                        )
                    raise ValueError("Invalid character sequence in SFEN hands")
                count_str, piece_char = match_hand.groups()
                count = int(count_str) if count_str else 1

                is_current_piece_white = piece_char.islower()
                is_current_piece_black = piece_char.isupper()

                if is_current_piece_white:
                    parsing_white_hand_pieces = True
                elif is_current_piece_black and parsing_white_hand_pieces:
                    raise ValueError(
                        "Invalid SFEN hands: Black's pieces must precede White's pieces."
                    )

                try:
                    piece_type_in_hand = SYMBOL_TO_PIECE_TYPE[piece_char.upper()]
                    hand_color = Color.BLACK if is_current_piece_black else Color.WHITE
                except KeyError as e:
                    raise ValueError("Invalid character sequence in SFEN hands") from e
                if piece_type_in_hand == PieceType.KING:
                    raise ValueError(
                        "Invalid piece character 'K' or non-droppable piece type in SFEN hands"
                    )
                if piece_type_in_hand in PROMOTED_TYPES_SET:
                    raise ValueError("Invalid character sequence in SFEN hands")
                current_hand_for_color = game.hands[hand_color.value]
                current_hand_for_color[piece_type_in_hand] = (
                    current_hand_for_color.get(piece_type_in_hand, 0) + count
                )
                pos = match_hand.end()
        game._initial_board_setup_done = True
        game.board_history.append(game._board_state_hash())

        # Evaluate termination conditions for the position
        # Similar to what's done in apply_move_to_board but without actually making a move
        # This is needed because from_sfen doesn't call apply_move_to_board
        king_in_check = shogi_rules_logic.is_in_check(game, game.current_player)
        legal_moves = shogi_rules_logic.generate_all_legal_moves(game)

        if not legal_moves:
            if king_in_check:
                game.game_over = True
                # In checkmate (tsumi), the opponent wins
                game.winner = (
                    Color.WHITE if game.current_player == Color.BLACK else Color.BLACK
                )
                game.termination_reason = "Tsumi"
            else:
                game.game_over = True
                game.winner = None  # Stalemate means no winner
                game.termination_reason = "Stalemate"
        elif shogi_rules_logic.check_for_sennichite(game):
            game.game_over = True
            game.winner = None
            game.termination_reason = "Sennichite"

        # Removed hardcoded SFEN string checks for termination.
        # The general logic above should handle game termination.

        return game

    def _board_state_hash(self) -> tuple:
        """
        Returns a hashable representation of the current board state, hands, and player to move.
        Used for checking for repetition (sennichite).
        """
        board_tuple = tuple(
            tuple((p.type.value, p.color.value) if p else None for p in row)
            for row in self.board
        )
        hands_tuple = (
            tuple(
                sorted(
                    (pt.value, count)
                    for pt, count in self.hands[Color.BLACK.value].items()
                    if count > 0
                )
            ),
            tuple(
                sorted(
                    (pt.value, count)
                    for pt, count in self.hands[Color.WHITE.value].items()
                    if count > 0
                )
            ),
        )
        return (board_tuple, hands_tuple, self.current_player.value)

    def get_board_state_hash(self) -> tuple:
        """
        Public interface to access the board state hash.
        """
        return self._board_state_hash()

    def is_sennichite(self) -> bool:
        """
        Checks if the current position has occurred four times, resulting in a draw.
        """
        return shogi_rules_logic.check_for_sennichite(self)

    def make_move(
        self, move_tuple: MoveTuple, is_simulation: bool = False
    ) -> Union[Dict[str, Any], Tuple[np.ndarray, float, bool, Dict[str, Any]]]:
        """
        Applies a move, updates history, and delegates to shogi_move_execution.
        This is the primary method for making a move.

        Args:
            move_tuple: The move to apply
            is_simulation: If True, this is a simulation move for legal move checking

        Returns:
            If is_simulation is True: move details dictionary
            Otherwise: A 4-tuple (observation, reward, done, info) for RL training
        """
        if self.game_over and not is_simulation:
            # Game is over, return the current state, reward 0, done True, and termination info
            next_obs = self.get_observation()
            reward = 0.0  # No reward for trying to move in a completed game
            done = True
            info = {
                "termination_reason": (
                    self.termination_reason
                    if self.termination_reason
                    else "Game already over"
                )
            }
            return next_obs, reward, done, info

        player_who_made_the_move = self.current_player
        move_count_before_move = self.move_count

        # Validate move_tuple structure early
        if not (
            isinstance(move_tuple, tuple)
            and len(move_tuple) == 5
            and (
                (  # Board move
                    isinstance(move_tuple[0], int)
                    and isinstance(move_tuple[1], int)
                    and isinstance(move_tuple[2], int)
                    and isinstance(move_tuple[3], int)
                    and isinstance(move_tuple[4], bool)
                )
                or (  # Drop move
                    move_tuple[0] is None  # Allow None for drop move
                    and move_tuple[1] is None  # Allow None for drop move
                    and isinstance(move_tuple[2], int)
                    and isinstance(move_tuple[3], int)
                    and isinstance(move_tuple[4], PieceType)
                )
            )
        ):
            raise ValueError(f"Invalid move_tuple format: {move_tuple}")

        r_from, c_from, r_to, c_to = (
            move_tuple[0],
            move_tuple[1],
            move_tuple[2],
            move_tuple[3],
        )

        move_details_for_history: Dict[str, Any] = {
            "move": move_tuple,
            "is_drop": False,  # Default to False, override for drops
            "captured": None,
            "was_promoted_in_move": False,
            "original_type_before_promotion": None,  # For board moves
            "dropped_piece_type": None,  # For drop moves
            "original_color_of_moved_piece": None,  # For board moves, to aid undo
            "player_who_made_the_move": player_who_made_the_move,  # RESTORED COMMA HERE
            "move_count_before_move": move_count_before_move,
        }

        # --- Part 1: Gather details for history & perform initial piece manipulation ---
        # This part happens *before* calling apply_move_to_board,
        # so we have the state *before* the piece is moved/dropped.

        if r_from is None:  # Drop move
            move_details_for_history["is_drop"] = True
            if isinstance(move_tuple[4], PieceType):
                drop_piece_type_for_move = move_tuple[4]
                move_details_for_history["dropped_piece_type"] = (
                    drop_piece_type_for_move
                )
                # The actual board update and hand removal will happen in Part 2 for consistency
            else:
                raise ValueError(
                    f"Invalid drop move: move_tuple[4] is not a PieceType: {move_tuple[4]}"
                )
        else:  # Board move
            if (
                r_from is not None and c_from is not None
            ):  # Should always be true for board move
                piece_to_move = self.get_piece(r_from, c_from)
            else:
                # This case should ideally be caught by the tuple validation earlier
                raise ValueError("Invalid board move: r_from or c_from is None")

            if piece_to_move is None:
                raise ValueError(
                    f"Invalid move: No piece at source ({r_from},{c_from})"
                )
            if piece_to_move.color != player_who_made_the_move:
                raise ValueError(
                    f"Invalid move: Piece at ({r_from},{c_from}) does not belong to current player."
                )

            # --- ADDED: Hard-fail for illegal movement pattern ---
            potential_squares = shogi_rules_logic.generate_piece_potential_moves(
                self, piece_to_move, r_from, c_from
            )
            if (r_to, c_to) not in potential_squares:
                raise ValueError(
                    f"Illegal movement pattern: {piece_to_move.type.name} at "
                    f"({r_from},{c_from}) cannot move to ({r_to},{c_to}). "
                    f"Potential squares: {potential_squares}"
                )
            # --- END ADDED ---

            move_details_for_history["original_type_before_promotion"] = (
                piece_to_move.type
            )
            move_details_for_history["original_color_of_moved_piece"] = (
                piece_to_move.color
            )

            if r_to is not None and c_to is not None:  # Should always be true
                target_piece_on_board = self.get_piece(r_to, c_to)
            else:
                # This case should ideally be caught by the tuple validation earlier
                raise ValueError("Invalid board move: r_to or c_to is None")

            if target_piece_on_board:
                if target_piece_on_board.color == player_who_made_the_move:
                    raise ValueError(
                        f"Invalid move: Cannot capture own piece at ({r_to},{c_to})"
                    )
                move_details_for_history["captured"] = copy.deepcopy(
                    target_piece_on_board
                )

            promote_flag = move_tuple[4]
            if (
                isinstance(promote_flag, bool) and promote_flag
            ):  # Ensure promote_flag is bool for board moves
                if not shogi_rules_logic.can_promote_specific_piece(
                    self, piece_to_move, r_from, r_to
                ):
                    raise ValueError("Invalid promotion.")
                move_details_for_history["was_promoted_in_move"] = True
            elif not isinstance(promote_flag, bool):
                raise ValueError(
                    f"Invalid promotion flag type for board move: {type(promote_flag)}"
                )

        # --- Part 2: Execute the move on the board ---
        if move_details_for_history["is_drop"]:
            if isinstance(move_tuple[4], PieceType):
                drop_piece_type = move_tuple[4]
                if (
                    r_to is not None and c_to is not None
                ):  # Should always be true for drop
                    self.set_piece(
                        r_to, c_to, Piece(drop_piece_type, player_who_made_the_move)
                    )
                self.remove_from_hand(drop_piece_type, player_who_made_the_move)
            # Error case for invalid type already handled in Part 1
        else:  # Board move
            # piece_to_move was fetched in Part 1 and validated
            # r_from, c_from, r_to, c_to are validated to be not None for board moves
            if r_from is None or c_from is None:  # Add assertion for type checker
                raise RuntimeError(
                    "r_from and c_from should not be None for a board move at this stage."
                )
            current_piece_to_move = self.get_piece(
                r_from, c_from
            )  # Get it again, as it might be needed for promotion logic
            if current_piece_to_move is None:  # Should not happen due to prior checks
                raise RuntimeError(
                    f"Consistency check failed: piece at ({r_from},{c_from}) disappeared before move execution"
                )

            # Handle capture by adding to hand
            if move_details_for_history["captured"]:
                captured_p: Piece = move_details_for_history["captured"]
                self.add_to_hand(captured_p, player_who_made_the_move)

            # Move the piece
            # Ensure r_to and c_to are not None (already validated by move_tuple structure check)
            if r_to is None or c_to is None:  # Should ideally not be reached
                raise ValueError(
                    "Invalid board move: r_to or c_to is None during piece placement."
                )
            self.set_piece(r_to, c_to, current_piece_to_move)  # type: ignore

            # Clear original square
            # Ensure r_from and c_from are not None (already validated)
            if r_from is None or c_from is None:  # Should ideally not be reached
                raise ValueError(
                    "Invalid board move: r_from or c_from is None during piece removal."
                )
            self.set_piece(r_from, c_from, None)

            # Handle promotion
            if move_details_for_history["was_promoted_in_move"]:
                # r_to, c_to are known to be not None for board moves
                piece_at_dest = self.get_piece(r_to, c_to)
                if piece_at_dest:  # Should exist as we just placed it
                    piece_at_dest.promote()
                else:  # Should not happen
                    raise RuntimeError(
                        "Consistency check failed: piece_at_dest is None after move for promotion"
                    )

        # --- Part 3: Update history and game state (delegating parts to shogi_move_execution) ---
        # Store state hash *after* the move is made on the board, but *before* player switch.
        # The hash should reflect the board, hands, and the player *who just made the move*.
        current_state_hash = self._board_state_hash()
        move_details_for_history["state_hash"] = current_state_hash

        if not is_simulation:
            self.move_history.append(move_details_for_history)
            # board_history is for sennichite and should store the hash of the state
            # *after* the move, associated with the player who made it.
            self.board_history.append(current_state_hash)

        # Store who made the move before we switch players
        player_who_made_the_move = self.current_player

        # Call apply_move_to_board to switch player, increment move count, and check game end.
        # Pass the original move_tuple as it might be used by apply_move_to_board for its logic.
        shogi_move_execution.apply_move_to_board(self, is_simulation)

        if is_simulation:
            return move_details_for_history

        # For training, return a 4-tuple (observation, reward, done, info)
        next_obs = self.get_observation()
        reward = self.get_reward(
            player_who_made_the_move
        )  # Get reward from perspective of the player who moved
        done = self.game_over
        info = (
            {"termination_reason": self.termination_reason}
            if self.termination_reason
            else {}
        )

        return next_obs, reward, done, info

    def undo_move(
        self, simulation_undo_details: Optional[Dict[str, Any]] = None
    ) -> None:  # Added return type hint & param
        """
        Reverts the last move made, restoring the previous game state.
        Can use simulation_undo_details to undo a simulated move not in history.
        """
        shogi_move_execution.revert_last_applied_move(self, simulation_undo_details)

    def add_to_hand(self, captured_piece: Piece, capturing_player_color: Color) -> None:
        """
        Adds a captured piece (as unpromoted) to the capturing player's hand.
        """
        if captured_piece.type == PieceType.KING:
            return

        hand_piece_type = PIECE_TYPE_TO_HAND_TYPE.get(captured_piece.type)
        if hand_piece_type is None:
            raise ValueError(
                f"Invalid piece type {captured_piece.type} to add to hand."
            )

        self.hands[capturing_player_color.value][hand_piece_type] = (
            self.hands[capturing_player_color.value].get(hand_piece_type, 0) + 1
        )

    def remove_from_hand(
        self, piece_type: PieceType, color: Color
    ) -> bool:  # Corrected signature
        """Removes one piece of piece_type from the specified color's hand."""
        if piece_type not in get_unpromoted_types():
            # Attempting to remove a promoted type from hand, which is invalid.
            # Or, piece_type is KING, which cannot be in hand.
            # print(f\"Warning: Attempted to remove invalid piece type '{piece_type}' from hand.\")
            return False  # Or raise error

        hand_to_modify = self.hands[color.value]
        if hand_to_modify.get(piece_type, 0) > 0:
            hand_to_modify[piece_type] -= 1
            return True
        # print(f\"Warning: Attempted to remove {piece_type} from {color}\'s hand, but not available.\")
        return False

    def get_pieces_in_hand(self, color: Color) -> Dict[PieceType, int]:
        """
        Returns a copy of the pieces in hand for the specified player.
        """
        return self.hands[color.value].copy()

    def is_in_promotion_zone(self, row: int, color: Color) -> bool:
        """
        Checks if the specified row is in the promotion zone for the given color.
        """
        if color == Color.BLACK:
            return 0 <= row <= 2
        return 6 <= row <= 8

    def can_drop_piece(
        self, piece_type: PieceType, row: int, col: int, color: Color
    ) -> bool:
        """
        Checks if a piece of the specified type can be legally dropped on the given square.
        Delegates to shogi_rules_logic.can_drop_specific_piece for all rule checks.
        """
        return shogi_rules_logic.can_drop_specific_piece(
            self, piece_type, row, col, color, is_escape_check=False
        )

    def __repr__(self):
        return f"<ShogiGame move_count={self.move_count} current_player={self.current_player}>"

    # --- Reward Function ---
    def get_reward(self, player_color: Optional[Color] = None) -> float:
        """Calculates the reward for the player_color based on the game outcome."""
        if not self.game_over:
            return 0.0

        perspective_player = player_color
        if perspective_player is None:
            # If no specific player perspective, use the player whose turn it would have been
            # if the game hadn't ended, or the winner if clear.
            # This logic might need refinement based on how rewards are assigned post-game.
            # For now, let's assume if a winner exists, it's from their perspective.
            # If stalemate, it's neutral.
            if self.winner is not None:
                perspective_player = self.winner  # Win is +1 for winner
            else:  # Stalemate or other draw
                return 0.0

        if self.winner == perspective_player:
            return 1.0  # Win
        if self.winner is not None and self.winner != perspective_player:
            return -1.0  # Loss
        return 0.0  # Draw or game not over from this perspective

    def seed(self, _seed_value=None):
        """Seed the game environment for reproducibility (no-op for standard Shogi)."""
        # No operation needed for standard Shogi
        return

]]></file>
  <file path="shogi/shogi_core_definitions.py"><![CDATA[
"""
shogi_core_definitions.py: Core type definitions, enums, constants,
and the Piece class for the Shogi game engine.

This module provides fundamental building blocks for a Shogi game,
including representations for piece colors, types, game termination
reasons, move structures, and the `Piece` class itself. It also defines
constants related to game notation (KIF) and observation tensors for
potential AI applications.
"""

from enum import Enum
from typing import Dict, List, Optional, Set, Tuple, Union

# --- Public API ---
__all__ = [
    "Color",
    "PieceType",
    "KIF_PIECE_SYMBOLS",
    "TerminationReason",
    "BoardMoveTuple",
    "DropMoveTuple",
    "MoveTuple",
    "get_unpromoted_types",
    "PROMOTED_TYPES_SET",
    "BASE_TO_PROMOTED_TYPE",
    "PROMOTED_TO_BASE_TYPE",
    "PIECE_TYPE_TO_HAND_TYPE",
    "OBS_CURR_PLAYER_UNPROMOTED_START",
    "OBS_CURR_PLAYER_PROMOTED_START",
    "OBS_OPP_PLAYER_UNPROMOTED_START",
    "OBS_OPP_PLAYER_PROMOTED_START",
    "OBS_CURR_PLAYER_HAND_START",
    "OBS_OPP_PLAYER_HAND_START",
    "OBS_CURR_PLAYER_INDICATOR",
    "OBS_MOVE_COUNT",
    "OBS_RESERVED_1",
    "OBS_RESERVED_2",
    "SYMBOL_TO_PIECE_TYPE",
    "get_piece_type_from_symbol",
    "OBS_UNPROMOTED_ORDER",
    "OBS_PROMOTED_ORDER",
    "Piece",
]


# --- Enums and Constants ---
class Color(Enum):
    """
    Represents the player color.
    In Shogi, Black (Sente) typically moves first.
    """

    BLACK = 0  # Sente (先手), typically moves first
    WHITE = 1  # Gote (後手)


class PieceType(Enum):
    """
    Represents the type of a Shogi piece, including promoted states.
    """

    PAWN = 0
    LANCE = 1
    KNIGHT = 2
    SILVER = 3
    GOLD = 4
    BISHOP = 5
    ROOK = 6
    KING = 7
    PROMOTED_PAWN = 8  # Tokin (と)
    PROMOTED_LANCE = 9  # Promoted Lance (成香 - Narikyō)
    PROMOTED_KNIGHT = 10  # Promoted Knight (成桂 - Narikei)
    PROMOTED_SILVER = 11  # Promoted Silver (成銀 - Narigin)
    # Gold does not promote
    PROMOTED_BISHOP = 12  # Horse (竜馬 - Ryūma)
    PROMOTED_ROOK = 13  # Dragon (竜王 - Ryūō)
    # King does not promote

    def to_usi_char(self) -> str:
        """Returns the USI character for the piece type (for unpromoted pieces used in drops)."""
        # USI representation for pieces (typically uppercase)
        # P, L, N, S, G, B, R
        match self:
            case PieceType.PAWN:
                return "P"
            case PieceType.LANCE:
                return "L"
            case PieceType.KNIGHT:
                return "N"
            case PieceType.SILVER:
                return "S"
            case PieceType.GOLD:
                return "G"
            case PieceType.BISHOP:
                return "B"
            case PieceType.ROOK:
                return "R"
            # King and promoted pieces are not dropped, so they don't have a simple USI char in this context.
            # However, USI move format for board moves uses piece letters for disambiguation in CSA format,
            # but not typically in standard USI like 7g7f.
            # For drops, it's P*5e.
            # This method is primarily for getting the char for a drop.
            case _:
                raise ValueError(
                    f"Piece type {self.name} cannot be dropped or has no standard single USI drop character."
                )


# KIF Piece Symbol Mapping (Standard two-letter KIF symbols)
KIF_PIECE_SYMBOLS: Dict[PieceType, str] = {
    PieceType.PAWN: "FU",
    PieceType.LANCE: "KY",
    PieceType.KNIGHT: "KE",
    PieceType.SILVER: "GI",
    PieceType.GOLD: "KI",
    PieceType.BISHOP: "KA",
    PieceType.ROOK: "HI",
    PieceType.KING: "OU",  # Or "GY" for Gyoku (玉), "OU" (王) is common for King
    PieceType.PROMOTED_PAWN: "TO",  # Tokin
    PieceType.PROMOTED_LANCE: "NY",  # Nari-Kyo (Promoted Lance)
    PieceType.PROMOTED_KNIGHT: "NK",  # Nari-Kei (Promoted Knight)
    PieceType.PROMOTED_SILVER: "NG",  # Nari-Gin (Promoted Silver)
    PieceType.PROMOTED_BISHOP: "UM",  # Uma (Horse)
    PieceType.PROMOTED_ROOK: "RY",  # Ryu (Dragon)
}


class TerminationReason(Enum):
    """
    Enumerates reasons why a Shogi game might terminate.
    """

    CHECKMATE = "checkmate"
    RESIGNATION = "resignation"
    MAX_MOVES_EXCEEDED = "max_moves_exceeded"
    REPETITION = "repetition"  # Sennichite (千日手)
    IMPASSE = "impasse"  # Jishogi (持将棋) (by points, declaration, etc.)
    ILLEGAL_MOVE = "illegal_move"
    TIME_FORFEIT = "time_forfeit"
    NO_CONTEST = "no_contest"  # E.g. server error, mutual agreement for no result


# --- Custom Types for Moves ---

# BoardMoveTuple: (from_row, from_col, to_row, to_col, promote_flag)
#   - from_row, from_col: 0-indexed source square coordinates.
#   - to_row, to_col: 0-indexed destination square coordinates.
#   - promote_flag: Boolean indicating if promotion occurs on this move.
BoardMoveTuple = Tuple[int, int, int, int, bool]

# DropMoveTuple: (None, None, to_row, to_col, piece_type_to_drop)
#   - from_row, from_col: Always None to distinguish from board moves.
#   - to_row, to_col: 0-indexed destination square coordinates for the drop.
#   - piece_type_to_drop: The PieceType (unpromoted) to be dropped.
DropMoveTuple = Tuple[Optional[int], Optional[int], int, int, PieceType]

# MoveTuple is a union of the two types of moves.
MoveTuple = Union[BoardMoveTuple, DropMoveTuple]


def get_unpromoted_types() -> List[PieceType]:
    """
    Returns a list of all PieceType enums that represent unpromoted pieces
    capable of being held in hand. King is not included as it cannot be held.
    """
    return [
        PieceType.PAWN,
        PieceType.LANCE,
        PieceType.KNIGHT,
        PieceType.SILVER,
        PieceType.GOLD,
        PieceType.BISHOP,
        PieceType.ROOK,
    ]


PROMOTED_TYPES_SET: Set[PieceType] = {
    PieceType.PROMOTED_PAWN,
    PieceType.PROMOTED_LANCE,
    PieceType.PROMOTED_KNIGHT,
    PieceType.PROMOTED_SILVER,
    PieceType.PROMOTED_BISHOP,
    PieceType.PROMOTED_ROOK,
}
"""A set of all piece types that are in a promoted state."""

BASE_TO_PROMOTED_TYPE: Dict[PieceType, PieceType] = {
    PieceType.PAWN: PieceType.PROMOTED_PAWN,
    PieceType.LANCE: PieceType.PROMOTED_LANCE,
    PieceType.KNIGHT: PieceType.PROMOTED_KNIGHT,
    PieceType.SILVER: PieceType.PROMOTED_SILVER,
    PieceType.BISHOP: PieceType.PROMOTED_BISHOP,
    PieceType.ROOK: PieceType.PROMOTED_ROOK,
}
"""Maps base (unpromoted) piece types to their promoted counterparts."""

PROMOTED_TO_BASE_TYPE: Dict[PieceType, PieceType] = {
    v: k for k, v in BASE_TO_PROMOTED_TYPE.items()
}
"""Maps promoted piece types back to their base (unpromoted) counterparts."""


PIECE_TYPE_TO_HAND_TYPE: Dict[PieceType, PieceType] = {
    PieceType.PAWN: PieceType.PAWN,
    PieceType.LANCE: PieceType.LANCE,
    PieceType.KNIGHT: PieceType.KNIGHT,
    PieceType.SILVER: PieceType.SILVER,
    PieceType.GOLD: PieceType.GOLD,  # Gold is already its base type
    PieceType.BISHOP: PieceType.BISHOP,
    PieceType.ROOK: PieceType.ROOK,
    PieceType.PROMOTED_PAWN: PieceType.PAWN,
    PieceType.PROMOTED_LANCE: PieceType.LANCE,
    PieceType.PROMOTED_KNIGHT: PieceType.KNIGHT,
    PieceType.PROMOTED_SILVER: PieceType.SILVER,
    PieceType.PROMOTED_BISHOP: PieceType.BISHOP,
    PieceType.PROMOTED_ROOK: PieceType.ROOK,
    # Note: Kings (PieceType.KING) are not capturable in a way that adds them to hand.
    # Gold pieces (PieceType.GOLD) do not promote, so they remain Gold when captured.
}
"""
Maps a captured piece type (which could be promoted) to the
PieceType it becomes when added to a player's hand (always unpromoted).
"""

# Observation Plane Constants
# ---------------------------
# These constants define the structure of a (46, 9, 9) observation tensor,
# commonly used as input for neural networks in Shogi AI.
#
# Channel map:
#
# Board Piece Channels (28 total):
#   Channels  0-7: Current player's unpromoted pieces (P, L, N, S, G, B, R, K)
#   Channels  8-13: Current player's promoted pieces (+P, +L, +N, +S, +B, +R)
#   Channels 14-21: Opponent's unpromoted pieces (P, L, N, S, G, B, R, K)
#   Channels 22-27: Opponent's promoted pieces (+P, +L, +N, +S, +B, +R)
#
# Hand Piece Channels (14 total):
#   Channels 28-34: Current player's hand (P, L, N, S, G, B, R count planes)
#   Channels 35-41: Opponent's hand (P, L, N, S, G, B, R count planes)
#
# Meta Information Channels (4 total):
#   Channel 42: Current player indicator (1.0 if Black, 0.0 if White)
#   Channel 43: Move count (normalized or raw)
#   Channel 44: Reserved for future use (e.g., repetition count)
#   Channel 45: Reserved for future use

OBS_CURR_PLAYER_UNPROMOTED_START = 0
OBS_CURR_PLAYER_PROMOTED_START = 8
OBS_OPP_PLAYER_UNPROMOTED_START = 14
OBS_OPP_PLAYER_PROMOTED_START = 22

OBS_CURR_PLAYER_HAND_START = 28
OBS_OPP_PLAYER_HAND_START = 35

OBS_CURR_PLAYER_INDICATOR = 42
OBS_MOVE_COUNT = 43
OBS_RESERVED_1 = 44  # Potentially for repetition count or other game state
OBS_RESERVED_2 = 45  # Potentially for game phase or other features

# Total observation planes: 46 channels
#   - 8 current player unpromoted board pieces
#   - 6 current player promoted board pieces
#   - 8 opponent unpromoted board pieces
#   - 6 opponent promoted board pieces
#   - 7 current player hand pieces (types)
#   - 7 opponent hand pieces (types)
#   - 4 meta information planes


# Symbol to PieceType mapping.
# Uses uppercase symbols as canonical representation.
# E.g., "P" for Pawn, "+P" for Promoted Pawn (Tokin).
SYMBOL_TO_PIECE_TYPE: Dict[str, PieceType] = {
    "P": PieceType.PAWN,
    "L": PieceType.LANCE,
    "N": PieceType.KNIGHT,
    "S": PieceType.SILVER,
    "G": PieceType.GOLD,
    "B": PieceType.BISHOP,
    "R": PieceType.ROOK,
    "K": PieceType.KING,
    "+P": PieceType.PROMOTED_PAWN,
    "+L": PieceType.PROMOTED_LANCE,
    "+N": PieceType.PROMOTED_KNIGHT,
    "+S": PieceType.PROMOTED_SILVER,
    "+B": PieceType.PROMOTED_BISHOP,
    "+R": PieceType.PROMOTED_ROOK,
}


def get_piece_type_from_symbol(symbol: str) -> PieceType:
    """
    Converts a piece symbol string (e.g., "P", "+R", "p", "+r") to a PieceType enum.

    The function prefers canonical uppercase symbols (e.g., "P", "+R") but will
    attempt to normalize common lowercase variations (e.g., "p" to "P", "+p" to "+P").

    Args:
        symbol: The piece symbol string.

    Returns:
        The corresponding PieceType enum.

    Raises:
        ValueError: If the symbol is unknown or malformed.
    """
    # Primary check for canonical symbols (already in SYMBOL_TO_PIECE_TYPE)
    if symbol in SYMBOL_TO_PIECE_TYPE:
        return SYMBOL_TO_PIECE_TYPE[symbol]

    # Handle normalization for common lowercase variants
    # Case 1: Promoted piece like "+p"
    if len(symbol) == 2 and symbol.startswith("+") and symbol[1].islower():
        upper_symbol = "+" + symbol[1].upper()
        if upper_symbol in SYMBOL_TO_PIECE_TYPE:
            return SYMBOL_TO_PIECE_TYPE[upper_symbol]
    # Case 2: Unpromoted piece like "p"
    elif len(symbol) == 1 and symbol.islower():
        upper_symbol = symbol.upper()
        if upper_symbol in SYMBOL_TO_PIECE_TYPE:
            return SYMBOL_TO_PIECE_TYPE[upper_symbol]

    raise ValueError(f"Unknown piece symbol: {symbol}")


# Order of unpromoted pieces for observation channels (excluding King for hand)
OBS_UNPROMOTED_ORDER: List[PieceType] = [
    PieceType.PAWN,
    PieceType.LANCE,
    PieceType.KNIGHT,
    PieceType.SILVER,
    PieceType.GOLD,
    PieceType.BISHOP,
    PieceType.ROOK,
    PieceType.KING,  # King is included for on-board representation
]

# Order of promoted pieces for observation channels
OBS_PROMOTED_ORDER: List[PieceType] = [
    PieceType.PROMOTED_PAWN,
    PieceType.PROMOTED_LANCE,
    PieceType.PROMOTED_KNIGHT,
    PieceType.PROMOTED_SILVER,
    PieceType.PROMOTED_BISHOP,
    PieceType.PROMOTED_ROOK,
]

# Internal mapping for Piece.symbol() method for conciseness.
_PIECE_TYPE_TO_CHAR_SYMBOL: Dict[PieceType, str] = {
    PieceType.PAWN: "P",
    PieceType.LANCE: "L",
    PieceType.KNIGHT: "N",
    PieceType.SILVER: "S",
    PieceType.GOLD: "G",
    PieceType.BISHOP: "B",
    PieceType.ROOK: "R",
    PieceType.KING: "K",
    PieceType.PROMOTED_PAWN: "+P",
    PieceType.PROMOTED_LANCE: "+L",
    PieceType.PROMOTED_KNIGHT: "+N",
    PieceType.PROMOTED_SILVER: "+S",
    PieceType.PROMOTED_BISHOP: "+B",
    PieceType.PROMOTED_ROOK: "+R",
}


# --- Piece Class ---
class Piece:
    """
    Represents a single Shogi piece.

    Attributes:
        type (PieceType): The type of the piece (e.g., PAWN, GOLD, PROMOTED_ROOK).
        color (Color): The color of the piece (BLACK or WHITE).
        is_promoted (bool): True if the piece is in its promoted state, False otherwise.
                            This is derived from `type`.
    """

    def __init__(self, piece_type: PieceType, color: Color):
        """
        Initializes a Piece instance.

        Args:
            piece_type: The type of the piece.
            color: The color of the piece.

        Raises:
            TypeError: If `piece_type` is not a `PieceType` or
                       `color` is not a `Color`.
        """
        if not isinstance(piece_type, PieceType):
            raise TypeError("piece_type must be an instance of PieceType")
        if not isinstance(color, Color):
            raise TypeError("color must be an instance of Color")

        self.type: PieceType = piece_type
        self.color: Color = color
        # is_promoted is derived from the piece_type for consistency
        self.is_promoted: bool = piece_type in PROMOTED_TYPES_SET

    def symbol(self) -> str:
        """
        Returns a string symbol for the piece (e.g., "P", "+P", "k").
        Uppercase for Black (Sente), lowercase for White (Gote).
        Promoted pieces are prefixed with '+'.

        Returns:
            The piece symbol string.

        Raises:
            ValueError: If the piece has an unknown type.
        """
        try:
            base_symbol = _PIECE_TYPE_TO_CHAR_SYMBOL[self.type]
        except KeyError as exc:
            # This should ideally not happen if PieceType enum is comprehensive
            # and _PIECE_TYPE_TO_CHAR_SYMBOL is kept in sync.
            raise ValueError(f"Unknown piece type: {self.type}") from exc

        return base_symbol.lower() if self.color == Color.WHITE else base_symbol

    def promote(self) -> None:
        """
        Promotes the piece if it is promotable and not already promoted.
        If the piece type cannot be promoted or is already promoted,
        this method has no effect.
        Updates `self.type` and `self.is_promoted` accordingly.
        """
        if self.type in BASE_TO_PROMOTED_TYPE and not self.is_promoted:
            self.type = BASE_TO_PROMOTED_TYPE[self.type]
            self.is_promoted = True
        # No change if not promotable or already promoted

    def unpromote(self) -> None:
        """
        Unpromotes the piece if it is currently in a promoted state.
        If the piece is not promoted, this method has no effect.
        Updates `self.type` and `self.is_promoted` accordingly.
        """
        if self.is_promoted and self.type in PROMOTED_TO_BASE_TYPE:
            self.type = PROMOTED_TO_BASE_TYPE[self.type]
            self.is_promoted = False
        # No change if not promoted

    def __repr__(self) -> str:
        """
        Returns an unambiguous string representation of the Piece.
        """
        return f"Piece({self.type.name}, {self.color.name})"

    def __eq__(self, other: object) -> bool:
        """
        Checks if this Piece is equal to another object.
        Two pieces are equal if they have the same type and color.
        """
        if not isinstance(other, Piece):
            return NotImplemented
        return self.type == other.type and self.color == other.color

    def __hash__(self) -> int:
        """
        Returns a hash value for the Piece.
        Based on the piece type and color.
        """
        return hash((self.type, self.color))

    def __deepcopy__(self, memo: Dict[int, "Piece"]) -> "Piece":
        """
        Creates a deep copy of this Piece instance.
        Since Piece instances are relatively simple and their core attributes
        (type, color) define their state, creating a new instance is sufficient.

        Args:
            memo: The memoization dictionary used by `copy.deepcopy`.

        Returns:
            A new Piece instance identical to this one.
        """
        # Piece objects are simple enough that creating a new one with the same
        # attributes serves as a deep copy. `is_promoted` is derived correctly
        # by the __init__ method.
        new_piece = Piece(self.type, self.color)
        memo[id(self)] = new_piece  # Store in memo for `deepcopy` consistency
        return new_piece

]]></file>
  <file path="shogi/shogi_rules_logic.py"><![CDATA[
"""
Core Shogi game rules, move generation, and validation logic.
Functions in this module operate on a ShogiGame instance.
"""

from typing import TYPE_CHECKING, List, Optional, Set, Tuple  # Added Set

# Ensure all necessary types are imported:
from .shogi_core_definitions import (
    BASE_TO_PROMOTED_TYPE,
    Color,
    MoveTuple,
    Piece,
    PieceType,
)

if TYPE_CHECKING:
    from .shogi_game import ShogiGame  # For type hinting the 'game' parameter


# --- Helper functions for move generation and validation ---


# ADDED: Centralized king finding function
def find_king(game: "ShogiGame", color: Color) -> Optional[Tuple[int, int]]:
    """Finds the king of the specified color on the board."""
    for r_k in range(9):
        for c_k in range(9):
            p = game.get_piece(r_k, c_k)
            if p and p.type == PieceType.KING and p.color == color:
                return (r_k, c_k)
    return None


# ADDED: is_in_check function to be called by ShogiGame.is_in_check
def is_in_check(
    game: "ShogiGame", player_color: Color, debug_recursion: bool = False
) -> bool:
    """Checks if the king of 'player_color' is in check."""
    king_pos: Optional[Tuple[int, int]] = find_king(game, player_color)

    if not king_pos:
        if debug_recursion:
            # Try to get SFEN, but game object might be in an intermediate state for this print
            sfen_str: str = "unavailable (game object might be partial)"
            try:
                sfen_str = game.to_sfen_string()
            except Exception:  # pylint: disable=broad-except
                pass  # Keep sfen_str as unavailable
            print(
                f"DEBUG_IS_IN_CHECK: King of color {player_color} not found. Game state (SFEN): {sfen_str}. Returning True (check)."
            )
        return (
            True  # King not found implies a lost/invalid state, effectively in check.
        )

    opponent_color: Color = Color.WHITE if player_color == Color.BLACK else Color.BLACK

    if debug_recursion:
        print(
            f"DEBUG_IS_IN_CHECK: [{player_color}] King at {king_pos}. Checking if attacked by {opponent_color}. Debug on."
        )
        # Detailed print for attack check will come from check_if_square_is_attacked

    return check_if_square_is_attacked(
        game, king_pos[0], king_pos[1], opponent_color, debug=debug_recursion
    )


def is_piece_type_sliding(piece_type: PieceType) -> bool:  # Removed 'game' parameter
    """Returns True if the piece type is a sliding piece (Lance, Bishop, Rook or their promoted versions)."""
    sliding_types: Set[PieceType] = {
        PieceType.LANCE,
        PieceType.BISHOP,
        PieceType.ROOK,
        PieceType.PROMOTED_BISHOP,
        PieceType.PROMOTED_ROOK,
    }
    return piece_type in sliding_types


def generate_piece_potential_moves(
    game: "ShogiGame", piece: Piece, r_from: int, c_from: int
) -> List[Tuple[int, int]]:  # Changed to List[Tuple[int, int]]
    """
    Returns a list of (r_to, c_to) tuples for a piece, considering its
    fundamental movement rules and path-blocking by other pieces.
    This function generates squares a piece *attacks* or can move to if empty.
    It stops at the first piece encountered. If that piece is an opponent,
    the square is included (as a capture). If friendly, it's not included.
    (Formerly ShogiGame.get_individual_piece_moves)
    """

    moves: List[Tuple[int, int]] = []
    # Black (Sente, 0) moves towards smaller row indices, White (Gote, 1) towards larger
    forward: int = (
        -1 if piece.color == Color.BLACK else 1
    )  # Assuming game instance has Color enum

    piece_type: PieceType = piece.type

    # Define move offsets
    gold_move_offsets: List[Tuple[int, int]] = [
        (forward, 0),
        (forward, -1),
        (forward, 1),
        (0, -1),
        (0, 1),
        (-forward, 0),  # Backwards for Gold
    ]
    king_move_offsets: List[Tuple[int, int]] = [
        (-1, -1),
        (-1, 0),
        (-1, 1),
        (0, -1),
        (0, 1),
        (1, -1),
        (1, 0),
        (1, 1),
    ]
    knight_move_offsets: List[Tuple[int, int]] = [(forward * 2, -1), (forward * 2, 1)]
    silver_move_offsets: List[Tuple[int, int]] = [
        (forward, 0),
        (forward, -1),
        (forward, 1),
        (-forward, -1),
        (-forward, 1),  # Backwards-diagonal
    ]
    promoted_rook_extra_offsets: List[Tuple[int, int]] = [
        (-1, -1),
        (-1, 1),
        (1, -1),
        (1, 1),
    ]
    promoted_bishop_extra_offsets: List[Tuple[int, int]] = [
        (-1, 0),
        (1, 0),
        (0, -1),
        (0, 1),
    ]

    current_offsets: List[Tuple[int, int]] = []
    is_sliding: bool = False

    if piece_type == PieceType.PAWN:
        current_offsets = [(forward, 0)]
    elif piece_type == PieceType.KNIGHT:
        current_offsets = knight_move_offsets
    elif piece_type == PieceType.SILVER:
        current_offsets = silver_move_offsets
    elif (
        piece_type == PieceType.GOLD
        or piece_type == PieceType.PROMOTED_PAWN
        or piece_type == PieceType.PROMOTED_LANCE
        or piece_type == PieceType.PROMOTED_KNIGHT
        or piece_type == PieceType.PROMOTED_SILVER
    ):
        current_offsets = gold_move_offsets
    elif piece_type == PieceType.KING:
        current_offsets = king_move_offsets

    for dr, dc in current_offsets:
        nr, nc = r_from + dr, c_from + dc
        if game.is_on_board(nr, nc):
            target_piece: Optional[Piece] = game.get_piece(nr, nc)
            if target_piece is None or target_piece.color != piece.color:
                moves.append((nr, nc))

    sliding_directions: List[Tuple[int, int]] = []
    if piece_type == PieceType.LANCE:
        is_sliding = True
        sliding_directions = [(forward, 0)]
    elif piece_type == PieceType.BISHOP or piece_type == PieceType.PROMOTED_BISHOP:
        is_sliding = True
        sliding_directions.extend([(-1, -1), (-1, 1), (1, -1), (1, 1)])
        if piece_type == PieceType.PROMOTED_BISHOP:
            for dr, dc in promoted_bishop_extra_offsets:
                nr, nc = r_from + dr, c_from + dc
                if game.is_on_board(nr, nc):
                    target_piece = game.get_piece(nr, nc)
                    if target_piece is None or target_piece.color != piece.color:
                        moves.append((nr, nc))
    elif piece_type == PieceType.ROOK or piece_type == PieceType.PROMOTED_ROOK:
        is_sliding = True
        sliding_directions.extend([(-1, 0), (1, 0), (0, -1), (0, 1)])
        if piece_type == PieceType.PROMOTED_ROOK:
            for dr, dc in promoted_rook_extra_offsets:
                nr, nc = r_from + dr, c_from + dc
                if game.is_on_board(nr, nc):
                    target_piece = game.get_piece(nr, nc)
                    if target_piece is None or target_piece.color != piece.color:
                        moves.append((nr, nc))

    if is_sliding:
        for dr_slide, dc_slide in sliding_directions:
            for i in range(1, 9):  # Max 8 steps
                nr, nc = r_from + dr_slide * i, c_from + dc_slide * i
                if not game.is_on_board(nr, nc):
                    break
                target_piece = game.get_piece(nr, nc)
                if target_piece is None:
                    moves.append((nr, nc))
                else:
                    if target_piece.color != piece.color:
                        moves.append((nr, nc))
                    break

    return list(set(moves))  # Remove duplicates, type is List[Tuple[int, int]]


def check_for_nifu(game: "ShogiGame", color: Color, col: int) -> bool:
    """
    Checks for two unpromoted pawns of the same color on the given file.
    (Formerly ShogiGame.is_nifu)
    """
    for r in range(9):
        p = game.get_piece(r, col)
        if (
            p
            and p.type == PieceType.PAWN
            and p.color == color
            # and not p.is_promoted # This check is redundant if type is PAWN
        ):
            # Found one pawn, need to check for a *second* one.
            # The original logic implies if *any* pawn is found, it's nifu,
            # which is incorrect. Nifu = "two pawns".
            # This function as written in original code is actually "is_pawn_on_file"
            # For a correct nifu, one pawn must already be on the file.
            # This function is used when *dropping* a pawn. So it checks if a pawn *already exists*.
            return True  # A pawn of that color already exists on this file.
    return False


def check_if_square_is_attacked(
    game: "ShogiGame",
    r_target: int,
    c_target: int,
    attacker_color: Color,
    debug: bool = False,  # ADDED debug flag
) -> bool:
    """
    Checks if the square (r_target, c_target) is attacked by any piece of attacker_color.
    """  # Corrected string literal
    if debug:
        print(
            f"DEBUG_CHECK_SQ_ATTACKED: Checking if ({r_target},{c_target}) is attacked by {attacker_color}"
        )
        print(f"DEBUG_CHECK_SQ_ATTACKED: Game state for check: {game.to_sfen_string()}")

    for r_attacker in range(9):
        for c_attacker in range(9):
            piece = game.get_piece(r_attacker, c_attacker)
            if piece and piece.color == attacker_color:
                if debug:
                    print(
                        f"DEBUG_CHECK_SQ_ATTACKED: Checking attacker {piece.type.name} ({piece.color.name}) at ({r_attacker},{c_attacker}) against target ({r_target},{c_target})"
                    )

                potential_moves_of_attacker = generate_piece_potential_moves(
                    game, piece, r_attacker, c_attacker
                )
                if (r_target, c_target) in potential_moves_of_attacker:
                    if debug:
                        print(
                            f"DEBUG_CHECK_SQ_ATTACKED: ***ASSERTION TRIGGER*** YES, {piece.type.name} ({piece.color.name}) at ({r_attacker},{c_attacker}) attacks target ({r_target},{c_target}). Attacker potential moves: {potential_moves_of_attacker}. Game SFEN: {game.to_sfen_string()}"
                        )
                    return True
    if debug:
        print(
            f"DEBUG_CHECK_SQ_ATTACKED: NO, ({r_target},{c_target}) is NOT attacked by {attacker_color}"
        )
    return False


def check_for_uchi_fu_zume(
    game: "ShogiGame", drop_row: int, drop_col: int, color: Color
) -> bool:
    """
    Returns True if dropping a pawn at (drop_row, drop_col) by 'color'
    results in immediate, unescapable checkmate for the opponent.
    This is an illegal move by Shogi rules.

    RECURSION PREVENTION: This function calls generate_all_legal_moves() with
    is_uchi_fu_zume_check=True. This flag prevents infinite recursion by:
    1. Being passed to can_drop_specific_piece() as is_escape_check
    2. When is_escape_check=True, pawn drops skip their own uchi_fu_zume check

    This function does NOT check if the drop itself leaves 'color's king in check;
    that is handled by the main move generation logic.
    """
    # print(f"DEBUG_UCHI_FU_ZUME_DETAILED: Entered for color {color}, drop PAWN at ({drop_row}, {drop_col})")
    # print(f"DEBUG_UCHI_FU_ZUME_DETAILED: Initial game state (SFEN): {game.to_sfen_string()}")
    # print(f"DEBUG_UCHI_FU_ZUME_DETAILED: Current player in initial game: {game.current_player}")

    opp_color = Color.WHITE if color == Color.BLACK else Color.BLACK
    original_current_player = game.current_player  # Store original current player

    # Ensure the square is empty (pre-condition)
    if game.get_piece(drop_row, drop_col) is not None:
        # print("DEBUG_UCHI_FU_ZUME_DETAILED: Target square not empty. Returning False.")
        return False

    # Ensure the player has a pawn in hand
    if not (
        PieceType.PAWN in game.hands[color.value]
        and game.hands[color.value][PieceType.PAWN] > 0
    ):
        # print("DEBUG_UCHI_FU_ZUME_DETAILED: No pawn in hand to drop. Returning False.")
        return False

    # Simulate the pawn drop directly on the 'game' object
    # No deepcopy needed; we will manually revert the changes.
    game.set_piece(drop_row, drop_col, Piece(PieceType.PAWN, color))
    game.hands[color.value][PieceType.PAWN] -= 1
    # print(f"DEBUG_UCHI_FU_ZUME_DETAILED: Game state after pawn drop (SFEN): {game.to_sfen_string()}")

    # Find the opponent's king
    opp_king_pos = find_king(game, opp_color)
    # print(f"DEBUG_UCHI_FU_ZUME_DETAILED: Opponent color: {opp_color}, Opponent king pos: {opp_king_pos}")

    if not opp_king_pos:
        # print("DEBUG_UCHI_FU_ZUME_DETAILED: Opponent king not found. Reverting drop and returning False.")
        # Revert pawn drop before returning
        game.set_piece(drop_row, drop_col, None)  # Corrected: Use set_piece with None
        game.hands[color.value][PieceType.PAWN] += 1
        game.current_player = original_current_player  # Restore original player
        return False

    # 1. Check if the drop delivers check to the opponent's king.
    drop_delivers_check = check_if_square_is_attacked(
        game, opp_king_pos[0], opp_king_pos[1], color
    )
    # print(f"DEBUG_UCHI_FU_ZUME_DETAILED: Drop by {color} delivers check to {opp_color} king: {drop_delivers_check}")

    if not drop_delivers_check:
        # print("DEBUG_UCHI_FU_ZUME_DETAILED: Drop does not deliver check. Reverting drop and returning False.")
        # Revert pawn drop before returning
        game.set_piece(drop_row, drop_col, None)  # Corrected: Use set_piece with None
        game.hands[color.value][PieceType.PAWN] += 1
        game.current_player = original_current_player
        return False

    # 2. Check if the opponent's king has any legal moves to escape the check.
    #    Temporarily switch current player in the game to opponent to generate their legal moves.
    game.current_player = opp_color  # Now it's opponent's turn to find escapes

    opponent_legal_moves = generate_all_legal_moves(game, is_uchi_fu_zume_check=True)

    # print(f\"DEBUG_UCHI_FU_ZUME_DETAILED: Opponent ({opp_color}) legal moves found: {len(opponent_legal_moves)} moves: {opponent_legal_moves}\")
    # Revert pawn drop and restore original player *before* returning the result
    game.set_piece(drop_row, drop_col, None)  # Corrected: Use set_piece with None
    game.hands[color.value][PieceType.PAWN] += 1
    game.current_player = original_current_player  # Restore original player

    # If opponent_legal_moves is empty, it means the opponent is checkmated by the pawn drop.
    # Therefore, it IS uchi_fu_zume.
    result = not opponent_legal_moves
    # print(f"DEBUG_UCHI_FU_ZUME_DETAILED: Returning {result} (is uchi_fu_zume if True)")
    return result


def is_king_in_check_after_simulated_move(
    game: "ShogiGame", player_color: Color
) -> bool:
    """
    Checks if the king of 'player_color' is in check on the current board.
    Assumes the board reflects the state *after* a move has been made.
    (Formerly ShogiGame._king_in_check_after_move)
    """
    king_pos = find_king(game, player_color)

    if not king_pos:
        return True

    opponent_color = Color.WHITE if player_color == Color.BLACK else Color.BLACK
    in_check = check_if_square_is_attacked(
        game, king_pos[0], king_pos[1], opponent_color, debug=False
    )  # debug was from the removed flag
    return in_check


def can_promote_specific_piece(
    game: "ShogiGame", piece: Piece, r_from: int, r_to: int
) -> bool:
    """
    Checks if a piece *can* be promoted given its type and move.
    (Formerly ShogiGame.can_promote_piece)
    """
    if piece.type in [PieceType.GOLD, PieceType.KING] or piece.is_promoted:
        return False

    if piece.type not in BASE_TO_PROMOTED_TYPE:  # Not a base type that can promote
        return False

    # Must start in, end in, or cross promotion zone
    # Assumes game instance has is_in_promotion_zone(row, color) method
    if game.is_in_promotion_zone(r_from, piece.color) or game.is_in_promotion_zone(
        r_to, piece.color
    ):
        return True
    return False


def must_promote_specific_piece(
    piece: Piece, r_to: int
) -> bool:  # Removed 'game' parameter
    """Checks if a piece *must* promote when moving to r_to."""
    # Pawns and Lances must promote if they reach the last rank.
    if piece.type == PieceType.PAWN or piece.type == PieceType.LANCE:
        if (piece.color == Color.BLACK and r_to == 0) or (
            piece.color == Color.WHITE and r_to == 8
        ):
            return True

    # Knights on the last two ranks
    if piece.type == PieceType.KNIGHT:
        if (piece.color == Color.BLACK and r_to <= 1) or (
            piece.color == Color.WHITE and r_to >= 7
        ):
            return True
    return False


def can_drop_specific_piece(
    game: "ShogiGame",
    piece_type: PieceType,
    r_to: int,
    c_to: int,
    color: Color,
    is_escape_check: bool = False,  # Added for uchi_fu_zume fix
) -> bool:
    """
    Checks if a specific piece_type can be legally dropped by 'color' at (r_to, c_to).
    This function checks rules like:
    - Square must be empty.
    - Nifu (two pawns on the same file).
    - Piece cannot be dropped where it has no further moves (Pawn, Lance on last rank; Knight on last two ranks).
    - Uchi Fu Zume (dropping a pawn for an immediate checkmate that cannot be escaped).
    It does NOT check if the drop leaves the current player's king in check;
    that is handled by the calling function (e.g., generate_all_legal_moves).
    """
    # print(f"DEBUG_CAN_DROP: Checking drop of {piece_type} by {color} at ({r_to},{c_to}), is_escape_check={is_escape_check}") # Basic log
    if game.get_piece(r_to, c_to) is not None:
        # print(f"DEBUG_CAN_DROP: Square ({r_to},{c_to}) not empty. Returning False.") # Basic log
        return False  # Square must be empty

    # Determine player-specific "forward" direction and promotion zone boundaries
    last_rank = 0 if color == Color.BLACK else 8
    second_last_rank = 1 if color == Color.BLACK else 7

    if piece_type == PieceType.PAWN:
        # 1. Nifu check: Cannot drop a pawn on a file that already contains an unpromoted pawn of the same color.
        if check_for_nifu(game, color, c_to):
            return False
        # 2. Cannot drop a pawn on the last rank (it would have no moves).
        if r_to == last_rank:
            return False
        # 3. Uchi Fu Zume check: Cannot drop a pawn to give immediate checkmate if that checkmate has no escape.
        #    The check_for_uchi_fu_zume function returns True if it *is* uchi_fu_zume.
        #    This check is skipped if we are evaluating an escape move during an uchi_fu_zume check.
        if (
            not is_escape_check
        ):  # <-- MODIFIED: Only check uchi_fu_zume if not an escape check
            # print(f"DEBUG_CAN_DROP: PAWN drop, is_escape_check is False. Calling check_for_uchi_fu_zume for {color} at ({r_to},{c_to}).") # Basic log
            if check_for_uchi_fu_zume(game, r_to, c_to, color):
                # print(f"DEBUG_CAN_DROP: uchi_fu_zume check for {color} dropping PAWN at ({r_to},{c_to}) returned True. Preventing drop.")
                return False  # DO NOT COMMENT THIS OUT, IT IS A LOAD BEARING RETURN.
        # else: # Basic log
        # print(f"DEBUG_CAN_DROP: PAWN drop, is_escape_check is True. Skipping uchi_fu_zume check for {color} at ({r_to},{c_to}).") # Basic log
    elif piece_type == PieceType.LANCE:
        # Cannot drop a lance on the last rank.
        if r_to == last_rank:
            return False
    elif piece_type == PieceType.KNIGHT:
        # Cannot drop a knight on the last two ranks.
        if r_to == last_rank or r_to == second_last_rank:
            return False

    # Other pieces (Gold, Silver, Bishop, Rook, King) can be dropped on any empty square
    # provided other conditions (like not leaving king in check) are met by the caller.
    # King is not a droppable piece type, but included for completeness if logic changes.
    # For now, hands will only contain P, L, N, S, G, B, R.
    return True


def generate_all_legal_moves(
    game: "ShogiGame",
    is_uchi_fu_zume_check: bool = False,  # Restored is_uchi_fu_zume_check
) -> List[MoveTuple]:
    # Basic entry log
    # print(
    #    f"DEBUG_GALM: Entered for player {game.current_player}. SFEN: {game.to_sfen_string()}"
    # )
    # if is_uchi_fu_zume_check:
    #    print("DEBUG_GALM: Mode: is_uchi_fu_zume_check=True")  # Corrected f-string

    legal_moves: List[MoveTuple] = []
    original_player_color = game.current_player

    # I. Generate Board Moves
    for r_from in range(9):
        for c_from in range(9):
            piece = game.get_piece(r_from, c_from)
            if piece and piece.color == original_player_color:
                potential_squares = generate_piece_potential_moves(
                    game, piece, r_from, c_from
                )
                for r_to, c_to in potential_squares:
                    can_promote = can_promote_specific_piece(game, piece, r_from, r_to)
                    must_promote = must_promote_specific_piece(piece, r_to)
                    possible_promotions = [False]
                    if can_promote:
                        possible_promotions.append(True)
                    if must_promote:
                        possible_promotions = [True]

                    for promote_option in possible_promotions:
                        if must_promote and not promote_option:
                            continue
                        move_tuple = (r_from, c_from, r_to, c_to, promote_option)

                        simulation_details = game.make_move(
                            move_tuple, is_simulation=True
                        )

                        # --- Commented out unused trace code ---
                        king_pos_trace = find_king(game, original_player_color)
                        king_r_trace, king_c_trace = (
                            king_pos_trace if king_pos_trace else (-1, -1)
                        )
                        opponent_color_trace = (
                            Color.WHITE
                            if original_player_color == Color.BLACK
                            else Color.BLACK
                        )
                        is_attacked_after_sim = False  # Default if king not found
                        if king_pos_trace:  # Only check if king exists
                            is_attacked_after_sim = check_if_square_is_attacked(
                                game, king_r_trace, king_c_trace, opponent_color_trace
                            )

                        target_square_content_after_sim = game.get_piece(r_to, c_to)
                        king_is_safe_eval = not is_attacked_after_sim

                        # print(f"TRACE_SIM_BOARD_MOVE: Player {original_player_color}, Move {move_tuple}, Piece {piece}, Promoted: {promote_option}")
                        # print(f"  King at ({king_r_trace},{king_c_trace}), Target sq ({r_to},{c_to}) content after sim: {target_square_content_after_sim}")
                        # print(f"  Is king attacked after sim? {is_attacked_after_sim}. Final king_is_safe: {king_is_safe_eval}")
                        # --- TRACE PRINT BLOCK END ---

                        king_is_safe = not is_king_in_check_after_simulated_move(
                            game, original_player_color
                        )
                        if king_is_safe:
                            legal_moves.append(move_tuple)
                        # Ensure we only pass Dict[str, Any] to undo_move
                        if isinstance(simulation_details, dict):
                            game.undo_move(simulation_undo_details=simulation_details)
                        else:
                            game.undo_move()

    # II. Generate Drop Moves
    for piece_type_to_drop_val, count in game.hands[
        original_player_color.value
    ].items():
        if count > 0:
            piece_type_to_drop = PieceType(piece_type_to_drop_val)
            for r_to_drop in range(9):
                for c_to_drop in range(9):
                    if can_drop_specific_piece(
                        game,
                        piece_type_to_drop,
                        r_to_drop,
                        c_to_drop,
                        original_player_color,
                        is_escape_check=is_uchi_fu_zume_check,
                    ):
                        drop_move_tuple = (
                            None,
                            None,
                            r_to_drop,
                            c_to_drop,
                            piece_type_to_drop,
                        )
                        simulation_details_drop = game.make_move(
                            drop_move_tuple, is_simulation=True
                        )

                        # --- Commented out unused trace code for drops ---
                        king_pos_trace_drop = find_king(game, original_player_color)
                        king_r_trace_drop, king_c_trace_drop = (
                            king_pos_trace_drop if king_pos_trace_drop else (-1, -1)
                        )
                        opponent_color_trace_drop = (
                            Color.WHITE
                            if original_player_color == Color.BLACK
                            else Color.BLACK
                        )
                        is_attacked_after_drop_sim = False  # Default if king not found
                        if king_pos_trace_drop:  # Only check if king exists
                            is_attacked_after_drop_sim = check_if_square_is_attacked(
                                game,
                                king_r_trace_drop,
                                king_c_trace_drop,
                                opponent_color_trace_drop,
                            )
                        target_square_content_after_drop_sim = game.get_piece(
                            r_to_drop, c_to_drop
                        )
                        # king_is_safe_eval_drop = not is_attacked_after_drop_sim

                        # print(f"TRACE_SIM_DROP_MOVE: Player {original_player_color}, Drop {drop_move_tuple}")
                        # print(f"  King at ({king_r_trace_drop},{king_c_trace_drop}), Target sq ({r_to_drop},{c_to_drop}) content after sim: {target_square_content_after_drop_sim}")
                        # print(f"  Is king attacked after drop sim? {is_attacked_after_drop_sim}. Final king_is_safe: {king_is_safe_eval_drop}")
                        # --- TRACE PRINT BLOCK END (DROP) ---

                        king_is_safe_after_drop = (
                            not is_king_in_check_after_simulated_move(
                                game, original_player_color
                            )
                        )
                        if king_is_safe_after_drop:
                            legal_moves.append(drop_move_tuple)
                        # Ensure we only pass Dict[str, Any] to undo_move
                        if isinstance(simulation_details_drop, dict):
                            game.undo_move(
                                simulation_undo_details=simulation_details_drop
                            )
                        else:
                            game.undo_move()

    # Keep this final print for now to confirm the list content before returning
    # print(
    #    f"DEBUG_GALM: FINALIZING for {original_player_color}. Total legal moves: {len(legal_moves)}. Moves: {legal_moves}"
    # )
    return legal_moves


def check_for_sennichite(game: "ShogiGame") -> bool:
    """
    Returns True if the current board state has occurred four times (Sennichite).
    (Formerly ShogiGame.is_sennichite)
    Relies on game.move_history and game._board_state_hash().
    The hash in move_history is for the state *after* a move, including whose turn it was *before* switching players.
    Sennichite rule states: same game position (pieces on board, pieces in hand, and player to move)
    has appeared for the fourth time.
    """
    # The state hash to check for repetition should represent (board, hands, current_player_to_move)
    # In the original make_move:
    # 1. move pieces
    # 2. state_hash = _board_state_hash() (current_player is still P_old who made the move) -> stored in history
    # 3. current_player is switched to P_new
    # 4. if is_sennichite(): ... is called.
    # Inside is_sennichite (this function):
    # game.current_player is P_new.
    # So, state_to_check_for_repetition = game._board_state_hash() will use P_new.
    # This means we are checking if the state (board, hands, P_old_who_just_moved) has repeated.
    # The history stores (board, hands, P_old_who_just_moved). This comparison is subtle.

    # The problem description (II.8) implies the hash in history is the one to count.
    # "The state_hash stored in move_history by make_move is for the board
    # state *after* the move and includes self.current_player *before* it's switched."
    # "is_sennichite is called *after* self.current_player is switched."
    # "If current make_move appends hash for state *after* move & *before* player switch,
    # then is_sennichite is called *after* player switch. The hashes won't match."

    # The original `is_sennichite` code in the prompt:
    # final_state_hash_of_move = (
    #         game.move_history[-1].get("state_hash") if game.move_history else None
    # ) # This is (board_after_P_old_move, hands, P_old_turn)
    # ...
    # count_of_this_state = 0
    # for record in game.move_history:
    #     if record.get("state_hash") == final_state_hash_of_move:
    #         count_of_this_state += 1
    # return count_of_this_state >= 4
    # This means sennichite is declared if the state *just achieved by the previous player*
    # (which includes that player as the one whose turn it was for that hash) has appeared 4 times.
    # This is a common interpretation for how repetition is tracked.

    if not game.move_history:
        return False

    # This hash represents the board state achieved by the *previous* player's move,
    # and importantly, the game._board_state_hash() includes whose turn it *was* when that state was recorded.
    last_recorded_state_hash: Optional[Tuple] = game.move_history[-1].get("state_hash")
    if not last_recorded_state_hash:
        return False  # Should not happen if history is populated correctly

    count: int = 0
    for move_record in game.move_history:
        if move_record.get("state_hash") == last_recorded_state_hash:
            count += 1

    # If this state (board, hands, player_who_just_moved) has now occurred 4 times.
    return count >= 4

]]></file>
  <file path="shogi/shogi_move_execution.py"><![CDATA[
# shogi_move_execution.py
"""
Contains functions for applying and reverting moves in the Shogi game.
These functions operate on a ShogiGame instance.
"""

from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Optional,
    cast,
)

from . import shogi_rules_logic
from .shogi_core_definitions import PROMOTED_TO_BASE_TYPE  # Added PROMOTED_TO_BASE_TYPE
from .shogi_core_definitions import (
    Color,
    MoveTuple,
    Piece,
    PieceType,
    get_unpromoted_types,
)

if TYPE_CHECKING:
    from .shogi_game import ShogiGame


def apply_move_to_board(
    game: "ShogiGame",
    is_simulation: bool = False,
) -> None:
    """
    Updates the game state after a move has been applied to the board by ShogiGame.make_move.
    Its primary roles here are:
    1. Updating game.current_player.
    2. Incrementing game.move_count.
    3. Checking for game termination conditions (checkmate, stalemate, sennichite, max_moves)
       if not a simulation.

    Args:
        game: The ShogiGame instance.
        is_simulation: If True, indicates the move is part of a simulation
                       and game-ending checks (checkmate, stalemate) should be skipped.
    """
    player_who_made_the_move = game.current_player

    game.current_player = (
        Color.WHITE if player_who_made_the_move == Color.BLACK else Color.BLACK
    )
    game.move_count += 1

    if not is_simulation:
        # Pass the game instance itself to shogi_rules_logic functions
        king_of_current_player_in_check = shogi_rules_logic.is_in_check(
            game, game.current_player
        )
        legal_moves_for_current_player = game.get_legal_moves()

        if not legal_moves_for_current_player:
            if king_of_current_player_in_check:
                game.game_over = True
                game.winner = player_who_made_the_move
                game.termination_reason = "Tsumi"
            else:
                game.game_over = True
                game.winner = None
                game.termination_reason = "Stalemate"
        # Pass the game instance to check_for_sennichite
        elif shogi_rules_logic.check_for_sennichite(game):
            game.game_over = True
            game.winner = None
            game.termination_reason = "Sennichite"
        elif game.move_count >= game.max_moves_per_game:
            game.game_over = True
            game.winner = None
            game.termination_reason = "Max moves reached"


def revert_last_applied_move(
    game: "ShogiGame", simulation_undo_details: Optional[Dict[str, Any]] = None
) -> None:
    """
    Reverts the last move made in the game, completely restoring the previous state.
    Operates on the 'game' (ShogiGame instance).

    State restoration includes:
    1. Restoring the board position
    2. Restoring hand pieces
    3. Switching back to previous player
    4. Reversing move count
    5. Clearing termination status

    If simulation_undo_details is provided, it uses that to undo a simulated move
    that wasn't added to the game's history.
    """
    last_move_details: Dict[str, Any]
    player_who_made_the_undone_move: Color

    if simulation_undo_details:
        last_move_details = simulation_undo_details
        # Restore player and move count from before the simulated move
        game.current_player = last_move_details["player_who_made_the_move"]
        player_who_made_the_undone_move = game.current_player  # This is correct now
        game.move_count = last_move_details["move_count_before_move"]
        # No board_history manipulation for simulated moves
    else:
        if not game.move_history:
            raise RuntimeError("No move to undo from history")
        last_move_details = game.move_history.pop()
        if game.board_history:
            game.board_history.pop()  # Pop only if not a simulation

        # For historical undo, player and move count are decremented/switched
        game.current_player = (
            Color.WHITE if game.current_player == Color.BLACK else Color.BLACK
        )
        player_who_made_the_undone_move = game.current_player
        game.move_count -= 1

    move_tuple: MoveTuple = last_move_details["move"]

    # Common logic for reverting the move based on last_move_details
    if last_move_details["is_drop"]:
        dropped_piece_type_any = last_move_details["dropped_piece_type"]
        if not isinstance(dropped_piece_type_any, PieceType):
            raise TypeError(
                f"Expected PieceType for dropped_piece_type, got {type(dropped_piece_type_any)}"
            )
        dropped_piece_type: PieceType = dropped_piece_type_any

        r_to_drop_any = move_tuple[2]
        c_to_drop_any = move_tuple[3]

        if not isinstance(r_to_drop_any, int) or not isinstance(c_to_drop_any, int):
            raise TypeError(
                f"Expected int for r_to_drop and c_to_drop, got {type(r_to_drop_any)} and {type(c_to_drop_any)} respectively."
            )
        r_to_drop: int = r_to_drop_any
        c_to_drop: int = c_to_drop_any

        game.set_piece(r_to_drop, c_to_drop, None)
        if (
            dropped_piece_type in get_unpromoted_types()
            and dropped_piece_type != PieceType.KING
        ):
            game.hands[player_who_made_the_undone_move.value][dropped_piece_type] = (
                game.hands[player_who_made_the_undone_move.value].get(
                    dropped_piece_type, 0
                )
                + 1
            )
        else:
            raise ValueError(
                f"Attempted to return invalid piece type {dropped_piece_type} to hand during undo of drop."
            )

    else:  # Board move
        orig_r_from: int = cast(int, move_tuple[0])
        orig_c_from: int = cast(int, move_tuple[1])
        orig_r_to: int = cast(int, move_tuple[2])
        orig_c_to: int = cast(int, move_tuple[3])

        original_type_before_promotion_any = last_move_details[
            "original_type_before_promotion"
        ]
        original_color_of_moved_piece_any = last_move_details[
            "original_color_of_moved_piece"
        ]

        if not isinstance(original_type_before_promotion_any, PieceType):
            raise TypeError(
                f"Expected PieceType for original_type_before_promotion, got {type(original_type_before_promotion_any)}"
            )
        current_original_type_before_promotion: PieceType = (
            original_type_before_promotion_any
        )

        if not isinstance(original_color_of_moved_piece_any, Color):
            raise TypeError(
                f"Expected Color for original_color_of_moved_piece, got {type(original_color_of_moved_piece_any)}"
            )
        current_original_color_of_moved_piece: Color = original_color_of_moved_piece_any

        piece_to_restore_at_from = Piece(
            current_original_type_before_promotion,
            current_original_color_of_moved_piece,
        )

        game.set_piece(orig_r_from, orig_c_from, piece_to_restore_at_from)

        captured_piece_object: Optional[Piece] = last_move_details.get("captured")

        if captured_piece_object:
            game.set_piece(orig_r_to, orig_c_to, captured_piece_object)

            type_to_remove_from_hand: PieceType
            # Use PROMOTED_TO_BASE_TYPE directly from shogi_core_definitions
            if captured_piece_object.type in PROMOTED_TO_BASE_TYPE:
                type_to_remove_from_hand = PROMOTED_TO_BASE_TYPE[
                    captured_piece_object.type
                ]
            else:
                type_to_remove_from_hand = captured_piece_object.type

            if type_to_remove_from_hand != PieceType.KING:  # Kings are not held in hand
                game.remove_from_hand(
                    type_to_remove_from_hand, player_who_made_the_undone_move
                )
        else:
            game.set_piece(orig_r_to, orig_c_to, None)

    # game.move_count -= 1 # Moved up for non-simulation case, set directly for simulation
    game.game_over = False  # Reset game over status regardless
    game.winner = None
    game.termination_reason = None

]]></file>
  <file path="shogi/shogi_game_io.py"><![CDATA[
# shogi_game_io.py

import datetime  # For KIF Date header
import os
import re  # Import the re module
import sys
from typing import TYPE_CHECKING, Dict, List, Optional, Tuple

import numpy as np

from .shogi_core_definitions import (  # Observation plane constants
    KIF_PIECE_SYMBOLS,
    OBS_CURR_PLAYER_HAND_START,
    OBS_CURR_PLAYER_INDICATOR,
    OBS_CURR_PLAYER_PROMOTED_START,
    OBS_CURR_PLAYER_UNPROMOTED_START,
    OBS_MOVE_COUNT,
    OBS_OPP_PLAYER_HAND_START,
    OBS_OPP_PLAYER_PROMOTED_START,
    OBS_OPP_PLAYER_UNPROMOTED_START,
    OBS_PROMOTED_ORDER,
    OBS_UNPROMOTED_ORDER,
    SYMBOL_TO_PIECE_TYPE,
    Color,
    MoveTuple,
    Piece,
    PieceType,
    TerminationReason,
    get_unpromoted_types,
)

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

if TYPE_CHECKING:
    from .shogi_game import ShogiGame  # For type hinting the 'game' parameter


def generate_neural_network_observation(game: "ShogiGame") -> np.ndarray:
    """
    Returns the current board state as a (Channels, 9, 9) NumPy array for RL input.
    Channels:
    Player planes (current player POV):
     0-7: Unpromoted pieces (P, L, N, S, G, B, R, K)
     8-13: Promoted pieces (+P, +L, +N, +S, +B, +R)
    Opponent planes:
     14-21: Unpromoted pieces
     22-27: Promoted pieces
    Hand planes:
     28-34: Current player's hand (P, L, N, S, G, B, R) - 7 types
     35-41: Opponent's hand (P, L, N, S, G, B, R) - 7 types
    Meta planes:
     42: Current player (all 1.0 if Black, all 0.0 if White playing as Black's opponent view)
     43: Move count (normalized)
     44: Reserved for potential future features like repetition count (currently all zeros)
     45: Reserved for potential future features like game phase indicators (currently all zeros)
    Total: 46 channels (14 player board + 14 opponent board + 7 player hand + 7 opponent hand + 4 meta).
    """
    obs = np.zeros((46, 9, 9), dtype=np.float32)
    is_black_perspective = game.current_player == Color.BLACK

    # Map PieceType to its index in OBS_UNPROMOTED_ORDER or OBS_PROMOTED_ORDER
    unpromoted_map: Dict[PieceType, int] = {
        pt: i for i, pt in enumerate(OBS_UNPROMOTED_ORDER)
    }
    promoted_map: Dict[PieceType, int] = {
        pt: i for i, pt in enumerate(OBS_PROMOTED_ORDER)
    }

    for r in range(9):
        for c in range(9):
            # For white's perspective, we need to mirror the board coordinates
            flipped_r = r if is_black_perspective else 8 - r
            flipped_c = c if is_black_perspective else 8 - c

            p: Optional[Piece] = game.board[r][c]
            if p is None:
                continue

            is_current_player_piece: bool = p.color == game.current_player
            channel_offset: int = -1

            if p.is_promoted:
                if p.type in promoted_map:
                    # Offset for promoted planes
                    promoted_block_offset = (
                        OBS_CURR_PLAYER_PROMOTED_START
                        if is_current_player_piece
                        else OBS_OPP_PLAYER_PROMOTED_START
                    )
                    channel_offset = promoted_block_offset + promoted_map[p.type]
            else:  # Unpromoted or non-promotable (King, Gold)
                if p.type in unpromoted_map:
                    # Offset for unpromoted planes
                    unpromoted_block_offset = (
                        OBS_CURR_PLAYER_UNPROMOTED_START
                        if is_current_player_piece
                        else OBS_OPP_PLAYER_UNPROMOTED_START
                    )
                    channel_offset = unpromoted_block_offset + unpromoted_map[p.type]

            if channel_offset != -1:
                # Use the flipped coordinates for setting the observation plane
                obs[channel_offset, flipped_r, flipped_c] = 1.0

    # Pieces in hand (7 channels per player: P,L,N,S,G,B,R)
    hand_piece_order: List[PieceType] = (
        get_unpromoted_types()
    )  # Use the imported function

    # Current player's hand
    for i, piece_type_enum_player in enumerate(hand_piece_order):
        player_hand_count: int = game.hands[game.current_player.value].get(
            piece_type_enum_player, 0
        )
        if player_hand_count > 0:
            player_ch: int = OBS_CURR_PLAYER_HAND_START + i
            obs[player_ch, :, :] = (
                player_hand_count / 18.0
            )  # Normalize (e.g., by max pawns)

    # Opponent's hand
    opponent_color_val: int = (
        Color.WHITE.value if game.current_player == Color.BLACK else Color.BLACK.value
    )
    for i, piece_type_enum_opponent in enumerate(hand_piece_order):
        opponent_hand_count: int = game.hands[opponent_color_val].get(
            piece_type_enum_opponent, 0
        )
        if opponent_hand_count > 0:
            opponent_ch: int = OBS_OPP_PLAYER_HAND_START + i
            obs[opponent_ch, :, :] = opponent_hand_count / 18.0

    # Current player indicator plane
    obs[OBS_CURR_PLAYER_INDICATOR, :, :] = (
        1.0 if game.current_player == Color.BLACK else 0.0
    )

    # Move count plane (normalized)
    max_moves = float(game.max_moves_per_game)
    obs[OBS_MOVE_COUNT, :, :] = game.move_count / max_moves if max_moves > 0 else 0.0

    # Planes for OBS_RESERVED_1 and OBS_RESERVED_2 remain zeros.
    return obs


def convert_game_to_text_representation(game: "ShogiGame") -> str:
    """
    Returns a string representation of the Shogi game board and state.
    """
    lines = []
    for r_idx, row_data in enumerate(game.board):
        line_str: str = f"{9-r_idx} "  # Shogi board rank numbers (9 down to 1)
        line_pieces: List[str] = []
        for (
            p_cell
        ) in (
            row_data
        ):  # Renamed p to p_cell to avoid conflict with p in outer scope if any
            if p_cell:
                symbol: str = p_cell.symbol()
                if len(symbol) == 1:  # e.g., P
                    line_pieces.append(f" {symbol} ")  # Results in " P "
                else:  # e.g., +P
                    line_pieces.append(f"{symbol} ")  # Results in "+P "
            else:
                line_pieces.append(" . ")  # Consistent 3-char width
        lines.append(line_str + "".join(line_pieces))
    # Add file numbers at the bottom with consistent single-space formatting
    lines.append(
        "a b c d e f g h i"  # Single space between column labels as expected by tests
    )

    # Add player turn and move info first, then hands info
    lines.append(f"Turn: {game.current_player.name}, Move: {game.move_count+1}")

    black_hand_dict: Dict[str, int] = {
        pt.name: count
        for pt, count in game.hands[Color.BLACK.value].items()
        if count > 0
    }
    lines.append(f"Black's hand: {black_hand_dict}")

    white_hand_dict: Dict[str, int] = {
        pt.name: count
        for pt, count in game.hands[Color.WHITE.value].items()
        if count > 0
    }
    lines.append(f"White's hand: {white_hand_dict}")
    return "\n".join(lines)


def game_to_kif(
    game: "ShogiGame",
    filename: Optional[str] = None,
    sente_player_name: str = "Sente",
    gote_player_name: str = "Gote",
) -> Optional[str]:
    """
    Converts a game to a KIF file or string representation.
    Uses standard KIF piece notation (+FU, -FU, etc.) and includes more headers.

    Args:
        game: The ShogiGame to convert
        filename: If provided, the KIF will be written to this file
        sente_player_name: Name of the black/sente player
        gote_player_name: Name of the white/gote player

    Returns:
        If filename is None, returns a string representation of the KIF.
        Otherwise returns None after writing to the file.
    """

    # Helper function to create KIF content
    def create_kif_content():
        lines = []
        # --- KIF Headers ---
        lines.append("#KIF version=2.0 encoding=UTF-8")
        lines.append("*Event: Casual Game")
        lines.append("*Site: Local Machine")
        lines.append(f"*Date: {datetime.date.today().strftime('%Y/%m/%d')}")
        lines.append(f"*Player Sente: {sente_player_name}")
        lines.append(f"*Player Gote: {gote_player_name}")
        lines.append("*Handicap: HIRATE")

        # Standard HIRATE starting position
        lines.append("P1-KY-KE-GI-KI-OU-KI-GI-KE-KY")
        lines.append("P2 * -HI * * * * * -KA * ")
        lines.append("P3-FU-FU-FU-FU-FU-FU-FU-FU-FU")
        lines.append("P4 * * * * * * * * * ")
        lines.append("P5 * * * * * * * * * ")
        lines.append("P6 * * * * * * * * * ")
        lines.append("P7+FU+FU+FU+FU+FU+FU+FU+FU+FU")
        lines.append("P8 * +KA * * * * * +HI * ")
        lines.append("P9+KY+KE+GI+KI+OU+KI+GI+KE+KY")

        # --- Initial Hands (KIF format: P+00FU00KY... for Sente, P-00FU00KY... for Gote) ---
        # This assumes starting with empty hands for a standard game from initial board setup.
        sente_hand_str: str = "P+"
        gote_hand_str: str = "P-"
        hand_order_for_kif: List[PieceType] = [
            PieceType.ROOK,
            PieceType.BISHOP,
            PieceType.GOLD,
            PieceType.SILVER,
            PieceType.KNIGHT,
            PieceType.LANCE,
            PieceType.PAWN,
        ]  # Common KIF hand order

        initial_sente_hand: Dict[PieceType, int] = game.hands[Color.BLACK.value]
        initial_gote_hand: Dict[PieceType, int] = game.hands[Color.WHITE.value]

        for pt in hand_order_for_kif:
            sente_hand_str += (
                f"{initial_sente_hand.get(pt, 0):02d}{KIF_PIECE_SYMBOLS.get(pt, '??')}"
            )
            gote_hand_str += (
                f"{initial_gote_hand.get(pt, 0):02d}{KIF_PIECE_SYMBOLS.get(pt, '??')}"
            )
        lines.append(f"{sente_hand_str}")
        lines.append(f"{gote_hand_str}")

        # --- Player to move first ---
        lines.append(
            f"{'+' if game.current_player == Color.BLACK else '-'}"
        )  # + for Sente, - for Gote

        lines.append("moves")  # Start of the moves section

        # --- Moves ---
        for i, move_entry in enumerate(game.move_history):
            move_obj: Optional[MoveTuple] = move_entry.get(
                "move"
            )  # Your internal move object/tuple
            if not move_obj:
                continue
            # Defensive: ensure all indices are not None
            if (
                move_obj[0] is None
                or move_obj[1] is None
                or move_obj[2] is None
                or move_obj[3] is None
            ):
                continue  # Skip malformed move
            usi_move_str: str = (
                f"{move_obj[0]+1}{chr(move_obj[1]+ord('a'))}{move_obj[2]+1}{chr(move_obj[3]+ord('a'))}"
            )
            if move_obj[4]:  # Promote flag
                usi_move_str += "+"
            lines.append(f"{i+1} {usi_move_str}")

        # --- Game Termination ---
        if game.game_over:
            termination_map: Dict[str, str] = {
                "Tsumi": "詰み",
                "Toryo": "投了",
                "Sennichite": "千日手",
                "Stalemate": "持将棋",
                "Max moves reached": "持将棋",  # Or "最大手数" or similar
                # Add other mappings for values set in game.termination_reason
            }
            reason_str: Optional[str] = game.termination_reason
            kif_termination_reason_display: str

            if reason_str is None:
                kif_termination_reason_display = ""  # No reason string if None
            else:
                # Now reason_str is str, so termination_map.get(str, str) is used
                kif_termination_reason_display = termination_map.get(
                    reason_str, reason_str
                )

            if kif_termination_reason_display:
                lines.append(kif_termination_reason_display)

            # Append the RESULT line based on winner
            if game.winner == Color.BLACK:
                lines.append("RESULT:SENTE_WIN")
            elif game.winner == Color.WHITE:
                lines.append("RESULT:GOTE_WIN")
            elif game.winner is None:  # Draw conditions
                # More specific draw reasons could be mapped to KIF draw results
                if game.termination_reason in [
                    TerminationReason.REPETITION.value,
                    TerminationReason.IMPASSE.value,
                    TerminationReason.MAX_MOVES_EXCEEDED.value,
                ]:
                    lines.append("RESULT:DRAW")  # Or HIKIWAKE etc.

        lines.append("*EOF")  # Standard KIF end marker
        return "\n".join(lines)

    # Generate KIF content
    kif_content = create_kif_content()

    # Either write to file or return as string
    if filename:
        with open(filename, "w", encoding="utf-8") as kif_file:
            kif_file.write(kif_content)
        return None
    else:
        return kif_content


# --- SFEN Move Parsing ---


def _parse_sfen_square(sfen_sq: str) -> Tuple[int, int]:
    """
    Converts an SFEN square string (e.g., "7g", "5e") to 0-indexed (row, col).
    File (1-9) maps to column (8-0). Rank (a-i) maps to row (0-8).
    Example: "9a" -> (0,0), "1i" -> (8,8)
    """
    if not (
        len(sfen_sq) == 2 and "1" <= sfen_sq[0] <= "9" and "a" <= sfen_sq[1] <= "i"
    ):
        raise ValueError(f"Invalid SFEN square format: {sfen_sq}")

    file_char: str = sfen_sq[0]
    rank_char: str = sfen_sq[1]

    col: int = 9 - int(file_char)
    row: int = ord(rank_char) - ord("a")

    return row, col


def _get_piece_type_from_sfen_char(char: str) -> PieceType:
    """
    Converts an SFEN piece character (e.g., 'P', 'L', 'B') to a PieceType enum.
    SFEN uses uppercase single letters for standard pieces.
    """
    if char in SYMBOL_TO_PIECE_TYPE:
        pt = SYMBOL_TO_PIECE_TYPE[char]
        # Ensure it's a basic piece type that can be dropped
        if (
            pt in get_unpromoted_types() and pt != PieceType.KING
        ):  # King cannot be dropped
            return pt
    raise ValueError(f"Invalid SFEN piece character for drop: {char}")


def sfen_to_move_tuple(sfen_move_str: str) -> MoveTuple:
    """
    Parses an SFEN move string (e.g., "7g7f", "P*5e", "2b3a+")
    and converts it into an internal MoveTuple using regular expressions.
    """
    sfen_move_str = sfen_move_str.strip()

    # Regex for drop moves: e.g., "P*5e"
    # Group 1: Piece character (P, L, N, S, G, B, R)
    # Group 2: Square (e.g., 5e)
    drop_move_pattern = re.compile(r"^([PLNSGBR])\*([1-9][a-i])$")

    # Regex for board moves: e.g., "7g7f", "2b3a+"
    # Group 1: From square (e.g., 7g)
    # Group 2: To square (e.g., 7f)
    # Group 3: Optional promotion character (+)
    board_move_pattern = re.compile(r"^([1-9][a-i])([1-9][a-i])(\+)?$")

    drop_match = drop_move_pattern.match(sfen_move_str)
    if drop_match:
        piece_char: str = drop_match.group(1)
        sfen_sq_to: str = drop_match.group(2)

        try:
            piece_to_drop: PieceType = _get_piece_type_from_sfen_char(piece_char)
            r_to, c_to = _parse_sfen_square(sfen_sq_to)
            return (None, None, r_to, c_to, piece_to_drop)
        except ValueError as e:
            raise ValueError(
                f"Error parsing SFEN drop move '{sfen_move_str}': {e}"
            ) from e

    board_match = board_move_pattern.match(sfen_move_str)
    if board_match:
        sfen_sq_from_str: str = board_match.group(1)
        sfen_sq_to_str: str = board_match.group(2)
        promote_flag: bool = board_match.group(3) is not None

        try:
            r_from, c_from = _parse_sfen_square(sfen_sq_from_str)
            r_to, c_to = _parse_sfen_square(sfen_sq_to_str)
            return (r_from, c_from, r_to, c_to, promote_flag)
        except ValueError as e:
            raise ValueError(
                f"Error parsing SFEN board move '{sfen_move_str}': {e}"
            ) from e

    raise ValueError(f"Invalid SFEN move format: {sfen_move_str}")


# TODO: Consider adding kif_to_game and sfen_to_game functions if needed.
# These would involve more complex parsing of full game states or move sequences.

]]></file>
  <file path="shogi/__init__.py"><![CDATA[
"""
Shogi Module - Japanese Chess game engine and related components.

This package contains all the components related to the Shogi game:
- Core definitions: piece types, colors, move tuples
- Game logic and rules
- Move execution
- Game input/output utilities
"""

# Export the main components for easy access
from .shogi_core_definitions import Color, MoveTuple, Piece, PieceType
from .shogi_game import ShogiGame

__all__ = [
    # Core types
    "Color",
    "PieceType",
    "Piece",
    "MoveTuple",
    # Game class
    "ShogiGame",
]

]]></file>
  <file path="core/experience_buffer.py"><![CDATA[
"""
Minimal ExperienceBuffer for DRL Shogi Client.
"""

# import numpy as np
import torch  # Ensure torch is imported


class ExperienceBuffer:
    """Experience buffer for storing transitions during RL training."""

    def __init__(
        self, buffer_size: int, gamma: float, lambda_gae: float, device: str = "cpu"
    ):
        self.buffer_size = buffer_size
        self.gamma = gamma
        self.lambda_gae = lambda_gae
        self.device = torch.device(device)  # Store as torch.device

        # Initialize buffers as empty lists
        self.obs: list[torch.Tensor] = []  # Assuming obs are stored as tensors
        self.actions: list[int] = []
        self.rewards: list[float] = []
        self.log_probs: list[float] = []
        self.values: list[float] = []
        self.dones: list[bool] = []
        self.legal_masks: list[torch.Tensor] = []  # Added to store legal masks
        self.advantages: list[torch.Tensor] = (
            []
        )  # Populated by compute_advantages_and_returns
        self.returns: list[torch.Tensor] = (
            []
        )  # Populated by compute_advantages_and_returns
        self.ptr = 0

    def add(
        self,
        obs: torch.Tensor,
        action: int,
        reward: float,
        log_prob: float,
        value: float,
        done: bool,
        legal_mask: torch.Tensor,  # Added legal_mask
    ):
        """
        Add a transition to the buffer.
        'obs' is expected to be a PyTorch tensor of shape (C, H, W) on self.device.
        'legal_mask' is expected to be a PyTorch tensor on self.device.
        """
        if self.ptr < self.buffer_size:
            # obs should already be on self.device and have shape (C,H,W) when passed from train.py
            self.obs.append(obs)
            self.actions.append(action)
            self.rewards.append(reward)
            self.log_probs.append(log_prob)
            self.values.append(value)  # Storing scalar value estimates
            self.dones.append(done)
            self.legal_masks.append(legal_mask)  # Store legal_mask
            self.ptr += 1
        else:
            # This case should ideally be handled by the training loop,
            # which calls learn() and then clear() when buffer is full.
            print("Warning: ExperienceBuffer is full. Cannot add new experience.")

    def compute_advantages_and_returns(
        self, last_value: float
    ):  # last_value is a float
        """
        Computes Generalized Advantage Estimation (GAE) and returns for the collected experiences.
        This should be called after the buffer is full (i.e., self.ptr == self.buffer_size).
        Uses PyTorch tensor operations for GAE calculation.
        """
        if self.ptr == 0:
            print("Warning: compute_advantages_and_returns called on an empty buffer.")
            self.advantages = []
            self.returns = []
            return

        # Convert lists to tensors for GAE computation
        rewards_tensor = torch.tensor(
            self.rewards[: self.ptr], dtype=torch.float32, device=self.device
        )
        values_tensor = torch.tensor(
            self.values[: self.ptr], dtype=torch.float32, device=self.device
        )
        # Dones tensor: 1.0 if not done, 0.0 if done (for mask)
        dones_tensor = torch.tensor(
            self.dones[: self.ptr], dtype=torch.float32, device=self.device
        )
        masks_tensor = 1.0 - dones_tensor

        advantages_list: list[torch.Tensor] = [
            torch.tensor(0.0, device=self.device)
        ] * self.ptr
        returns_list: list[torch.Tensor] = [
            torch.tensor(0.0, device=self.device)
        ] * self.ptr
        gae = torch.tensor(0.0, device=self.device)  # Ensure gae is always a tensor

        # last_value is V(S_t+1) for the last state in the buffer
        # If the last state was terminal, next_value should be 0, handled by mask.
        next_value_tensor = torch.tensor(
            [last_value], dtype=torch.float32, device=self.device
        )

        for t in reversed(range(self.ptr)):
            if t == self.ptr - 1:
                current_next_value = next_value_tensor
            else:
                current_next_value = values_tensor[t + 1]

            delta = (
                rewards_tensor[t]
                + self.gamma * current_next_value * masks_tensor[t]
                - values_tensor[t]
            )
            gae = delta + self.gamma * self.lambda_gae * masks_tensor[t] * gae

            advantages_list[t] = gae  # Store as tensor
            returns_list[t] = gae + values_tensor[t]  # Store as tensor

        self.advantages = advantages_list
        self.returns = returns_list

    def get_batch(self) -> dict:
        """
        Returns all collected experiences as a dictionary of PyTorch tensors on self.device.
        Assumes compute_advantages_and_returns has been called.
        """
        if self.ptr == 0:
            # This should be handled by PPOAgent.learn() checking for empty batch_data
            # For safety, one might return structured empty tensors if needed upstream.
            print("Warning: get_batch called on an empty or not-yet-computed buffer.")
            return {}  # PPOAgent.learn already checks for this

        num_samples = self.ptr

        # --- Efficient Batching for Observations ---
        # self.obs is a list of tensors, each (C,H,W), already on self.device.
        # Stack them along a new batch dimension (dim=0).
        try:
            obs_tensor = torch.stack(self.obs[:num_samples], dim=0)
        except RuntimeError as e:
            # This might happen if tensors in self.obs are not on the same device or have inconsistent shapes.
            # Should not happen if train.py's add() is consistent.
            print(f"Error stacking observation tensors in ExperienceBuffer: {e}")
            # Fallback or re-raise, for now, let's indicate a problem by returning empty.
            return {}

        # --- Convert other lists to tensors ---
        actions_tensor = torch.tensor(
            self.actions[:num_samples], dtype=torch.int64, device=self.device
        )
        log_probs_tensor = torch.tensor(
            self.log_probs[:num_samples], dtype=torch.float32, device=self.device
        )
        # self.values are scalar V(s_t) estimates stored as floats
        values_tensor = torch.tensor(
            self.values[:num_samples], dtype=torch.float32, device=self.device
        )

        advantages_tensor = torch.stack(self.advantages[:num_samples])
        returns_tensor = torch.stack(self.returns[:num_samples])

        # Dones can be bool or float, PPO often uses float for masking.
        dones_tensor = torch.tensor(
            self.dones[:num_samples], dtype=torch.bool, device=self.device
        )

        # Stack legal_masks (list of tensors) into a single tensor
        # Assuming legal_masks are already on self.device
        try:
            legal_masks_tensor = torch.stack(self.legal_masks[:num_samples], dim=0)
        except RuntimeError as e:
            print(f"Error stacking legal_mask tensors in ExperienceBuffer: {e}")
            return {}

        return {
            "obs": obs_tensor,
            "actions": actions_tensor,
            "log_probs": log_probs_tensor,  # These are old_log_probs for PPO
            "values": values_tensor,  # These are old_V(s_t) for PPO, used with GAE
            "advantages": advantages_tensor,
            "returns": returns_tensor,  # These are the GAE-based returns (targets for value func)
            "dones": dones_tensor,  # For record keeping or if needed by learn()
            "legal_masks": legal_masks_tensor,  # Added legal_masks_tensor
        }

    def clear(self):
        """Clears all stored experiences from the buffer."""
        self.obs.clear()
        self.actions.clear()
        self.rewards.clear()
        self.log_probs.clear()
        self.values.clear()
        self.dones.clear()
        self.legal_masks.clear()  # Clear legal_masks
        self.advantages.clear()
        self.returns.clear()
        self.ptr = 0

    def __len__(self):
        """Return the number of transitions currently in the buffer."""
        return self.ptr

]]></file>
  <file path="core/ppo_agent.py"><![CDATA[
"""
Minimal PPOAgent for DRL Shogi Client.
"""

import os
import sys  # For stderr
from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple

import numpy as np
import torch
import torch.nn.functional as F

from keisei.config_schema import AppConfig
from keisei.core.experience_buffer import ExperienceBuffer
from keisei.core.neural_network import ActorCritic
from keisei.utils import PolicyOutputMapper

if TYPE_CHECKING:
    from keisei.shogi.shogi_core_definitions import MoveTuple


class PPOAgent:
    """Proximal Policy Optimization agent for Shogi (PPO logic)."""

    def __init__(
        self,
        config: AppConfig,
        device: torch.device,
        name: str = "PPOAgent",
    ):
        """
        Initialize the PPOAgent with model, optimizer, and PPO hyperparameters.
        """
        self.config = config
        self.device = device
        self.name = name

        input_channels = config.env.input_channels
        policy_output_mapper = PolicyOutputMapper()

        self.policy_output_mapper = policy_output_mapper
        self.num_actions_total = self.policy_output_mapper.get_total_actions()
        self.model = ActorCritic(input_channels, self.num_actions_total).to(self.device)
        self.optimizer = torch.optim.Adam(
            self.model.parameters(), lr=config.training.learning_rate
        )

        # PPO hyperparameters
        self.gamma = config.training.gamma
        self.clip_epsilon = config.training.clip_epsilon
        self.value_loss_coeff = config.training.value_loss_coeff
        self.entropy_coef = config.training.entropy_coef
        self.ppo_epochs = config.training.ppo_epochs
        self.minibatch_size = config.training.minibatch_size

        self.last_kl_div = 0.0  # Initialize KL divergence tracker
        self.gradient_clip_max_norm = config.training.gradient_clip_max_norm # Added from config

    def select_action(
        self,
        obs: np.ndarray,
        legal_mask: torch.Tensor,
        *,  # Force is_training to be keyword-only
        is_training: bool = True,
    ) -> Tuple[
        Optional["MoveTuple"],
        int,
        float,
        float,
    ]:
        """
        Select an action given an observation, legal Shogi moves, and a precomputed legal_mask.
        Returns the selected Shogi move, its policy index, log probability, and value estimate.
        """
        self.model.train(is_training)

        obs_tensor = torch.tensor(
            obs, dtype=torch.float32, device=self.device
        ).unsqueeze(0)

        if not legal_mask.any():
            print(
                "Warning: PPOAgent.select_action called with no legal moves (based on input legal_mask).",
                file=sys.stderr,
            )
            # Fallback behavior might be needed if model.get_action_and_value can't handle all-False mask.
            # neural_network.py's get_action_and_value attempts to handle this.
            # If this path is hit, it implies the caller might not have checked for no legal moves.
            # The train.py logic should ideally prevent calling select_action if no legal_moves.
            # Let it proceed, model.get_action_and_value will use the all-false mask.

        # Get action, log_prob, and value from the ActorCritic model
        # Pass deterministic based on not is_training
        (
            selected_policy_index_tensor,
            log_prob_tensor,
            value_tensor,
        ) = self.model.get_action_and_value(
            obs_tensor, legal_mask=legal_mask, deterministic=not is_training
        )

        selected_policy_index_val = int(selected_policy_index_tensor.item())
        log_prob_val = float(log_prob_tensor.item())
        value_float = float(
            value_tensor.item()
        )  # Value is already squeezed in get_action_and_value

        selected_shogi_move: Optional["MoveTuple"] = None
        try:
            selected_shogi_move = self.policy_output_mapper.policy_index_to_shogi_move(
                selected_policy_index_val
            )
        except IndexError as e:
            print(
                f"Error in PPOAgent.select_action: Policy index {selected_policy_index_val} out of bounds. {e}",
                file=sys.stderr,
            )
            # Handle by returning no move or re-raising, depending on desired robustness.
            return None, -1, 0.0, value_float
            # Or raise the error

        return (
            selected_shogi_move,
            selected_policy_index_val,
            log_prob_val,
            value_float,
        )

    def get_value(self, obs_np: np.ndarray) -> float:
        """Get the value prediction from the critic for a given NumPy observation."""
        self.model.eval()
        obs_tensor = torch.tensor(
            obs_np, dtype=torch.float32, device=self.device
        ).unsqueeze(0)
        with torch.no_grad():
            _, _, value_estimate = self.model.get_action_and_value(
                obs_tensor, deterministic=True
            )
        return float(value_estimate.item())

    def learn(self, experience_buffer: ExperienceBuffer) -> Dict[str, float]:
        """
        Perform PPO update using experiences from the buffer.
        Returns a dictionary of logging metrics.
        """
        self.model.train()

        batch_data = experience_buffer.get_batch()
        # It's good practice for ExperienceBuffer.get_batch() to already place tensors on self.device
        # or for the buffer itself to live on self.device if memory allows.
        # If not, the .to(self.device) calls below are necessary.

        current_lr = self.optimizer.param_groups[0]["lr"]

        if not batch_data or batch_data["obs"].shape[0] == 0:
            print(
                "Warning: PPOAgent.learn called with empty batch_data.", file=sys.stderr
            )
            return {
                "ppo/policy_loss": 0.0,
                "ppo/value_loss": 0.0,
                "ppo/entropy": 0.0,
                "ppo/kl_divergence_approx": self.last_kl_div,
                "ppo/learning_rate": current_lr,
            }

        obs_batch = batch_data["obs"].to(self.device)
        actions_batch = batch_data["actions"].to(self.device)
        old_log_probs_batch = batch_data["log_probs"].to(self.device)
        advantages_batch = batch_data["advantages"].to(self.device)
        returns_batch = batch_data["returns"].to(self.device)
        legal_masks_batch = batch_data["legal_masks"].to(self.device)

        # Normalize advantages
        advantages_batch = (advantages_batch - advantages_batch.mean()) / (
            advantages_batch.std() + 1e-8
        )

        num_samples = obs_batch.shape[0]
        indices = np.arange(num_samples)

        total_policy_loss_epoch, total_value_loss_epoch, total_entropy_epoch = (
            0.0,
            0.0,
            0.0,
        )
        num_updates = 0

        for _ in range(self.ppo_epochs):
            np.random.shuffle(indices)
            for start_idx in range(0, num_samples, self.minibatch_size):
                end_idx = start_idx + self.minibatch_size
                minibatch_indices = indices[start_idx:end_idx]

                obs_minibatch = obs_batch[minibatch_indices]
                actions_minibatch = actions_batch[minibatch_indices]
                old_log_probs_minibatch = old_log_probs_batch[minibatch_indices]
                advantages_minibatch = advantages_batch[minibatch_indices]
                returns_minibatch = returns_batch[minibatch_indices]
                legal_masks_minibatch = legal_masks_batch[minibatch_indices]

                # Get new log_probs, entropy, and value from the model
                # Note on entropy: legal_mask is now passed here. Entropy is calculated
                # over legal actions only.
                new_log_probs, entropy, new_values = self.model.evaluate_actions(
                    obs_minibatch,
                    actions_minibatch,
                    legal_mask=legal_masks_minibatch,
                )

                # PPO Loss Calculation
                ratio = torch.exp(new_log_probs - old_log_probs_minibatch)

                # Clipped surrogate objective
                surr1 = ratio * advantages_minibatch
                surr2 = (
                    torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon)
                    * advantages_minibatch
                )
                policy_loss = -torch.min(surr1, surr2).mean()

                # Value loss (MSE)
                value_loss = F.mse_loss(
                    new_values.squeeze(), returns_minibatch.squeeze()
                )

                # Entropy bonus
                entropy_loss = -entropy.mean()

                # Total loss
                loss = (
                    policy_loss
                    + self.value_loss_coeff * value_loss
                    + self.entropy_coef * entropy_loss
                )

                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), max_norm=self.gradient_clip_max_norm # Use config value
                )  # Optional: gradient clipping
                self.optimizer.step()

                total_policy_loss_epoch += policy_loss.item()
                total_value_loss_epoch += value_loss.item()
                total_entropy_epoch += entropy_loss.item()
                num_updates += 1

        avg_policy_loss = (
            total_policy_loss_epoch / num_updates if num_updates > 0 else 0.0
        )
        avg_value_loss = (
            total_value_loss_epoch / num_updates if num_updates > 0 else 0.0
        )
        avg_entropy = total_entropy_epoch / num_updates if num_updates > 0 else 0.0
        kl_divergence_final_approx = 0.0

        if num_updates > 0:
            with torch.no_grad():
                # For KL divergence, we need to evaluate actions with the current policy
                # considering the legal masks that were active when those actions were chosen.
                # The ActorCritic.evaluate_actions method handles the legal_mask internally
                # for calculating log_probs and entropy. For KL, we need the log_probs
                # from the current policy for the actions taken, using the same legal_masks.
                # The call to evaluate_actions for the full batch (if needed for KL) should also pass legal_masks_batch.
                # However, the current KL approximation uses model(obs_batch) which doesn't take legal_mask.
                # For a more accurate KL involving legal actions, the distribution from model()
                # would need to be masked before calculating log_prob.
                # For simplicity, current KL approx is kept, but note this subtlety.

                # Re-evaluate log_probs for the entire batch with current policy and original legal masks
                # to get a consistent comparison for KL divergence.
                current_log_probs_for_kl, _, _ = self.model.evaluate_actions(
                    obs_batch, actions_batch, legal_mask=legal_masks_batch
                )
                kl_divergence_final_approx = (
                    (old_log_probs_batch - current_log_probs_for_kl).mean().item()
                )

        self.last_kl_div = kl_divergence_final_approx

        metrics: Dict[str, float] = {
            "ppo/policy_loss": avg_policy_loss,
            "ppo/value_loss": avg_value_loss,
            "ppo/entropy": avg_entropy,
            "ppo/kl_divergence_approx": self.last_kl_div,
            "ppo/learning_rate": current_lr,
        }
        return metrics

    def save_model(
        self,
        file_path: str,
        global_timestep: int = 0,
        total_episodes_completed: int = 0,
        stats_to_save: Optional[Dict[str, int]] = None,  # MODIFIED: Added stats_to_save
    ) -> None:
        """Saves the model, optimizer, and training state to a file."""
        save_dict = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "global_timestep": global_timestep,
            "total_episodes_completed": total_episodes_completed,
        }
        if stats_to_save:
            save_dict.update(stats_to_save)  # Add black_wins, white_wins, draws

        torch.save(save_dict, file_path)
        print(f"PPOAgent model, optimizer, and state saved to {file_path}")

    def load_model(
        self, file_path: str
    ) -> Dict[str, Any]:  # MODIFIED: Return type to Dict
        """Loads the model, optimizer, and training state from a file."""
        if not os.path.exists(file_path):
            print(f"Error: Checkpoint file {file_path} not found.", file=sys.stderr)
            # Return a dictionary indicating failure or default values
            return {
                "global_timestep": 0,
                "total_episodes_completed": 0,
                "black_wins": 0,
                "white_wins": 0,
                "draws": 0,
                "error": "File not found",
            }
        try:
            checkpoint = torch.load(file_path, map_location=self.device)
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

            # Return all data from checkpoint for the caller to use
            # Defaults are provided for backward compatibility if keys are missing
            return {
                "global_timestep": checkpoint.get("global_timestep", 0),
                "total_episodes_completed": checkpoint.get(
                    "total_episodes_completed", 0
                ),
                "black_wins": checkpoint.get("black_wins", 0),
                "white_wins": checkpoint.get("white_wins", 0),
                "draws": checkpoint.get("draws", 0),
                # Include other potential data if needed in the future
            }
        except (KeyError, RuntimeError, EOFError) as e:
            print(f"Error loading checkpoint from {file_path}: {e}", file=sys.stderr)
            # Return a dictionary indicating failure or default values
            return {
                "global_timestep": 0,
                "total_episodes_completed": 0,
                "black_wins": 0,
                "white_wins": 0,
                "draws": 0,
                "error": str(e),
            }

    def get_name(self) -> str:  # Added getter for name
        return self.name

]]></file>
  <file path="core/neural_network.py"><![CDATA[
"""
Minimal ActorCritic neural network for DRL Shogi Client (dummy forward pass).
"""

import sys  # For stderr
from typing import Optional, Tuple  # Added Optional

import torch
import torch.nn.functional as F
from torch import nn  # Corrected import


class ActorCritic(nn.Module):
    """Actor-Critic neural network for Shogi RL agent (PPO-ready)."""

    def __init__(self, input_channels: int, num_actions_total: int):
        """Initialize the ActorCritic network with convolutional and linear layers."""
        super().__init__()
        self.conv = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.flatten = nn.Flatten()
        self.policy_head = nn.Linear(16 * 9 * 9, num_actions_total)
        self.value_head = nn.Linear(16 * 9 * 9, 1)

    def forward(self, x):
        """Forward pass: returns policy logits and value estimate."""
        x = self.conv(x)
        x = self.relu(x)
        x = self.flatten(x)
        policy_logits = self.policy_head(x)
        value = self.value_head(x)
        return policy_logits, value

    def get_action_and_value(
        self,
        obs: torch.Tensor,
        legal_mask: Optional[torch.Tensor] = None,
        deterministic: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Given an observation (and optional legal action mask), return a sampled or deterministically chosen action,
        its log probability, and value estimate.
        Args:
            obs: Input observation tensor.
            legal_mask: Optional boolean tensor indicating legal actions.
                        If provided, illegal actions will be masked out before sampling/argmax.
            deterministic: If True, choose the action with the highest probability (argmax).
                           If False, sample from the distribution.
        Returns:
            action: Chosen action tensor.
            log_prob: Log probability of the chosen action.
            value: Value estimate tensor.
        """
        policy_logits, value = self.forward(obs)

        if legal_mask is not None:
            # Apply the legal mask: set logits of illegal actions to -infinity
            # Ensure legal_mask has the same shape as policy_logits or is broadcastable
            if (
                legal_mask.ndim == 1
                and policy_logits.ndim == 2
                and policy_logits.shape[0] == 1
            ):
                legal_mask = legal_mask.unsqueeze(0)  # Adapt for batch size 1

            masked_logits = torch.where(
                legal_mask,
                policy_logits,
                torch.tensor(float("-inf"), device=policy_logits.device),
            )
            # Handle case where all masked_logits are -inf to prevent NaN in softmax
            if not torch.any(legal_mask):  # Or check if masked_logits are all -inf
                # If no legal moves, softmax over original logits might be one option,
                # or let it produce NaNs which should be caught upstream.
                # For now, this will lead to NaNs if all are masked.
                # This situation should ideally be caught by the caller (e.g. PPOAgent)
                pass  # Let it proceed, PPOAgent should handle no legal moves.
            probs = F.softmax(masked_logits, dim=-1)
        else:
            probs = F.softmax(policy_logits, dim=-1)

        # Check for NaNs in probs, which can happen if all logits were -inf
        if torch.isnan(probs).any():
            # This is a fallback: if probs are NaN (e.g. all legal actions masked out and all logits became -inf),
            # distribute probability uniformly over all actions to avoid erroring out in Categorical.
            # A better solution is for the caller to handle "no legal actions" gracefully.
            print(
                "Warning: NaNs in probabilities in ActorCritic.get_action_and_value. Check legal_mask and logits. Defaulting to uniform.",
                file=sys.stderr,
            )
            probs = torch.ones_like(policy_logits) / policy_logits.shape[-1]

        dist = torch.distributions.Categorical(probs=probs)

        if deterministic:
            action = torch.argmax(probs, dim=-1)
        else:
            action = dist.sample()

        log_prob = dist.log_prob(action)
        return action, log_prob, value.squeeze(-1)  # Squeeze value to match typical use

    def evaluate_actions(
        self,
        obs: torch.Tensor,
        actions: torch.Tensor,
        legal_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate the log probabilities, entropy, and value for given observations and actions.
        Args:
            obs: Input observation tensor.
            actions: Actions taken tensor.
            legal_mask: Optional boolean tensor indicating legal actions.
                        If provided, it can be used to ensure probabilities are calculated correctly,
                        though for evaluating *taken* actions, it's often assumed they were legal.
                        It primarily affects entropy calculation if used to restrict the distribution.
        Returns:
            log_probs: Log probabilities of the taken actions.
            entropy: Entropy of the action distribution.
            value: Value estimate tensor.
        """
        policy_logits, value = self.forward(obs)

        # If legal_mask is None (e.g., when called from PPOAgent.learn during batch processing),
        # the policy distribution (probs) and entropy are calculated over all possible actions,
        # not just those that were legal in the specific states from which 'actions' were sampled.
        # This is a common approach. To calculate entropy strictly over the legal action space
        # for each state in the batch, legal masks for each observation in obs_minibatch
        # would need to be stored in the experience buffer and passed here.
        if legal_mask is not None:
            # Apply the legal mask for calculating probabilities and entropy correctly
            # Ensure legal_mask has the same shape as policy_logits or is broadcastable

            # The shape of legal_mask should be (batch_size, num_actions)
            # The shape of policy_logits is (batch_size, num_actions)
            # No unsqueezing or broadcasting adjustment should be needed here if shapes are consistent.
            # The previous check for legal_mask.ndim == 1 was more for get_action_and_value with batch_size=1.
            # Here, we expect legal_mask to match policy_logits if provided.

            masked_logits = torch.where(
                legal_mask,
                policy_logits,
                torch.tensor(float("-inf"), device=policy_logits.device),
            )
            probs = F.softmax(masked_logits, dim=-1)
        else:
            probs = F.softmax(policy_logits, dim=-1)

        # Check for NaNs in probs (e.g. if all logits in a row were -inf due to masking)
        # Replace NaNs with uniform distribution for stability in entropy calculation for those rows
        if torch.isnan(probs).any():
            print(
                "Warning: NaNs in probabilities in ActorCritic.evaluate_actions. Check legal_mask and logits. Defaulting to uniform for affected rows.",
                file=sys.stderr,
            )
            nan_rows = torch.isnan(probs).any(dim=1)
            probs[nan_rows] = torch.ones_like(probs[nan_rows]) / policy_logits.shape[-1]

        dist = torch.distributions.Categorical(probs=probs)
        log_probs = dist.log_prob(actions)
        entropy = dist.entropy()
        return log_probs, entropy, value.squeeze(-1)  # Squeeze value

]]></file>
  <file path="core/__init__.py"><![CDATA[
# keisei/core/__init__.py

]]></file>
  <file path="utils/move_formatting.py"><![CDATA[
"""
move_formatting.py: Contains utilities for formatting Shogi moves.
"""

from keisei.shogi.shogi_core_definitions import PieceType


def format_move_with_description(selected_shogi_move, policy_output_mapper, game=None):
    """
    Formats a shogi move with USI notation and English description.

    Args:
        selected_shogi_move: The MoveTuple (either BoardMoveTuple or DropMoveTuple)
        policy_output_mapper: PolicyOutputMapper instance for USI conversion
        game: Optional ShogiGame instance for getting piece information

    Returns:
        str: Formatted string like "7g7f (pawn move to 7f)" or "P*5e (pawn drop to 5e)"
    """
    if selected_shogi_move is None:
        return "None"

    try:
        # Get USI notation
        usi_notation = policy_output_mapper.shogi_move_to_usi(selected_shogi_move)

        # Determine if it's a drop or board move and create description
        if len(selected_shogi_move) == 5 and selected_shogi_move[0] is None:
            # Drop move: (None, None, to_r, to_c, piece_type)
            _, _, to_r, to_c, piece_type = selected_shogi_move
            piece_name = _get_piece_name(piece_type, False)
            to_square = _coords_to_square_name(to_r, to_c)
            description = f"{piece_name} drop to {to_square}"
        else:
            # Board move: (from_r, from_c, to_r, to_c, promote_flag)
            from_r, from_c, to_r, to_c, promote_flag = selected_shogi_move
            from_square = _coords_to_square_name(from_r, from_c)
            to_square = _coords_to_square_name(to_r, to_c)

            # Try to get piece information from game if available
            piece_name = "piece"
            if game is not None:
                try:
                    piece = game.get_piece(from_r, from_c)
                    if piece is not None:
                        piece_name = _get_piece_name(piece.type, promote_flag)
                except (AttributeError, KeyError, TypeError):
                    pass  # Fall back to generic "piece"
            else:
                # Without game context, assume it's a piece that can promote if promote_flag is True
                if promote_flag:
                    piece_name = "piece promoting"

            description = f"{piece_name} moving from {from_square} to {to_square}"

        return f"{usi_notation} - {description}."

    except Exception as e:  # pylint: disable=broad-except
        # Fallback to string representation if formatting fails
        return f"{str(selected_shogi_move)} (format error: {e})"


def format_move_with_description_enhanced(
    selected_shogi_move, policy_output_mapper, piece_info=None
):
    """
    Enhanced move formatting that takes piece info as parameter for better demo logging.

    Args:
        selected_shogi_move: The MoveTuple (either BoardMoveTuple or DropMoveTuple)
        policy_output_mapper: PolicyOutputMapper instance for USI conversion
        piece_info: Piece object from game.get_piece() call made before the move

    Returns:
        str: Formatted string like "7g7f - Fuhyō (Pawn) moving from 7g to 7f."
    """
    if selected_shogi_move is None:
        return "None"

    try:
        # Get USI notation
        usi_notation = policy_output_mapper.shogi_move_to_usi(selected_shogi_move)

        # Determine if it's a drop or board move and create description
        if len(selected_shogi_move) == 5 and selected_shogi_move[0] is None:
            # Drop move: (None, None, to_r, to_c, piece_type)
            _, _, to_r, to_c, piece_type = selected_shogi_move
            piece_name = _get_piece_name(piece_type, False)
            to_square = _coords_to_square_name(to_r, to_c)
            description = f"{piece_name} drop to {to_square}"
        else:
            # Board move: (from_r, from_c, to_r, to_c, promote_flag)
            from_r, from_c, to_r, to_c, promote_flag = selected_shogi_move
            from_square = _coords_to_square_name(from_r, from_c)
            to_square = _coords_to_square_name(to_r, to_c)

            # Use the piece info passed as parameter if available
            piece_name = "piece"
            if piece_info is not None:
                try:
                    piece_name = _get_piece_name(piece_info.type, promote_flag)
                except (AttributeError, KeyError, TypeError):
                    piece_name = "piece"  # Fall back to generic "piece"

            description = f"{piece_name} moving from {from_square} to {to_square}"

        return f"{usi_notation} - {description}."

    except Exception as e:  # pylint: disable=broad-except
        # Fallback to string representation if formatting fails
        return f"{str(selected_shogi_move)} (format error: {e})"


def _get_piece_name(piece_type, is_promoting=False):
    """Convert PieceType enum to Japanese name with English translation."""
    piece_names = {
        PieceType.PAWN: "Fuhyō (Pawn)",
        PieceType.LANCE: "Kyōsha (Lance)",
        PieceType.KNIGHT: "Keima (Knight)",
        PieceType.SILVER: "Ginsho (Silver General)",
        PieceType.GOLD: "Kinshō (Gold General)",
        PieceType.BISHOP: "Kakugyō (Bishop)",
        PieceType.ROOK: "Hisha (Rook)",
        PieceType.KING: "Ōshō (King)",
        PieceType.PROMOTED_PAWN: "Tokin (Promoted Pawn)",
        PieceType.PROMOTED_LANCE: "Narikyo (Promoted Lance)",
        PieceType.PROMOTED_KNIGHT: "Narikei (Promoted Knight)",
        PieceType.PROMOTED_SILVER: "Narigin (Promoted Silver)",
        PieceType.PROMOTED_BISHOP: "Ryūma (Dragon Horse)",
        PieceType.PROMOTED_ROOK: "Ryūō (Dragon King)",
    }

    # If promoting during this move, show the transformation
    if is_promoting:
        base_names = {
            PieceType.PAWN: "Fuhyō (Pawn) → Tokin (Promoted Pawn)",
            PieceType.LANCE: "Kyōsha (Lance) → Narikyo (Promoted Lance)",
            PieceType.KNIGHT: "Keima (Knight) → Narikei (Promoted Knight)",
            PieceType.SILVER: "Ginsho (Silver General) → Narigin (Promoted Silver)",
            PieceType.BISHOP: "Kakugyō (Bishop) → Ryūma (Dragon Horse)",
            PieceType.ROOK: "Hisha (Rook) → Ryūō (Dragon King)",
        }
        return base_names.get(piece_type, piece_names.get(piece_type, str(piece_type)))

    return piece_names.get(piece_type, str(piece_type))


def _coords_to_square_name(row, col):
    """Convert 0-indexed coordinates to square name like '7f'."""
    file = str(
        9 - col
    )  # Convert column to file (9-col because shogi files go 9-1 from left to right)
    rank = chr(ord("a") + row)  # Convert row to rank (a-i)
    return f"{file}{rank}"

]]></file>
  <file path="utils/utils.py"><![CDATA[
"""
utils.py: Contains PolicyOutputMapper and TrainingLogger.
"""

from __future__ import annotations

import datetime
import json
import os
import sys
from abc import ABC, abstractmethod
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    List,
    Optional,
    Set,
    TextIO,
    cast,
)

import torch
import yaml
from pydantic import ValidationError
from rich.console import Console
from rich.text import Text

from keisei.config_schema import AppConfig
from keisei.shogi.shogi_core_definitions import (
    BoardMoveTuple,
    DropMoveTuple,
    PieceType,
    get_unpromoted_types,
)

# --- Config Loader Utility ---

# Mapping of flat override keys to nested config paths
FLAT_KEY_TO_NESTED = {
    # Env
    "SEED": "env.seed",
    "DEVICE": "env.device",
    "INPUT_CHANNELS": "env.input_channels",
    # Training
    "TOTAL_TIMESTEPS": "training.total_timesteps",
    "LEARNING_RATE": "training.learning_rate",
    "PPO_EPOCHS": "training.ppo_epochs",
    "MINIBATCH_SIZE": "training.minibatch_size",
    "GAMMA": "training.gamma",
    "CLIP_EPSILON": "training.clip_epsilon",
    "VALUE_LOSS_COEFF": "training.value_loss_coeff",
    "ENTROPY_COEFF": "training.entropy_coef",
    "STEPS_PER_EPOCH": "training.steps_per_epoch",
    "CHECKPOINT_INTERVAL_TIMESTEPS": "training.checkpoint_interval_timesteps",
    # Logging
    "MODEL_DIR": "logging.model_dir",
    "LOG_FILE": "logging.log_file",
    # Evaluation
    "NUM_GAMES": "evaluation.num_games",
    "OPPONENT_TYPE": "evaluation.opponent_type",
    # WandB
    "WANDB_ENABLED": "wandb.enabled",
    "WANDB_PROJECT": "wandb.project",
    "WANDB_ENTITY": "wandb.entity",
    # Demo
    "ENABLE_DEMO_MODE": "demo.enable_demo_mode",
    "DEMO_MODE_DELAY": "demo.demo_mode_delay",
}


def load_config(
    config_path: Optional[str] = None, cli_overrides: Optional[Dict[str, Any]] = None
) -> AppConfig:
    """
    Loads configuration from a YAML or JSON file and applies CLI overrides.
    Always loads default_config.yaml as the base, then merges in overrides from config_path (if present), then CLI overrides.
    """
    # Always load the base config first
    base_config_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
        "default_config.yaml",
    )
    with open(base_config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f)
    # If config_path is provided and is not the default, treat as override file (JSON or YAML)
    if config_path and os.path.abspath(config_path) != os.path.abspath(
        base_config_path
    ):
        if config_path.endswith((".yaml", ".yml")):
            with open(config_path, "r", encoding="utf-8") as f:
                override_data = yaml.safe_load(f)
        elif config_path.endswith(".json"):
            with open(config_path, "r", encoding="utf-8") as f:
                override_data = json.load(f)
        else:
            raise ValueError(f"Unsupported config file type: {config_path}")
        # If the override file is a partial dict (not a full config), treat as overrides
        top_keys = {"env", "training", "evaluation", "logging", "wandb", "demo"}
        if not (
            isinstance(override_data, dict) and top_keys & set(override_data.keys())
        ):
            # It's a flat override dict, not a full config
            mapped_overrides = {}
            for k, v in override_data.items():
                if k.isupper() and k in FLAT_KEY_TO_NESTED:
                    mapped_overrides[FLAT_KEY_TO_NESTED[k]] = v
                else:
                    mapped_overrides[k] = v
            for k, v in mapped_overrides.items():
                parts = k.split(".")
                d = config_data
                for p in parts[:-1]:
                    if p not in d or not isinstance(d[p], dict):
                        d[p] = {}
                    d = d[p]
                d[parts[-1]] = v
        else:
            # It's a full config, merge top-level keys
            for k, v in override_data.items():
                config_data[k] = v
    # Merge CLI overrides (flat dict with dot notation keys)
    if cli_overrides:
        mapped_overrides = {}
        for k, v in cli_overrides.items():
            if k.isupper() and k in FLAT_KEY_TO_NESTED:
                mapped_overrides[FLAT_KEY_TO_NESTED[k]] = v
            else:
                mapped_overrides[k] = v
        for k, v in mapped_overrides.items():
            parts = k.split(".")
            d = config_data
            for p in parts[:-1]:
                if p not in d or not isinstance(d[p], dict):
                    d[p] = {}
                d = d[p]
            d[parts[-1]] = v
    try:
        config = AppConfig.parse_obj(config_data)
    except ValidationError as e:
        print("Configuration validation error:")
        print(e)
        raise
    return config


if TYPE_CHECKING:
    from keisei.config_schema import AppConfig  # Keep this for type hinting if used elsewhere
    from keisei.shogi.shogi_core_definitions import MoveTuple
    from keisei.shogi.shogi_game import ShogiGame  # Added for type hinting


class BaseOpponent(ABC):
    """Abstract base class for game opponents."""

    def __init__(self, name: str = "BaseOpponent"):
        self.name = name

    @abstractmethod
    def select_move(self, game_instance: "ShogiGame") -> "MoveTuple":
        """
        Selects a move given the current game state.

        Args:
            game_instance: The current instance of the ShogiGame.

        Returns:
            A MoveTuple representing the selected move.
        """


class PolicyOutputMapper:
    """Maps Shogi moves to/from policy network output indices."""

    def __init__(self) -> None:
        """Initializes the PolicyOutputMapper by generating all possible move representations."""
        self.idx_to_move: List["MoveTuple"] = []
        self.move_to_idx: Dict["MoveTuple", int] = {}
        current_idx = 0
        self._unrecognized_moves_log_cache: Set[str] = (
            set()
        )  # Cache for logging distinct unrecognized moves
        self._unrecognized_moves_logged_count = (
            0  # Counter for logged distinct unrecognized moves
        )
        self._max_distinct_unrecognized_to_log = (
            5  # Max distinct unrecognized moves to log in detail
        )
        # Piece type mapping for USI drop characters
        self._USI_DROP_PIECE_CHARS: Dict[PieceType, str] = {
            PieceType.PAWN: "P",
            PieceType.LANCE: "L",
            PieceType.KNIGHT: "N",
            PieceType.SILVER: "S",
            PieceType.GOLD: "G",
            PieceType.BISHOP: "B",
            PieceType.ROOK: "R",
        }

        # Generate Board Moves: (from_r, from_c, to_r, to_c, promote_flag: bool)
        # Ensure from_r, from_c, to_r, to_c are treated as integers.
        for r_from in range(9):
            for c_from in range(9):
                for r_to in range(9):
                    for c_to in range(9):
                        if r_from == r_to and c_from == c_to:
                            continue  # Skip null moves

                        # Move without promotion
                        move_no_promo: BoardMoveTuple = (
                            r_from,
                            c_from,
                            r_to,
                            c_to,
                            False,
                        )
                        self.idx_to_move.append(move_no_promo)
                        self.move_to_idx[move_no_promo] = current_idx
                        current_idx += 1

                        # Move with promotion (if applicable, though ShogiGame handles legality)
                        # For policy mapping, we generate all potential promotion moves.
                        # Actual legality (e.g., can piece promote from/to square) is game logic.
                        move_promo: BoardMoveTuple = (
                            r_from,
                            c_from,
                            r_to,
                            c_to,
                            True,
                        )
                        self.idx_to_move.append(move_promo)
                        self.move_to_idx[move_promo] = current_idx
                        current_idx += 1

        # Generate Drop Moves: (None, None, to_r, to_c, piece_type_to_drop: PieceType)
        # Ensure to_r, to_c are treated as integers.
        # piece_type_to_drop should be PieceType enum, not int, if DropMoveTuple expects PieceType.
        # If DropMoveTuple expects int, then piece.value is correct.
        # Based on current DropMoveTuple = Tuple[Optional[int], Optional[int], int, int, PieceType],
        # we should use the PieceType enum directly.

        # Get unpromoted piece types that can be held in hand (excluding King)
        hand_piece_types = get_unpromoted_types()

        for r_to in range(9):
            for c_to in range(9):
                for piece_type_enum in hand_piece_types:
                    # DropMoveTuple expects PieceType enum for the piece type
                    drop_move: DropMoveTuple = (
                        None,  # from_r is None for drops
                        None,  # from_c is None for drops
                        r_to,
                        c_to,
                        piece_type_enum,  # Use the PieceType enum directly
                    )
                    self.idx_to_move.append(drop_move)
                    self.move_to_idx[drop_move] = current_idx
                    current_idx += 1

    def get_total_actions(self) -> int:
        """Returns the total number of unique actions (moves) in the policy output."""
        return len(self.idx_to_move)

    def shogi_move_to_policy_index(self, move: "MoveTuple") -> int:
        """Convert a Shogi MoveTuple to its policy index."""
        idx = self.move_to_idx.get(move)
        if idx is None:
            # Attempt to handle cases where the exact tuple might not match due to PieceType enum identity
            # This is a fallback, ideally the move objects should match directly.
            if len(move) == 5 and move[0] is None:  # Potential DropMoveTuple
                # Reconstruct DropMoveTuple with PieceType from self.idx_to_move if a similar one exists
                # This is a bit of a heuristic and might need refinement
                for stored_move in self.move_to_idx:
                    if (
                        len(stored_move) == 5
                        and stored_move[0] is None
                        and stored_move[2] == move[2]
                        and stored_move[3] == move[3]
                        and stored_move[4].value == move[4].value
                    ):  # Compare PieceType by value
                        idx = self.move_to_idx.get(stored_move)
                        break
            if idx is None:  # If still not found after heuristic
                raise ValueError(
                    f"Move {move} (type: {type(move)}, element types: {[type(el) for el in move]}) "
                    f"not found in PolicyOutputMapper's known moves. Known keys example: {list(self.move_to_idx.keys())[0]}"
                )
        return idx

    def policy_index_to_shogi_move(self, idx: int) -> "MoveTuple":
        """Convert a policy index back to its Shogi MoveTuple."""
        if (
            0 <= idx < self.get_total_actions()
        ):  # MODIFIED: Changed self.total_actions to self.get_total_actions()
            return self.idx_to_move[idx]
        raise IndexError(
            f"Policy index {idx} is out of bounds (0-{self.get_total_actions() - 1})."  # MODIFIED: Changed self.total_actions to self.get_total_actions()
        )

    def get_legal_mask(
        self, legal_shogi_moves: List["MoveTuple"], device: torch.device
    ) -> torch.Tensor:
        """
        Creates a boolean mask indicating which actions in the policy output are legal.

        Args:
            legal_shogi_moves: A list of legal Shogi moves (MoveTuple).
            device: The torch device on which to create the mask tensor.

        Returns:
            A 1D boolean tensor where True indicates a legal move.
        """
        mask = torch.zeros(self.get_total_actions(), dtype=torch.bool, device=device)
        for move in legal_shogi_moves:
            try:
                idx = self.shogi_move_to_policy_index(move)
                mask[idx] = True
            except ValueError:
                # This can happen if a move from the game isn't in the mapper.
                # This should ideally not occur if the mapper covers all possible moves
                # and the game generates valid moves.
                # Consider logging this if it becomes an issue.
                pass  # Ignore moves not recognized by the mapper for now.
        return mask

    def _usi_sq(self, r: int, c: int) -> str:
        """Converts 0-indexed (row, col) to USI square string (e.g., (0,0) -> '9a')."""
        if not (0 <= r <= 8 and 0 <= c <= 8):
            raise ValueError(f"Invalid square coordinates: ({r}, {c})")
        file = str(9 - c)
        rank = chr(ord("a") + r)
        return f"{file}{rank}"

    def _get_usi_char_for_drop(self, piece_type: PieceType) -> str:
        """Gets the USI character for a droppable piece."""
        if piece_type not in self._USI_DROP_PIECE_CHARS:
            raise ValueError(
                f"Piece type {piece_type} cannot be dropped or is not a recognized droppable piece."
            )
        return self._USI_DROP_PIECE_CHARS[piece_type]

    def action_idx_to_shogi_move(self, action_idx: int) -> "MoveTuple":
        """Converts an action index from the policy output back to a Shogi MoveTuple."""
        if not (0 <= action_idx < len(self.idx_to_move)):
            # Log distinct unrecognized moves only a few times to avoid flooding logs
            log_message = f"Action index {action_idx} is out of bounds for idx_to_move (size {len(self.idx_to_move)})."
            raise IndexError(log_message)
        return self.idx_to_move[action_idx]

    def shogi_move_to_usi(self, move_tuple: "MoveTuple") -> str:
        """Converts a Shogi MoveTuple to its USI string representation."""
        if len(move_tuple) == 5 and isinstance(move_tuple[4], bool):  # BoardMoveTuple
            from_r, from_c, to_r, to_c, promote = cast(BoardMoveTuple, move_tuple)
            if not all(
                isinstance(coord, int) for coord in [from_r, from_c, to_r, to_c]
            ):
                raise ValueError(
                    "Invalid coordinates in BoardMoveTuple for USI conversion."
                )
            usi_from_sq = self._usi_sq(from_r, from_c)
            usi_to_sq = self._usi_sq(to_r, to_c)
            promo_char = "+" if promote else ""
            return f"{usi_from_sq}{usi_to_sq}{promo_char}"
        elif len(move_tuple) == 5 and isinstance(
            move_tuple[4], PieceType
        ):  # DropMoveTuple
            _none1, _none2, to_r, to_c, piece_type_enum = cast(
                DropMoveTuple, move_tuple
            )
            if not all(isinstance(coord, int) for coord in [to_r, to_c]):
                raise ValueError(
                    "Invalid coordinates in DropMoveTuple for USI conversion."
                )
            if not isinstance(piece_type_enum, PieceType):
                raise ValueError(
                    "Invalid piece type in DropMoveTuple for USI conversion."
                )
            try:
                piece_usi_char = self._get_usi_char_for_drop(piece_type_enum)
            except ValueError as e:
                raise ValueError(
                    f"Invalid piece type for drop in USI conversion: {piece_type_enum.name}"
                ) from e
            usi_to_sq = self._usi_sq(to_r, to_c)
            return f"{piece_usi_char}*{usi_to_sq}"
        else:
            raise ValueError(
                f"Unrecognized move_tuple format for USI conversion: "
                f"length {len(move_tuple)}, last element type {type(move_tuple[-1]) if move_tuple else 'N/A'}"
            )

    def usi_to_shogi_move(self, usi_move_str: str) -> "MoveTuple":
        """Converts a USI string to its Shogi MoveTuple representation."""
        if not isinstance(usi_move_str, str) or len(usi_move_str) < 4:
            raise ValueError(f"Invalid USI move string format: {usi_move_str}")

        # Helper to parse USI square (e.g., '9a' -> (0,0))
        def _parse_usi_sq(sq_str: str) -> tuple[int, int]:
            if not (len(sq_str) == 2 and sq_str[0].isdigit() and sq_str[1].isalpha()):
                raise ValueError(f"Invalid USI square format: {sq_str}")
            file = int(sq_str[0])
            rank_char = sq_str[1]
            c = 9 - file
            r = ord(rank_char) - ord("a")
            if not (0 <= r <= 8 and 0 <= c <= 8):
                raise ValueError(
                    f"Square coordinates out of bounds: {sq_str} -> ({r}, {c})"
                )
            return r, c

        # Drop move (e.g., P*5e)
        if usi_move_str[1] == "*":
            if len(usi_move_str) != 4:
                raise ValueError(f"Invalid USI drop move string length: {usi_move_str}")
            piece_char = usi_move_str[0]
            to_sq_str = usi_move_str[2:]

            dropped_piece_type: Optional[PieceType] = None
            for pt, char in self._USI_DROP_PIECE_CHARS.items():
                if char == piece_char:
                    dropped_piece_type = pt
                    break
            if dropped_piece_type is None:
                raise ValueError(f"Invalid piece character for drop: {piece_char}")

            to_r, to_c = _parse_usi_sq(to_sq_str)
            # MODIFIED: Correctly instantiate DropMoveTuple
            return (None, None, to_r, to_c, dropped_piece_type)

        # Board move (e.g., 7g7f, 2b3a+, 8h2b+)
        else:
            if not (4 <= len(usi_move_str) <= 5):
                raise ValueError(
                    f"Invalid USI board move string length: {usi_move_str}"
                )

            from_sq_str = usi_move_str[0:2]
            to_sq_str = usi_move_str[2:4]
            promote = False
            if len(usi_move_str) == 5:
                if usi_move_str[4] == "+":
                    promote = True
                else:
                    raise ValueError(
                        f"Invalid promotion character in USI move: {usi_move_str}"
                    )

            from_r, from_c = _parse_usi_sq(from_sq_str)
            to_r, to_c = _parse_usi_sq(to_sq_str)
            # MODIFIED: Correctly instantiate BoardMoveTuple
            return (from_r, from_c, to_r, to_c, promote)

    def action_idx_to_usi_move(self, action_idx: int, _board=None) -> str:
        """Converts an action index to its USI move string representation."""
        shogi_move = self.action_idx_to_shogi_move(action_idx)
        return self.shogi_move_to_usi(shogi_move)


class TrainingLogger:
    """Handles logging of training progress to a file and optionally to stdout."""

    def __init__(
        self,
        log_file_path: str,
        rich_console: Optional[Console] = None,
        rich_log_panel: Optional[List[Text]] = None,
        also_stdout: Optional[bool] = None,  # Added also_stdout argument
        **_kwargs: Any,  # Added to accept arbitrary keyword arguments
    ):
        """
        Initializes the TrainingLogger.

        Args:
            log_file_path: Path to the log file.
            rich_console: An optional rich.console.Console instance for TUI output.
            rich_log_panel: An optional list to which rich.text.Text log messages can be appended for TUI display.
            also_stdout: Whether to also print log messages to stdout (used if rich_console is None).
            **kwargs: To catch unexpected keyword arguments.
        """
        self.log_file_path = log_file_path
        self.log_file: Optional[TextIO] = None
        self.rich_console = rich_console
        self.rich_log_panel = rich_log_panel  # This will be a list of Text objects
        # If rich_console is not provided, this flag determines if logs go to stdout.
        # If rich_console IS provided, stdout is typically handled by the rich Live display.
        self.also_stdout_if_no_rich = also_stdout if also_stdout is not None else True

    def __enter__(self) -> "TrainingLogger":
        self.log_file = open(self.log_file_path, "a", encoding="utf-8")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        if self.log_file:
            self.log_file.close()
            self.log_file = None

    def log(self, message: str) -> None:
        """Logs a message to the file and to the rich log panel if configured."""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        full_message = f"[{timestamp}] {message}"

        if self.log_file:
            self.log_file.write(full_message + "\\n")
            self.log_file.flush()

        if self.rich_console and self.rich_log_panel is not None:
            # Create a Rich Text object for the message
            rich_message = Text(full_message)
            self.rich_log_panel.append(rich_message)
            # The Live display will handle the update. We don't print directly here.
        elif (
            self.also_stdout_if_no_rich
        ):  # Changed condition to use also_stdout_if_no_rich
            # Fallback to stdout if rich components are not provided and also_stdout_if_no_rich is True
            try:
                print(full_message, file=sys.stderr)
            except ImportError:
                print(full_message, file=sys.stderr)


class EvaluationLogger:
    """Handles logging of evaluation results to a file and optionally to stdout."""

    def __init__(self, log_file_path: str, also_stdout: bool = True, **_kwargs: Any):
        """
        Initializes the EvaluationLogger.

        Args:
            log_file_path: Path to the log file.
            also_stdout: Whether to also print log messages to stdout.
            **kwargs: To catch unexpected keyword arguments.
        """
        self.log_file_path = log_file_path
        self.also_stdout = also_stdout
        self.log_file: Optional[TextIO] = None

    def __enter__(self) -> "EvaluationLogger":
        self.log_file = open(self.log_file_path, "a", encoding="utf-8")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        if self.log_file:
            self.log_file.close()
            self.log_file = None

    def log(self, message: str) -> None:  # MODIFIED: Simplified to a single log method
        """Logs a message to the file and optionally to stdout."""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        full_message = f"[{timestamp}] {message}"

        if self.log_file:
            self.log_file.write(full_message + "\\n")
            self.log_file.flush()

        if self.also_stdout:
            print(full_message, file=sys.stderr)  # Print to stderr for visibility


def generate_run_name(config: "AppConfig", run_name: Optional[str] = None) -> str:
    """Generates a unique run name based on config and timestamp, or returns the provided run_name if set."""
    if run_name:
        return run_name
    # Example: keisei_resnet_feats_core46_20231027_153000
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_type_short = config.training.model_type[:10]  # Truncate for brevity
    feature_set_short = config.training.input_features.replace("_", "")[:15]
    run_name_parts = [
        config.wandb.run_name_prefix if config.wandb.run_name_prefix else "keisei",
        model_type_short,
        f"feats_{feature_set_short}",
        timestamp,
    ]
    return "_".join(filter(None, run_name_parts))

]]></file>
  <file path="utils/checkpoint.py"><![CDATA[
"""
checkpoint.py: Model checkpoint migration and compatibility utilities for Keisei Shogi.
"""
import torch
import torch.nn as nn
from typing import Dict, Any

def load_checkpoint_with_padding(model: nn.Module, checkpoint: Dict[str, Any], input_channels: int) -> None:
    """
    Loads a checkpoint into the model, zero-padding the first conv layer if input channels have increased.
    Args:
        model: The model instance (should have a .stem Conv2d layer).
        checkpoint: The loaded state_dict (from torch.load(...)).
        input_channels: The number of input channels expected by the current model.
    """
    state_dict = checkpoint["model_state_dict"] if "model_state_dict" in checkpoint else checkpoint
    model_state = model.state_dict()
    # Handle first conv layer (stem)
    stem_key = None
    for k in model_state:
        if k.endswith("stem.weight"):
            stem_key = k
            break
    if stem_key is not None:
        old_weight = state_dict[stem_key]
        new_weight = model_state[stem_key]
        if old_weight.shape[1] < new_weight.shape[1]:
            # Zero-pad input channels
            pad = torch.zeros(
                (old_weight.shape[0], new_weight.shape[1] - old_weight.shape[1], *old_weight.shape[2:]),
                dtype=old_weight.dtype, device=old_weight.device
            )
            padded_weight = torch.cat([old_weight, pad], dim=1)
            state_dict[stem_key] = padded_weight
        elif old_weight.shape[1] > new_weight.shape[1]:
            # Truncate input channels
            state_dict[stem_key] = old_weight[:, :new_weight.shape[1], :, :]
    # Load all other layers strictly
    model.load_state_dict(state_dict, strict=False)

]]></file>
  <file path="utils/__init__.py"><![CDATA[
# keisei/utils/__init__.py

from .move_formatting import (
    _coords_to_square_name,
    _get_piece_name,
    format_move_with_description,
    format_move_with_description_enhanced,
)
from .utils import (
    BaseOpponent,
    EvaluationLogger,
    PolicyOutputMapper,
    TrainingLogger,
    load_config,
)

]]></file>
</codebase>
