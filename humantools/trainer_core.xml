<?xml version="1.0" encoding="UTF-8"?>
<codebase>
  <file path="scheduler_factory.py"><![CDATA[
"""
Learning rate scheduler factory for PPO training.
"""

from typing import Any, Dict, Optional

import torch
from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR, LambdaLR, StepLR


class SchedulerFactory:
    """Factory for creating PyTorch learning rate schedulers."""

    @staticmethod
    def create_scheduler(
        optimizer: torch.optim.Optimizer,
        schedule_type: Optional[str],
        total_steps: int,
        schedule_kwargs: Optional[Dict[str, Any]] = None,
    ) -> Optional[Any]:
        """
        Create a learning rate scheduler based on configuration.

        Args:
            optimizer: PyTorch optimizer
            schedule_type: Type of scheduler ('linear', 'cosine', 'exponential', 'step')
            total_steps: Total number of training steps for scheduling
            schedule_kwargs: Additional arguments for the scheduler

        Returns:
            Configured scheduler or None if schedule_type is None
        """
        if schedule_type is None:
            return None
        schedule_kwargs = schedule_kwargs or {}

        if schedule_type == "linear":
            return SchedulerFactory._create_linear_scheduler(
                optimizer, total_steps, schedule_kwargs
            )
        elif schedule_type == "cosine":
            return SchedulerFactory._create_cosine_scheduler(
                optimizer, total_steps, schedule_kwargs
            )
        elif schedule_type == "exponential":
            return SchedulerFactory._create_exponential_scheduler(
                optimizer, schedule_kwargs
            )
        elif schedule_type == "step":
            return SchedulerFactory._create_step_scheduler(optimizer, schedule_kwargs)
        else:
            raise ValueError(f"Unsupported scheduler type: {schedule_type}")

    @staticmethod
    def _create_linear_scheduler(
        optimizer: torch.optim.Optimizer, total_steps: int, kwargs: Dict[str, Any]
    ) -> LambdaLR:
        """Create linear decay scheduler."""
        final_lr_fraction = kwargs.get("final_lr_fraction", 0.1)

        def linear_decay(step: int) -> float:
            return max(final_lr_fraction, 1.0 - step / total_steps)

        return LambdaLR(optimizer, lr_lambda=linear_decay)

    @staticmethod
    def _create_cosine_scheduler(
        optimizer: torch.optim.Optimizer, total_steps: int, kwargs: Dict[str, Any]
    ) -> CosineAnnealingLR:
        """Create cosine annealing scheduler."""
        eta_min_fraction = kwargs.get("eta_min_fraction", 0.0)
        initial_lr = optimizer.param_groups[0]["lr"]
        eta_min = initial_lr * eta_min_fraction

        return CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=eta_min)

    @staticmethod
    def _create_exponential_scheduler(
        optimizer: torch.optim.Optimizer, kwargs: Dict[str, Any]
    ) -> ExponentialLR:
        """Create exponential decay scheduler."""
        gamma = kwargs.get("gamma", 0.995)
        return ExponentialLR(optimizer, gamma=gamma)

    @staticmethod
    def _create_step_scheduler(
        optimizer: torch.optim.Optimizer, kwargs: Dict[str, Any]
    ) -> StepLR:
        """Create step decay scheduler."""
        step_size = kwargs.get("step_size", 1000)
        gamma = kwargs.get("gamma", 0.5)
        return StepLR(optimizer, step_size=step_size, gamma=gamma)

]]></file>
  <file path="experience_buffer.py"><![CDATA[
"""
Minimal ExperienceBuffer for DRL Shogi Client.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional

import torch  # Ensure torch is imported
from keisei.utils.unified_logger import log_warning_to_stderr


@dataclass
class Experience:
    """Single experience transition for parallel collection."""

    obs: torch.Tensor
    action: int
    reward: float
    log_prob: float
    value: float
    done: bool
    legal_mask: torch.Tensor


class ExperienceBuffer:
    """Experience buffer for storing transitions during RL training."""

    def __init__(
        self, buffer_size: int, gamma: float, lambda_gae: float, device: str = "cpu"
    ):
        self.buffer_size = buffer_size
        self.gamma = gamma
        self.lambda_gae = lambda_gae
        self.device = torch.device(device)  # Store as torch.device

        # Pre-allocate tensors for improved memory efficiency
        # Observations: (buffer_size, 46, 9, 9) - Shogi board representation
        self.obs = torch.zeros(
            (buffer_size, 46, 9, 9), dtype=torch.float32, device=self.device
        )
        # Actions: (buffer_size,) - Action indices
        self.actions = torch.zeros(buffer_size, dtype=torch.int64, device=self.device)
        # Rewards: (buffer_size,) - Immediate rewards
        self.rewards = torch.zeros(buffer_size, dtype=torch.float32, device=self.device)
        # Log probabilities: (buffer_size,) - Action log probabilities  
        self.log_probs = torch.zeros(buffer_size, dtype=torch.float32, device=self.device)
        # Values: (buffer_size,) - Value function estimates
        self.values = torch.zeros(buffer_size, dtype=torch.float32, device=self.device)
        # Done flags: (buffer_size,) - Episode termination flags
        self.dones = torch.zeros(buffer_size, dtype=torch.bool, device=self.device)
        # Legal masks: (buffer_size, 13527) - Legal action masks
        self.legal_masks = torch.zeros(
            (buffer_size, 13527), dtype=torch.bool, device=self.device
        )
        # Advantages and returns: populated by compute_advantages_and_returns
        self.advantages = torch.zeros(buffer_size, dtype=torch.float32, device=self.device)
        self.returns = torch.zeros(buffer_size, dtype=torch.float32, device=self.device)
        self.ptr = 0
        # Flag to track if advantages and returns have been computed
        self._advantages_computed = False

    def add(
        self,
        obs: torch.Tensor,
        action: int,
        reward: float,
        log_prob: float,
        value: float,
        done: bool,
        legal_mask: torch.Tensor,  # Added legal_mask
    ):
        """
        Add a transition to the buffer.
        'obs' is expected to be a PyTorch tensor of shape (C, H, W) on self.device.
        'legal_mask' is expected to be a PyTorch tensor on self.device.
        """
        if self.ptr < self.buffer_size:
            # Store data using tensor indexing for better performance
            self.obs[self.ptr] = obs.to(self.device)
            self.actions[self.ptr] = action
            self.rewards[self.ptr] = reward
            self.log_probs[self.ptr] = log_prob
            self.values[self.ptr] = value
            self.dones[self.ptr] = done
            self.legal_masks[self.ptr] = legal_mask.to(self.device)
            self.ptr += 1
        else:
            # This case should ideally be handled by the training loop,
            # which calls learn() and then clear() when buffer is full.
            log_warning_to_stderr("ExperienceBuffer", "Buffer is full. Cannot add new experience.")

    def compute_advantages_and_returns(
        self, last_value: float
    ):  # last_value is a float
        """
        Computes Generalized Advantage Estimation (GAE) and returns for the collected experiences.
        This should be called after the buffer is full (i.e., self.ptr == self.buffer_size).
        Uses PyTorch tensor operations for GAE calculation.
        """
        if self.ptr == 0:
            log_warning_to_stderr("ExperienceBuffer", "compute_advantages_and_returns called on an empty buffer.")
            return

        # Use tensor slices for GAE computation (much more efficient)
        rewards_tensor = self.rewards[: self.ptr]
        values_tensor = self.values[: self.ptr]
        dones_tensor = self.dones[: self.ptr].float()  # Convert bool to float for mask
        masks_tensor = 1.0 - dones_tensor

        # Initialize GAE variables
        gae = torch.tensor(0.0, device=self.device)
        next_value_tensor = torch.tensor(last_value, dtype=torch.float32, device=self.device)

        # Compute advantages and returns using reverse iteration
        for t in reversed(range(self.ptr)):
            if t == self.ptr - 1:
                current_next_value = next_value_tensor
            else:
                current_next_value = values_tensor[t + 1]

            delta = (
                rewards_tensor[t]
                + self.gamma * current_next_value * masks_tensor[t]
                - values_tensor[t]
            )
            gae = delta + self.gamma * self.lambda_gae * masks_tensor[t] * gae

            # Store computed values directly in pre-allocated tensors
            self.advantages[t] = gae
            self.returns[t] = gae + values_tensor[t]
        
        # Mark that advantages have been computed
        self._advantages_computed = True

    def get_batch(self) -> dict:
        """
        Returns all collected experiences as a dictionary of PyTorch tensors on self.device.
        Assumes compute_advantages_and_returns has been called.
        """
        if self.ptr == 0:
            # This should be handled by PPOAgent.learn() checking for empty batch_data
            # For safety, one might return structured empty tensors if needed upstream.
            log_warning_to_stderr("ExperienceBuffer", "get_batch called on an empty or not-yet-computed buffer.")
            return {}  # PPOAgent.learn already checks for this
        
        if not self._advantages_computed:
            raise RuntimeError("Cannot get batch: compute_advantages_and_returns() must be called first")

        num_samples = self.ptr

        # --- Efficient Batching with Pre-allocated Tensors ---
        # All data is already stored as tensors, just slice to get the active portion
        obs_tensor = self.obs[:num_samples]
        actions_tensor = self.actions[:num_samples]
        log_probs_tensor = self.log_probs[:num_samples]
        values_tensor = self.values[:num_samples]
        advantages_tensor = self.advantages[:num_samples]
        returns_tensor = self.returns[:num_samples]
        dones_tensor = self.dones[:num_samples]
        legal_masks_tensor = self.legal_masks[:num_samples]

        return {
            "obs": obs_tensor,
            "actions": actions_tensor,
            "log_probs": log_probs_tensor,  # These are old_log_probs for PPO
            "values": values_tensor,  # These are old_V(s_t) for PPO, used with GAE
            "rewards": self.rewards[:num_samples],  # Include rewards in batch data
            "advantages": advantages_tensor,
            "returns": returns_tensor,  # These are the GAE-based returns (targets for value func)
            "dones": dones_tensor,  # For record keeping or if needed by learn()
            "legal_masks": legal_masks_tensor,  # Added legal_masks_tensor
        }

    def clear(self):
        """Clears all stored experiences from the buffer."""
        # With pre-allocated tensors, we just reset the pointer
        # The tensor memory remains allocated for reuse
        self.ptr = 0
        # Reset the computation flag when clearing buffer
        self._advantages_computed = False

    def __len__(self):
        """Return the number of transitions currently in the buffer."""
        return self.ptr

    def add_batch(self, experiences: List[Experience]) -> None:
        """
        Add a batch of experiences to the buffer for parallel collection.

        Args:
            experiences: List of Experience objects to add
        """
        for exp in experiences:
            if self.ptr >= self.buffer_size:
                break  # Don't exceed buffer size

            self.add(
                obs=exp.obs,
                action=exp.action,
                reward=exp.reward,
                log_prob=exp.log_prob,
                value=exp.value,
                done=exp.done,
                legal_mask=exp.legal_mask,
            )

    def add_from_worker_batch(self, worker_data: Dict[str, torch.Tensor]) -> None:
        """
        Add experiences from worker batch data (optimized for parallel collection).

        Args:
            worker_data: Dictionary containing batched tensors from workers
        """
        batch_size = worker_data["obs"].shape[0]

        for i in range(batch_size):
            if self.ptr >= self.buffer_size:
                break

            self.add(
                obs=worker_data["obs"][i],
                action=int(worker_data["actions"][i].item()),
                reward=worker_data["rewards"][i].item(),
                log_prob=worker_data["log_probs"][i].item(),
                value=worker_data["values"][i].item(),
                done=bool(worker_data["dones"][i].item()),
                legal_mask=worker_data["legal_masks"][i],
            )

    def get_worker_batch_format(self) -> Optional[Dict[str, torch.Tensor]]:
        """
        Get current buffer contents in worker batch format for validation.

        Returns:
            Dictionary with batched tensors or None if buffer is empty
        """
        if self.ptr == 0:
            return None

        return {
            "obs": self.obs[: self.ptr],
            "actions": self.actions[: self.ptr],
            "rewards": self.rewards[: self.ptr],
            "log_probs": self.log_probs[: self.ptr],
            "values": self.values[: self.ptr],
            "dones": self.dones[: self.ptr],
            "legal_masks": self.legal_masks[: self.ptr],
        }

    def merge_from_parallel_buffers(
        self, parallel_buffers: List["ExperienceBuffer"]
    ) -> None:
        """
        Merge experiences from multiple parallel buffers.

        Args:
            parallel_buffers: List of ExperienceBuffer instances from workers
        """
        for buffer in parallel_buffers:
            if buffer.ptr == 0:
                continue

            # Add all experiences from this buffer
            for i in range(buffer.ptr):
                if self.ptr >= self.buffer_size:
                    return  # Main buffer is full

                self.add(
                    obs=buffer.obs[i],
                    action=int(buffer.actions[i].item()),
                    reward=float(buffer.rewards[i].item()),
                    log_prob=float(buffer.log_probs[i].item()),
                    value=float(buffer.values[i].item()),
                    done=bool(buffer.dones[i].item()),
                    legal_mask=buffer.legal_masks[i],
                )

]]></file>
  <file path="ppo_agent.py"><![CDATA[
"""
Minimal PPOAgent for DRL Shogi Client.
"""

import os
import sys  # For stderr
from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple

import numpy as np
import torch
import torch.nn.functional as F

from keisei.config_schema import AppConfig
from keisei.core.actor_critic_protocol import ActorCriticProtocol
from keisei.core.experience_buffer import ExperienceBuffer
from keisei.core.scheduler_factory import SchedulerFactory
from keisei.utils import PolicyOutputMapper
from keisei.utils.unified_logger import log_info_to_stderr, log_error_to_stderr

if TYPE_CHECKING:
    from keisei.shogi.shogi_core_definitions import MoveTuple


class PPOAgent:
    """Proximal Policy Optimization agent for Shogi (PPO logic)."""

    def __init__(
        self,
        model: ActorCriticProtocol,
        config: AppConfig,
        device: torch.device,
        name: str = "PPOAgent",
        scaler=None,
        use_mixed_precision: bool = False,
    ):
        """
        Initialize the PPOAgent with model, optimizer, and PPO hyperparameters.

        Args:
            model: ActorCritic model instance conforming to ActorCriticProtocol
            config: Application configuration
            device: PyTorch device for training
            name: Agent name for identification
        """
        self.config = config
        self.device = device
        self.name = name

        # Mixed precision support
        self.scaler = scaler
        self.use_mixed_precision = use_mixed_precision

        # Direct model assignment (dependency injection)
        self.model: ActorCriticProtocol = model.to(self.device)

        # Initialize policy mapper
        policy_output_mapper = PolicyOutputMapper()
        self.policy_output_mapper = policy_output_mapper
        self.num_actions_total = self.policy_output_mapper.get_total_actions()
        # Add weight_decay from config if present, else default to 0.0
        weight_decay = getattr(config.training, "weight_decay", 0.0)
        # Initialize optimizer, handle invalid learning rate gracefully
        try:
            self.optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=config.training.learning_rate,
                weight_decay=weight_decay,
            )
        except Exception as e:
            # Fallback to default learning rate on error
            log_error_to_stderr("PPOAgent", f"Could not initialize optimizer with lr={config.training.learning_rate}, using default lr=1e-3: {e}")
            self.optimizer = torch.optim.Adam(
                self.model.parameters(), lr=1e-3, weight_decay=weight_decay
            )

        # PPO hyperparameters
        self.gamma = config.training.gamma
        self.clip_epsilon = config.training.clip_epsilon
        self.value_loss_coeff = config.training.value_loss_coeff
        self.entropy_coef = config.training.entropy_coef
        self.ppo_epochs = config.training.ppo_epochs
        self.minibatch_size = config.training.minibatch_size
        # Normalization options
        self.normalize_advantages = getattr(
            config.training, "normalize_advantages", True
        )

        self.last_kl_div = 0.0  # Initialize KL divergence tracker
        self.gradient_clip_max_norm = (
            config.training.gradient_clip_max_norm
        )  # Added from config

        # Use a numpy Generator for shuffling
        self._rng = np.random.default_rng(getattr(config.env, "seed", None))

        # Learning rate scheduler configuration
        self.lr_schedule_type = config.training.lr_schedule_type
        self.lr_schedule_step_on = config.training.lr_schedule_step_on
        total_steps = self._calculate_total_scheduler_steps(config)
        self.scheduler = SchedulerFactory.create_scheduler(
            optimizer=self.optimizer,
            schedule_type=self.lr_schedule_type,
            total_steps=total_steps,
            schedule_kwargs=config.training.lr_schedule_kwargs,
        )

    def _calculate_total_scheduler_steps(self, config: AppConfig) -> int:
        """Calculate total number of scheduler steps based on configuration."""
        if config.training.lr_schedule_step_on == "epoch":
            # epochs = total_timesteps // steps_per_epoch
            return (
                config.training.total_timesteps // config.training.steps_per_epoch
            ) * config.training.ppo_epochs
        else:
            # updates per epoch = steps_per_epoch // minibatch_size * ppo_epochs
            updates_per_epoch = (
                config.training.steps_per_epoch // config.training.minibatch_size
            ) * config.training.ppo_epochs
            # number of epochs = total_timesteps // steps_per_epoch
            num_epochs = (
                config.training.total_timesteps // config.training.steps_per_epoch
            )
            return num_epochs * updates_per_epoch

    def select_action(
        self,
        obs: np.ndarray,
        legal_mask: torch.Tensor,
        *,  # Force is_training to be keyword-only
        is_training: bool = True,
    ) -> Tuple[
        Optional["MoveTuple"],
        int,
        float,
        float,
    ]:
        """
        Select an action given an observation, legal Shogi moves, and a precomputed legal_mask.
        Returns the selected Shogi move, its policy index, log probability, and value estimate.
        """
        self.model.train(is_training)

        obs_tensor = torch.tensor(
            obs, dtype=torch.float32, device=self.device
        ).unsqueeze(0)

        if not legal_mask.any():
            log_error_to_stderr("PPOAgent", "select_action called with no legal moves (based on input legal_mask)")
            # Fallback behavior might be needed if model.get_action_and_value can't handle all-False mask.
            # neural_network.py's get_action_and_value attempts to handle this.
            # If this path is hit, it implies the caller might not have checked for no legal moves.
            # The train.py logic should ideally prevent calling select_action if no legal_moves.
            # Let it proceed, model.get_action_and_value will use the all-false mask.

        # Get action, log_prob, and value from the ActorCritic model
        # Pass deterministic based on not is_training
        (
            selected_policy_index_tensor,
            log_prob_tensor,
            value_tensor,
        ) = self.model.get_action_and_value(
            obs_tensor, legal_mask=legal_mask, deterministic=not is_training
        )

        selected_policy_index_val = int(selected_policy_index_tensor.item())
        log_prob_val = float(log_prob_tensor.item())
        value_float = float(
            value_tensor.item()
        )  # Value is already squeezed in get_action_and_value

        selected_shogi_move: Optional["MoveTuple"] = None
        try:
            selected_shogi_move = self.policy_output_mapper.policy_index_to_shogi_move(
                selected_policy_index_val
            )
        except IndexError as e:
            log_error_to_stderr("PPOAgent", f"Policy index {selected_policy_index_val} out of bounds in select_action: {e}")
            # Handle by returning no move or re-raising, depending on desired robustness.
            return None, -1, 0.0, value_float
            # Or raise the error

        return (
            selected_shogi_move,
            selected_policy_index_val,
            log_prob_val,
            value_float,
        )

    def get_value(self, obs_np: np.ndarray) -> float:
        """Get the value prediction from the critic for a given NumPy observation."""
        self.model.eval()
        obs_tensor = torch.tensor(
            obs_np, dtype=torch.float32, device=self.device
        ).unsqueeze(0)
        with torch.no_grad():
            _, _, value_estimate = self.model.get_action_and_value(
                obs_tensor, deterministic=True
            )
        return float(value_estimate.item())

    def learn(self, experience_buffer: ExperienceBuffer) -> Dict[str, float]:
        """
        Perform PPO update using experiences from the buffer.
        Returns a dictionary of logging metrics.
        """
        self.model.train()

        batch_data = experience_buffer.get_batch()
        # It's good practice for ExperienceBuffer.get_batch() to already place tensors on self.device
        # or for the buffer itself to live on self.device if memory allows.
        # If not, the .to(self.device) calls below are necessary.

        current_lr = self.optimizer.param_groups[0]["lr"]

        if not batch_data or batch_data["obs"].shape[0] == 0:
            log_error_to_stderr("PPOAgent", "learn called with empty batch_data")
            return {
                "ppo/policy_loss": 0.0,
                "ppo/value_loss": 0.0,
                "ppo/entropy": 0.0,
                "ppo/kl_divergence_approx": self.last_kl_div,
                "ppo/learning_rate": current_lr,
            }

        obs_batch = batch_data["obs"].to(self.device)
        actions_batch = batch_data["actions"].to(self.device)
        old_log_probs_batch = batch_data["log_probs"].to(self.device)
        advantages_batch = batch_data["advantages"].to(self.device)
        returns_batch = batch_data["returns"].to(self.device)
        legal_masks_batch = batch_data["legal_masks"].to(self.device)

        # Conditionally normalize advantages based on configuration
        if self.normalize_advantages:
            advantage_std = advantages_batch.std()
            # Only normalize if we have multiple samples and non-zero std
            if advantage_std > 1e-8 and advantages_batch.shape[0] > 1:
                advantages_batch = (
                    advantages_batch - advantages_batch.mean()
                ) / advantage_std
            # For single sample or zero std, skip normalization to avoid numerical issues

        num_samples = obs_batch.shape[0]
        indices = np.arange(num_samples)

        total_policy_loss_epoch, total_value_loss_epoch, total_entropy_epoch = (
            0.0,
            0.0,
            0.0,
        )
        total_kl_div = 0.0
        num_updates = 0

        for _ in range(self.ppo_epochs):
            self._rng.shuffle(indices)
            for start_idx in range(0, num_samples, self.minibatch_size):
                end_idx = start_idx + self.minibatch_size
                minibatch_indices = indices[start_idx:end_idx]

                obs_minibatch = obs_batch[minibatch_indices]
                actions_minibatch = actions_batch[minibatch_indices]
                old_log_probs_minibatch = old_log_probs_batch[minibatch_indices]
                advantages_minibatch = advantages_batch[minibatch_indices]
                returns_minibatch = returns_batch[minibatch_indices]
                legal_masks_minibatch = legal_masks_batch[minibatch_indices]

                # Get new log_probs, entropy, and value from the model
                # Note on entropy: legal_mask is now passed here. Entropy is calculated
                # over legal actions only.
                if self.use_mixed_precision and self.scaler:
                    # Mixed precision forward pass
                    with torch.cuda.amp.autocast():
                        new_log_probs, entropy, new_values = self.model.evaluate_actions(
                            obs_minibatch,
                            actions_minibatch,
                            legal_mask=legal_masks_minibatch,
                        )
                else:
                    # Standard precision forward pass
                    new_log_probs, entropy, new_values = self.model.evaluate_actions(
                        obs_minibatch,
                        actions_minibatch,
                        legal_mask=legal_masks_minibatch,
                    )

                # PPO Loss Calculation
                ratio = torch.exp(new_log_probs - old_log_probs_minibatch)

                # Calculate KL divergence approximation
                kl_div = (old_log_probs_minibatch - new_log_probs).mean()

                # Clipped surrogate objective
                surr1 = ratio * advantages_minibatch
                surr2 = (
                    torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon)
                    * advantages_minibatch
                )
                policy_loss = -torch.min(surr1, surr2).mean()

                # Value loss (MSE)
                value_loss = F.mse_loss(
                    new_values.squeeze(), returns_minibatch.squeeze()
                )

                # Entropy bonus
                entropy_loss = -entropy.mean()

                # Total loss
                loss = (
                    policy_loss
                    + self.value_loss_coeff * value_loss
                    + self.entropy_coef * entropy_loss
                )

                self.optimizer.zero_grad()
                
                # Fix B4: Use mixed precision for backward pass if enabled
                if self.use_mixed_precision and self.scaler:
                    # Mixed precision backward pass
                    self.scaler.scale(loss).backward()
                    # Unscale gradients before clipping
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=self.gradient_clip_max_norm,
                    )
                    # Update with scaler
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # Standard precision backward pass
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=self.gradient_clip_max_norm,
                    )
                    self.optimizer.step()

                # Step scheduler on minibatch update if configured
                if self.scheduler is not None and self.lr_schedule_step_on == "update":
                    self.scheduler.step()

                total_policy_loss_epoch += policy_loss.item()
                total_value_loss_epoch += value_loss.item()
                total_entropy_epoch += entropy_loss.item()
                total_kl_div += kl_div.item()
                num_updates += 1

        # Step scheduler on epoch if configured
        if self.scheduler is not None and self.lr_schedule_step_on == "epoch":
            self.scheduler.step()

        # Recalculate current learning rate after scheduler step
        current_lr = self.optimizer.param_groups[0]["lr"]
        # Compute average losses over updates
        avg_policy_loss = (
            total_policy_loss_epoch / num_updates if num_updates > 0 else 0.0
        )
        avg_value_loss = (
            total_value_loss_epoch / num_updates if num_updates > 0 else 0.0
        )
        avg_entropy = total_entropy_epoch / num_updates if num_updates > 0 else 0.0
        avg_kl_div = total_kl_div / num_updates if num_updates > 0 else 0.0
        
        # Update tracked KL divergence
        self.last_kl_div = avg_kl_div
        
        # Compile metrics
        return {
            "ppo/policy_loss": avg_policy_loss,
            "ppo/value_loss": avg_value_loss,
            "ppo/entropy": avg_entropy,
            "ppo/kl_divergence_approx": avg_kl_div,
            "ppo/learning_rate": current_lr,
        }

    def save_model(
        self,
        file_path: str,
        global_timestep: int = 0,
        total_episodes_completed: int = 0,
        stats_to_save: Optional[Dict[str, int]] = None,
    ) -> None:
        """Saves the model, optimizer, scheduler, and training state to a file."""
        save_dict = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "global_timestep": global_timestep,
            "total_episodes_completed": total_episodes_completed,
        }
        if stats_to_save:
            save_dict.update(stats_to_save)
        # Include scheduler state if present
        if self.scheduler is not None:
            save_dict["scheduler_state_dict"] = self.scheduler.state_dict()
            save_dict["lr_schedule_type"] = self.lr_schedule_type
            save_dict["lr_schedule_step_on"] = self.lr_schedule_step_on

        torch.save(save_dict, file_path)
        log_info_to_stderr("PPOAgent", f"Model, optimizer, scheduler, and state saved to {file_path}")

    def load_model(self, file_path: str) -> Dict[str, Any]:
        """Loads the model, optimizer, scheduler, and training state from a file."""
        if not os.path.exists(file_path):
            log_error_to_stderr("PPOAgent", f"Checkpoint file {file_path} not found")
            return {
                "global_timestep": 0,
                "total_episodes_completed": 0,
                "black_wins": 0,
                "white_wins": 0,
                "draws": 0,
                "error": "File not found",
            }
        try:
            checkpoint = torch.load(file_path, map_location=self.device)
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            # Load scheduler state if present
            if self.scheduler is not None and "scheduler_state_dict" in checkpoint:
                self.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

            result = {
                "global_timestep": checkpoint.get("global_timestep", 0),
                "total_episodes_completed": checkpoint.get(
                    "total_episodes_completed", 0
                ),
                "black_wins": checkpoint.get("black_wins", 0),
                "white_wins": checkpoint.get("white_wins", 0),
                "draws": checkpoint.get("draws", 0),
                "lr_schedule_type": checkpoint.get("lr_schedule_type", None),
                "lr_schedule_step_on": checkpoint.get("lr_schedule_step_on", "epoch"),
            }
            return result
        except (KeyError, RuntimeError, EOFError) as e:
            log_error_to_stderr("PPOAgent", f"Error loading checkpoint from {file_path}: {e}")
            return {
                "global_timestep": 0,
                "total_episodes_completed": 0,
                "black_wins": 0,
                "white_wins": 0,
                "draws": 0,
                "error": str(e),
            }

    def get_name(self) -> str:  # Added getter for name
        return self.name

]]></file>
  <file path="neural_network.py"><![CDATA[
"""
Minimal ActorCritic neural network for DRL Shogi Client (dummy forward pass).
"""

from torch import nn

from .base_actor_critic import BaseActorCriticModel


class ActorCritic(BaseActorCriticModel):
    """Actor-Critic neural network for Shogi RL agent (PPO-ready)."""

    def __init__(self, input_channels: int, num_actions_total: int):
        """Initialize the ActorCritic network with convolutional and linear layers."""
        super().__init__()
        self.conv = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.flatten = nn.Flatten()
        self.policy_head = nn.Linear(16 * 9 * 9, num_actions_total)
        self.value_head = nn.Linear(16 * 9 * 9, 1)

    def forward(self, x):
        """Forward pass: returns policy logits and value estimate."""
        x = self.conv(x)
        x = self.relu(x)
        x = self.flatten(x)
        policy_logits = self.policy_head(x)
        value = self.value_head(x)
        return policy_logits, value

]]></file>
  <file path="actor_critic_protocol.py"><![CDATA[
"""
Protocol definition for Actor-Critic models in Keisei.
"""

# pylint: disable=unnecessary-ellipsis

from typing import Any, Dict, Iterator, Optional, Protocol, Tuple

import torch
import torch.nn as nn


class ActorCriticProtocol(Protocol):
    """Protocol defining the interface that all Actor-Critic models must implement."""

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through the model.

        Args:
            x: Input observation tensor

        Returns:
            Tuple of (policy_logits, value_estimate)
        """
        ...

    def get_action_and_value(
        self,
        obs: torch.Tensor,
        legal_mask: Optional[torch.Tensor] = None,
        deterministic: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Get action and value from observation.

        Args:
            obs: Input observation tensor
            legal_mask: Optional boolean tensor indicating legal actions
            deterministic: If True, choose action deterministically

        Returns:
            Tuple of (action, log_prob, value)
        """
        ...

    def evaluate_actions(
        self,
        obs: torch.Tensor,
        actions: torch.Tensor,
        legal_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate actions for given observations.

        Args:
            obs: Input observation tensor
            actions: Actions to evaluate
            legal_mask: Optional boolean tensor indicating legal actions

        Returns:
            Tuple of (log_probs, values, entropy)
        """
        ...

    # PyTorch Module methods that are used in PPOAgent
    def train(self, mode: bool = True) -> Any:
        """Set the module in training mode."""
        ...

    def eval(self) -> Any:
        """Set the module in evaluation mode."""
        ...

    def parameters(self) -> Iterator[nn.Parameter]:
        """Return an iterator over module parameters."""
        ...

    def state_dict(self, *args, **kwargs) -> Dict[str, Any]:
        """Return a dictionary containing a whole state of the module."""
        ...

    def load_state_dict(self, state_dict: Dict[str, Any], strict: bool = True) -> Any:
        """Copy parameters and buffers from state_dict into this module and its descendants."""
        ...

    def to(self, *args, **kwargs) -> Any:
        """Move and/or cast the parameters and buffers."""
        ...

]]></file>
  <file path="base_actor_critic.py"><![CDATA[
"""
Base Actor-Critic model implementation for Keisei.

This module provides a shared base class that implements the common ActorCritic
methods to reduce code duplication between different model architectures.
"""

import sys
from abc import ABC, abstractmethod
from typing import Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F

from .actor_critic_protocol import ActorCriticProtocol
from keisei.utils.unified_logger import log_error_to_stderr


class BaseActorCriticModel(nn.Module, ActorCriticProtocol, ABC):
    """
    Abstract base class for Actor-Critic models that implements shared methods.

    This class provides common implementations of get_action_and_value and
    evaluate_actions methods while requiring subclasses to implement the
    forward method.
    """

    @abstractmethod
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through the model. Must be implemented by subclasses.

        Args:
            x: Input observation tensor

        Returns:
            Tuple of (policy_logits, value_estimate)
        """
        raise NotImplementedError("Subclasses must implement forward method")

    def get_action_and_value(
        self,
        obs: torch.Tensor,
        legal_mask: Optional[torch.Tensor] = None,
        deterministic: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Given an observation (and optional legal action mask), return a sampled or deterministically chosen action,
        its log probability, and value estimate.

        Args:
            obs: Input observation tensor.
            legal_mask: Optional boolean tensor indicating legal actions.
                        If provided, illegal actions will be masked out before sampling/argmax.
            deterministic: If True, choose the action with the highest probability (argmax).
                           If False, sample from the distribution.
        Returns:
            action: Chosen action tensor.
            log_prob: Log probability of the chosen action.
            value: Value estimate tensor.
        """
        policy_logits, value = self.forward(obs)

        if legal_mask is not None:
            # Apply the legal mask: set logits of illegal actions to -infinity
            # Ensure legal_mask has the same shape as policy_logits or is broadcastable
            if (
                legal_mask.ndim == 1
                and policy_logits.ndim == 2
                and policy_logits.shape[0] == 1
            ):
                legal_mask = legal_mask.unsqueeze(0)  # Adapt for batch size 1

            masked_logits = torch.where(
                legal_mask,
                policy_logits,
                torch.tensor(float("-inf"), device=policy_logits.device),
            )
            # Handle case where all masked_logits are -inf to prevent NaN in softmax
            if not torch.any(legal_mask):  # Or check if masked_logits are all -inf
                # If no legal moves, softmax over original logits might be one option,
                # or let it produce NaNs which should be caught upstream.
                # For now, this will lead to NaNs if all are masked.
                # This situation should ideally be caught by the caller (e.g. PPOAgent)
                pass  # Let it proceed, PPOAgent should handle no legal moves.
            probs = F.softmax(masked_logits, dim=-1)
        else:
            probs = F.softmax(policy_logits, dim=-1)

        # Check for NaNs in probs, which can happen if all logits were -inf
        if torch.isnan(probs).any():
            # This is a fallback: if probs are NaN (e.g. all legal actions masked out and all logits became -inf),
            # distribute probability uniformly over all actions to avoid erroring out in Categorical.
            # A better solution is for the caller to handle "no legal actions" gracefully.
            log_error_to_stderr(self.__class__.__name__, "NaNs in probabilities in get_action_and_value. Check legal_mask and logits. Defaulting to uniform.")
            probs = torch.ones_like(policy_logits) / policy_logits.shape[-1]

        dist = torch.distributions.Categorical(probs=probs)

        if deterministic:
            action = torch.argmax(probs, dim=-1)
        else:
            action = dist.sample()

        log_prob = dist.log_prob(action)

        # Handle value squeezing - some models squeeze in forward, others don't
        if value.dim() > 1 and value.shape[-1] == 1:
            value = value.squeeze(-1)

        return action, log_prob, value

    def evaluate_actions(
        self,
        obs: torch.Tensor,
        actions: torch.Tensor,
        legal_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate the log probabilities, entropy, and value for given observations and actions.

        Args:
            obs: Input observation tensor.
            actions: Actions taken tensor.
            legal_mask: Optional boolean tensor indicating legal actions.
                        If provided, it can be used to ensure probabilities are calculated correctly,
                        though for evaluating *taken* actions, it's often assumed they were legal.
                        It primarily affects entropy calculation if used to restrict the distribution.
        Returns:
            log_probs: Log probabilities of the taken actions.
            entropy: Entropy of the action distribution.
            value: Value estimate tensor.
        """
        policy_logits, value = self.forward(obs)

        # If legal_mask is None (e.g., when called from PPOAgent.learn during batch processing),
        # the policy distribution (probs) and entropy are calculated over all possible actions,
        # not just those that were legal in the specific states from which 'actions' were sampled.
        # This is a common approach. To calculate entropy strictly over the legal action space
        # for each state in the batch, legal masks for each observation in obs_minibatch
        # would need to be stored in the experience buffer and passed here.
        if legal_mask is not None:
            # Apply the legal mask for calculating probabilities and entropy correctly
            # Ensure legal_mask has the same shape as policy_logits or is broadcastable

            # The shape of legal_mask should be (batch_size, num_actions)
            # The shape of policy_logits is (batch_size, num_actions)
            # No unsqueezing or broadcasting adjustment should be needed here if shapes are consistent.
            # The previous check for legal_mask.ndim == 1 was more for get_action_and_value with batch_size=1.
            # Here, we expect legal_mask to match policy_logits if provided.

            masked_logits = torch.where(
                legal_mask,
                policy_logits,
                torch.tensor(float("-inf"), device=policy_logits.device),
            )
            probs = F.softmax(masked_logits, dim=-1)
        else:
            probs = F.softmax(policy_logits, dim=-1)

        # Check for NaNs in probs (e.g. if all logits in a row were -inf due to masking)
        # Replace NaNs with uniform distribution for stability in entropy calculation for those rows
        if torch.isnan(probs).any():
            log_error_to_stderr(self.__class__.__name__, "NaNs in probabilities in evaluate_actions. Check legal_mask and logits. Defaulting to uniform for affected rows.")
            nan_rows = torch.isnan(probs).any(dim=1)
            probs[nan_rows] = torch.ones_like(probs[nan_rows]) / policy_logits.shape[-1]

        dist = torch.distributions.Categorical(probs=probs)
        log_probs = dist.log_prob(actions)
        entropy = dist.entropy()

        # Handle value squeezing - some models squeeze in forward, others don't
        if value.dim() > 1 and value.shape[-1] == 1:
            value = value.squeeze(-1)

        return log_probs, entropy, value

]]></file>
  <file path="__init__.py"><![CDATA[
# keisei/core/__init__.py

from .base_actor_critic import BaseActorCriticModel
from .neural_network import ActorCritic

__all__ = ["BaseActorCriticModel", "ActorCritic"]

]]></file>
</codebase>
