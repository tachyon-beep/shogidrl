<?xml version="1.0" encoding="UTF-8"?>
<codebase>
  <file path="callback_manager.py"><![CDATA[
"""
training/callback_manager.py: Manages training callbacks and their execution.
"""

from typing import TYPE_CHECKING, Any, List

from . import callbacks

if TYPE_CHECKING:
    from .trainer import Trainer


class CallbackManager:
    """Manages the setup and execution of training callbacks."""

    def __init__(self, config: Any, model_dir: str):
        """
        Initialize the CallbackManager.

        Args:
            config: Training configuration
            model_dir: Directory for saving model checkpoints
        """
        self.config = config
        self.model_dir = model_dir
        self.callbacks: List[callbacks.Callback] = []

    def setup_default_callbacks(self) -> List[callbacks.Callback]:
        """
        Setup default callbacks for training.

        Returns:
            List of configured callbacks
        """
        callback_list: List[callbacks.Callback] = []

        # Checkpoint callback
        checkpoint_interval = self.config.training.checkpoint_interval_timesteps
        callback_list.append(
            callbacks.CheckpointCallback(checkpoint_interval, self.model_dir)
        )

        # Evaluation callback
        eval_cfg = getattr(self.config, "evaluation", None)
        eval_interval = (
            eval_cfg.evaluation_interval_timesteps
            if eval_cfg and hasattr(eval_cfg, "evaluation_interval_timesteps")
            else getattr(self.config.training, "evaluation_interval_timesteps", 1000)
        )
        callback_list.append(callbacks.EvaluationCallback(eval_cfg, eval_interval))

        self.callbacks = callback_list
        return callback_list

    def execute_step_callbacks(self, trainer: "Trainer") -> None:
        """
        Execute on_step_end callbacks for all registered callbacks.

        Args:
            trainer: The trainer instance
        """
        for callback in self.callbacks:
            try:
                callback.on_step_end(trainer)
            except Exception as e:
                # Log error but don't stop training
                if hasattr(trainer, "log_both") and trainer.log_both:
                    trainer.log_both(
                        f"[ERROR] Callback {type(callback).__name__} failed: {e}",
                        also_to_wandb=False,
                    )

    def add_callback(self, callback: callbacks.Callback) -> None:
        """
        Add a custom callback to the manager.

        Args:
            callback: The callback to add
        """
        self.callbacks.append(callback)

    def remove_callback(self, callback_type: type) -> bool:
        """
        Remove callbacks of a specific type.

        Args:
            callback_type: The type of callback to remove

        Returns:
            True if any callbacks were removed, False otherwise
        """
        original_count = len(self.callbacks)
        self.callbacks = [
            cb for cb in self.callbacks if not isinstance(cb, callback_type)
        ]
        return len(self.callbacks) < original_count

    def get_callbacks(self) -> List[callbacks.Callback]:
        """
        Get the list of registered callbacks.

        Returns:
            List of callbacks
        """
        return self.callbacks.copy()

    def clear_callbacks(self) -> None:
        """Clear all registered callbacks."""
        self.callbacks.clear()

]]></file>
  <file path="compatibility_mixin.py"><![CDATA[
"""
compatibility_mixin.py: Provides backward compatibility properties and methods for the Trainer class.
"""

import os
from typing import Any, Callable, Dict, List, Optional


class CompatibilityMixin:
    """
    Mixin class providing backward compatibility properties and methods.
    Keeps the main Trainer class clean while preserving API compatibility.
    """

    # === Model Properties (delegated to ModelManager) ===
    @property
    def feature_spec(self):
        """Access the feature spec through ModelManager."""
        model_manager = getattr(self, "model_manager", None)
        return getattr(model_manager, "feature_spec", None) if model_manager else None

    @property
    def obs_shape(self):
        """Access the observation shape through ModelManager."""
        model_manager = getattr(self, "model_manager", None)
        return getattr(model_manager, "obs_shape", None) if model_manager else None

    @property
    def tower_depth(self):
        """Access the tower depth through ModelManager."""
        model_manager = getattr(self, "model_manager", None)
        return getattr(model_manager, "tower_depth", None) if model_manager else None

    @property
    def tower_width(self):
        """Access the tower width through ModelManager."""
        model_manager = getattr(self, "model_manager", None)
        return getattr(model_manager, "tower_width", None) if model_manager else None

    @property
    def se_ratio(self):
        """Access the SE ratio through ModelManager."""
        model_manager = getattr(self, "model_manager", None)
        return getattr(model_manager, "se_ratio", None) if model_manager else None

    # === Metrics Properties (delegated to MetricsManager) ===
    @property
    def global_timestep(self) -> int:
        """Current global timestep (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        return getattr(metrics_manager, "global_timestep", 0) if metrics_manager else 0

    @global_timestep.setter
    def global_timestep(self, value: int) -> None:
        """Set global timestep (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        if metrics_manager:
            metrics_manager.global_timestep = value

    @property
    def total_episodes_completed(self) -> int:
        """Total episodes completed (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        return (
            getattr(metrics_manager, "total_episodes_completed", 0)
            if metrics_manager
            else 0
        )

    @total_episodes_completed.setter
    def total_episodes_completed(self, value: int) -> None:
        """Set total episodes completed (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        if metrics_manager:
            metrics_manager.total_episodes_completed = value

    @property
    def black_wins(self) -> int:
        """Number of black wins (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        return getattr(metrics_manager, "black_wins", 0) if metrics_manager else 0

    @black_wins.setter
    def black_wins(self, value: int) -> None:
        """Set black wins (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        if metrics_manager:
            metrics_manager.black_wins = value

    @property
    def white_wins(self) -> int:
        """Number of white wins (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        return getattr(metrics_manager, "white_wins", 0) if metrics_manager else 0

    @white_wins.setter
    def white_wins(self, value: int) -> None:
        """Set white wins (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        if metrics_manager:
            metrics_manager.white_wins = value

    @property
    def draws(self) -> int:
        """Number of draws (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        return getattr(metrics_manager, "draws", 0) if metrics_manager else 0

    @draws.setter
    def draws(self, value: int) -> None:
        """Set draws (backward compatibility)."""
        metrics_manager = getattr(self, "metrics_manager", None)
        if metrics_manager:
            metrics_manager.draws = value

    # === Model Artifact Creation (delegated to ModelManager) ===
    def _create_model_artifact(
        self,
        model_path: str,
        artifact_name: Optional[str] = None,
        artifact_type: str = "model",
        description: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        aliases: Optional[List[str]] = None,
        log_both: Optional[Callable] = None,
    ) -> bool:
        """Backward compatibility method - delegates to ModelManager."""
        # Early missing file check
        if not os.path.exists(model_path):
            if log_both:
                log_both(f"Model file {model_path} does not exist.")
            return False

        # Use default artifact name if not provided
        if artifact_name is None:
            artifact_name = os.path.basename(model_path)

        # Use default description if not provided
        if description is None:
            # Use trainer's run_name for backward compatibility (tests set this manually)
            session_manager = getattr(self, "session_manager", None)
            session_run_name = (
                getattr(session_manager, "run_name", "unknown")
                if session_manager
                else "unknown"
            )
            run_name = getattr(self, "run_name", session_run_name)
            description = f"Model checkpoint from run {run_name}"

        # Use trainer's run_name for artifact naming (for backward compatibility with tests)
        session_manager = getattr(self, "session_manager", None)
        session_run_name = (
            getattr(session_manager, "run_name", "unknown")
            if session_manager
            else "unknown"
        )
        run_name = getattr(self, "run_name", session_run_name)

        # Store original logger function and temporarily replace if log_both provided
        model_manager = getattr(self, "model_manager", None)
        if not model_manager:
            return False

        original_logger = getattr(model_manager, "logger_func", None)
        if log_both:
            # Create a wrapper that detects error messages and adds log_level="error"
            def logger_wrapper(message):
                if "Error creating W&B artifact" in message:
                    return log_both(message, log_level="error")
                else:
                    return log_both(message)

            model_manager.logger_func = logger_wrapper

        try:
            result = model_manager.create_model_artifact(
                model_path=model_path,
                artifact_name=artifact_name,
                run_name=run_name,
                is_wandb_active=(
                    getattr(session_manager, "is_wandb_active", False)
                    if session_manager
                    else False
                ),
                artifact_type=artifact_type,
                description=description,
                metadata=metadata,
                aliases=aliases,
            )
        finally:
            # Restore original logger function
            if original_logger is not None:
                model_manager.logger_func = original_logger

        return result

]]></file>
  <file path="display.py"><![CDATA[
"""
training/display.py: Rich UI management for the Shogi RL trainer.
"""

from typing import Any, Dict, List, Union

from rich.console import Console, Group
from rich.layout import Layout
from rich.live import Live
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    ProgressColumn,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from rich.text import Text


class TrainingDisplay:
    def __init__(self, config, trainer, rich_console: Console):
        self.config = config
        self.trainer = trainer
        self.rich_console = rich_console
        self.rich_log_messages = trainer.rich_log_messages
        self.progress_bar, self.training_task, self.layout, self.log_panel = (
            self._setup_rich_progress_display()
        )

    def _setup_rich_progress_display(self):
        progress_columns: List[Union[str, ProgressColumn]]
        base_columns: List[Union[str, ProgressColumn]] = [
            "[progress.description]{task.description}",
            BarColumn(),
            TaskProgressColumn(),
            TextColumn("•"),
            TimeElapsedColumn(),
            TextColumn("•"),
            TimeRemainingColumn(),
            TextColumn(
                "• Steps: {task.completed}/{task.total} ({task.fields[speed]:.1f} it/s)"
            ),
            TextColumn("• {task.fields[ep_metrics]}", style="bright_cyan"),
            TextColumn("• {task.fields[ppo_metrics]}", style="bright_yellow"),
            TextColumn(
                "• Wins B:{task.fields[black_wins_cum]} W:{task.fields[white_wins_cum]} D:{task.fields[draws_cum]}",
                style="bright_green",
            ),
            TextColumn(
                "• Rates B:{task.fields[black_win_rate]:.1%} W:{task.fields[white_win_rate]:.1%} D:{task.fields[draw_rate]:.1%}",
                style="bright_blue",
            ),
        ]
        enable_spinner = getattr(self.config.training, "enable_spinner", True)
        if enable_spinner:
            progress_columns = [SpinnerColumn()] + base_columns
        else:
            progress_columns = base_columns
        progress_bar = Progress(
            *progress_columns,
            console=self.rich_console,
            transient=False,
        )
        initial_black_win_rate = (
            self.trainer.black_wins / self.trainer.total_episodes_completed
            if self.trainer.total_episodes_completed > 0
            else 0.0
        )
        initial_white_win_rate = (
            self.trainer.white_wins / self.trainer.total_episodes_completed
            if self.trainer.total_episodes_completed > 0
            else 0.0
        )
        initial_draw_rate = (
            self.trainer.draws / self.trainer.total_episodes_completed
            if self.trainer.total_episodes_completed > 0
            else 0.0
        )
        training_task = progress_bar.add_task(
            "Training",
            total=self.config.training.total_timesteps,
            completed=self.trainer.global_timestep,
            ep_metrics="Ep L:0 R:0.0",
            ppo_metrics="",
            black_wins_cum=self.trainer.black_wins,
            white_wins_cum=self.trainer.white_wins,
            draws_cum=self.trainer.draws,
            black_win_rate=initial_black_win_rate,
            white_win_rate=initial_white_win_rate,
            draw_rate=initial_draw_rate,
            speed=0.0,
            start=(self.trainer.global_timestep < self.config.training.total_timesteps),
        )
        log_panel = Panel(
            Text(""),
            title="[b]Live Training Log[/b]",
            border_style="bright_green",
            expand=True,
        )
        layout = Layout(name="root")
        layout.split_column(
            Layout(name="main_log", ratio=1),
            Layout(name="progress_display", size=2),
        )
        layout["main_log"].update(log_panel)
        layout["progress_display"].update(progress_bar)
        return progress_bar, training_task, layout, log_panel

    def update_progress(self, trainer, speed, pending_updates):
        update_data = {"completed": trainer.global_timestep, "speed": speed}
        update_data.update(pending_updates)
        self.progress_bar.update(self.training_task, **update_data)

    def update_log_panel(self, trainer):
        visible_rows = max(0, self.rich_console.size.height - 6)
        if trainer.rich_log_messages:
            display_messages = trainer.rich_log_messages[-visible_rows:]
            updated_panel_content = Group(*display_messages)
            self.log_panel.renderable = updated_panel_content
        else:
            self.log_panel.renderable = Text("")

    def start(self):
        return Live(
            self.layout,
            console=self.rich_console,
            refresh_per_second=self.config.training.refresh_per_second,
            transient=False,
        )

]]></file>
  <file path="train_wandb_sweep.py"><![CDATA[
"""
W&B Sweep-enabled training script for Keisei Shogi RL agent.
This script can be used with both regular training and W&B hyperparameter sweeps.
"""

import argparse
import multiprocessing
import sys

from keisei.config_schema import AppConfig
from keisei.training.trainer import Trainer
from keisei.training.utils import apply_wandb_sweep_config, build_cli_overrides
from keisei.utils import load_config
from keisei.utils.unified_logger import log_error_to_stderr


def main():
    """Main entry point for the W&B sweep-enabled training script."""
    parser = argparse.ArgumentParser(
        description="Train PPO agent for Shogi with W&B Sweep support."
    )
    parser.add_argument(
        "--config",
        type=str,
        default="default_config.yaml",
        help="Path to a YAML or JSON configuration file.",
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="Path to a checkpoint file to resume training from, or 'latest' to auto-detect.",
    )
    parser.add_argument(
        "--seed", type=int, default=None, help="Random seed for reproducibility."
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device to use for training (e.g., 'cpu', 'cuda').",
    )
    parser.add_argument(
        "--total-timesteps",
        type=int,
        default=None,
        help="Total timesteps to train for. Overrides config value.",
    )

    args = parser.parse_args()

    # Get W&B sweep overrides if running in a sweep
    sweep_overrides = apply_wandb_sweep_config()

    # Build CLI overrides using shared utility
    cli_overrides = build_cli_overrides(args)

    # Merge sweep overrides with CLI overrides (CLI takes precedence)
    final_overrides = {**sweep_overrides, **cli_overrides}

    # Load config with overrides
    config: AppConfig = load_config(args.config, final_overrides)

    # Initialize and run the trainer
    trainer = Trainer(config=config, args=args)
    trainer.run_training_loop()


if __name__ == "__main__":
    # Set multiprocessing start method for safety, especially with CUDA
    try:
        if multiprocessing.get_start_method(allow_none=True) != "spawn":
            multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError as e:
        log_error_to_stderr("TrainWandbSweep", f"Could not set multiprocessing start method to 'spawn': {e}. Using default: {multiprocessing.get_start_method(allow_none=True)}")
    except OSError as e:
        log_error_to_stderr("TrainWandbSweep", f"Error setting multiprocessing start_method: {e}")

    main()

]]></file>
  <file path="setup_manager.py"><![CDATA[
"""
setup_manager.py: Handles complex initialization and setup logic for the Trainer class.
"""

import sys
from datetime import datetime
from typing import Any, Optional, Tuple

import torch

from keisei.config_schema import AppConfig
from keisei.core.actor_critic_protocol import ActorCriticProtocol
from keisei.core.experience_buffer import ExperienceBuffer
from keisei.core.ppo_agent import PPOAgent
from keisei.utils import TrainingLogger
from keisei.utils.unified_logger import log_error_to_stderr

from .step_manager import StepManager


class SetupManager:
    """
    Manages the complex setup and initialization logic for training components.
    Extracts detailed setup methods from the main Trainer class.
    """

    def __init__(self, config: AppConfig, device: torch.device):
        """
        Initialize the SetupManager.

        Args:
            config: Application configuration
            device: PyTorch device for training
        """
        self.config = config
        self.device = device

    def setup_game_components(self, env_manager, rich_console):
        """
        Initialize game environment and policy mapper using EnvManager.

        Args:
            env_manager: Environment manager instance
            rich_console: Rich console for error display

        Returns:
            Tuple of (game, policy_output_mapper, action_space_size, obs_space_shape)
        """
        try:
            # Call EnvManager's setup_environment to get game and mapper
            game, policy_output_mapper = env_manager.setup_environment()

            # Retrieve environment info
            action_space_size = env_manager.action_space_size
            obs_space_shape = env_manager.obs_space_shape

            if game is None or policy_output_mapper is None:
                raise RuntimeError(
                    "EnvManager.setup_environment() failed to return valid game or policy_output_mapper."
                )

            return game, policy_output_mapper, action_space_size, obs_space_shape

        except (RuntimeError, ValueError, OSError) as e:
            rich_console.print(
                f"[bold red]Error initializing game components: {e}. Aborting.[/bold red]"
            )
            raise RuntimeError(f"Failed to initialize game components: {e}") from e

    def setup_training_components(self, model_manager):
        """
        Initialize PPO agent and experience buffer.

        Args:
            model_manager: Model manager instance

        Returns:
            Tuple of (model, agent, experience_buffer)
        """
        # Create model using ModelManager
        model = model_manager.create_model()

        if model is None:
            raise RuntimeError(
                "Model was not created successfully before agent initialization."
            )

        # Initialize PPOAgent with the model (dependency injection)
        agent = PPOAgent(
            model=model,
            config=self.config,
            device=self.device,
            scaler=model_manager.scaler,
            use_mixed_precision=model_manager.use_mixed_precision,
        )

        experience_buffer = ExperienceBuffer(
            buffer_size=self.config.training.steps_per_epoch,
            gamma=self.config.training.gamma,
            lambda_gae=self.config.training.lambda_gae,
            device=self.config.env.device,
        )

        return model, agent, experience_buffer

    def setup_step_manager(self, game, agent, policy_output_mapper, experience_buffer):
        """
        Initialize StepManager for step execution and episode management.

        Args:
            game: Game environment instance
            agent: PPO agent instance
            policy_output_mapper: Policy output mapper
            experience_buffer: Experience buffer instance

        Returns:
            Configured StepManager instance
        """
        step_manager = StepManager(
            config=self.config,
            game=game,
            agent=agent,
            policy_mapper=policy_output_mapper,
            experience_buffer=experience_buffer,
        )
        return step_manager

    def handle_checkpoint_resume(
        self,
        model_manager,
        agent,
        model_dir,
        resume_path_override,
        metrics_manager,
        logger,
    ):
        """
        Handle resuming from checkpoint using ModelManager.

        Args:
            model_manager: Model manager instance
            agent: PPO agent instance
            model_dir: Model directory path
            resume_path_override: Optional resume path override
            metrics_manager: Metrics manager instance
            logger: Logger instance

        Returns:
            True if resumed from checkpoint, False otherwise
        """
        if not agent:
            logger.log(
                "[ERROR] Agent not initialized before handling checkpoint resume. This should not happen."
            )
            raise RuntimeError("Agent not initialized before _handle_checkpoint_resume")

        model_manager.handle_checkpoint_resume(
            agent=agent,
            model_dir=model_dir,
            resume_path_override=resume_path_override,
        )

        resumed_from_checkpoint = model_manager.resumed_from_checkpoint

        # Restore training state from checkpoint data
        if model_manager.checkpoint_data:
            checkpoint_data = model_manager.checkpoint_data
            metrics_manager.restore_from_checkpoint(checkpoint_data)

        return resumed_from_checkpoint

    def log_event(self, message: str, log_file_path: str):
        """Log important events to the main training log file."""
        try:
            with open(log_file_path, "a", encoding="utf-8") as f:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{timestamp}] {message}\n")
        except (OSError, IOError) as e:
            log_error_to_stderr("SetupManager", "Failed to log event", e)

    def log_run_info(
        self, session_manager, model_manager, agent, metrics_manager, log_both
    ):
        """Log run information at the start of training."""
        # Delegate session info logging to SessionManager
        agent_name = "N/A"
        agent_type_name = "N/A"
        if agent:
            agent_name = getattr(agent, "name", "N/A")
            agent_type_name = type(agent).__name__

        agent_info = {"type": agent_type_name, "name": agent_name}

        def log_wrapper(msg):
            log_both(msg)

        session_manager.log_session_info(
            logger_func=log_wrapper,
            agent_info=agent_info,
            resumed_from_checkpoint=model_manager.resumed_from_checkpoint,
            global_timestep=metrics_manager.global_timestep,
            total_episodes_completed=metrics_manager.total_episodes_completed,
        )

        # Log model structure using ModelManager
        model_info = model_manager.get_model_info()
        log_both(f"Model Structure:\n{model_info}", also_to_wandb=False)

        # Log to main training log file
        self.log_event(f"Model Structure:\n{model_info}", session_manager.log_file_path)

]]></file>
  <file path="train.py"><![CDATA[
"""
Main training script for Keisei Shogi RL agent.
Refactored to use the Trainer class for better modularity.
"""

import argparse
import multiprocessing
import sys

from keisei.config_schema import AppConfig

# Import config module and related components
from keisei.utils import load_config
from keisei.utils.unified_logger import log_info_to_stderr, log_error_to_stderr

from .trainer import Trainer
from .utils import apply_wandb_sweep_config, build_cli_overrides


def main():
    """Main entry point for the training script (Pydantic config version)."""
    parser = argparse.ArgumentParser(
        description="Train PPO agent for Shogi with Rich TUI (Pydantic config)."
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to a YAML or JSON configuration file.",
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="Path to a checkpoint file to resume training from, or 'latest' to auto-detect.",
    )
    parser.add_argument(
        "--seed", type=int, default=None, help="Random seed for reproducibility."
    )
    # --- Model/feature CLI flags ---
    parser.add_argument(
        "--model", type=str, default=None, help="Model type (e.g. 'resnet')."
    )
    parser.add_argument(
        "--input_features",
        type=str,
        default=None,
        help="Feature set for observation builder.",
    )
    parser.add_argument(
        "--tower_depth", type=int, default=None, help="ResNet tower depth."
    )
    parser.add_argument(
        "--tower_width", type=int, default=None, help="ResNet tower width."
    )
    parser.add_argument(
        "--se_ratio", type=float, default=None, help="SE block squeeze ratio."
    )
    parser.add_argument(
        "--mixed_precision",
        action="store_true",
        help="Enable mixed-precision training.",
    )
    parser.add_argument(
        "--ddp", action="store_true", help="Enable DistributedDataParallel training."
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device to use for training (e.g., 'cpu', 'cuda').",
    )
    parser.add_argument(
        "--total-timesteps",
        type=int,
        default=None,
        help="Total timesteps to train for. Overrides config value.",
    )
    parser.add_argument(
        "--savedir",
        type=str,
        default=None,
        help="Directory to save models and logs. Overrides config MODEL_DIR.",
    )
    parser.add_argument(
        "--override",
        action="append",
        default=[],
        help="Override config values via KEY.SUBKEY=VALUE format.",
    )
    parser.add_argument(
        "--render-every",
        type=int,
        default=None,
        help="Update display every N steps to reduce flicker. Overrides config value.",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        default=None,
        help="Optional name for this run (overrides config and auto-generated name).",
    )
    # --- W&B Sweep support flags ---
    parser.add_argument(
        "--wandb-enabled",
        action="store_true",
        help="Force enable W&B logging (useful for sweeps).",
    )
    args = parser.parse_args()

    # Get W&B sweep overrides if running in a sweep
    sweep_overrides = apply_wandb_sweep_config()

    # Build CLI overrides dict (dot notation)
    cli_overrides = {}
    for override in args.override:
        if "=" in override:
            k, v = override.split("=", 1)
            cli_overrides[k] = v

    # Add direct CLI args as overrides using shared utility
    direct_cli_overrides = build_cli_overrides(args)
    cli_overrides.update(direct_cli_overrides)
    # Do NOT generate run_name here; let Trainer handle it for correct CLI/config/auto priority

    # Merge sweep overrides with CLI overrides (CLI takes precedence)
    final_overrides = {**sweep_overrides, **cli_overrides}

    # Load config (YAML/JSON + CLI overrides + sweep overrides)
    config: AppConfig = load_config(args.config, final_overrides)

    # Initialize and run the trainer (Trainer will determine run_name)
    trainer = Trainer(config=config, args=args)
    trainer.run_training_loop()


if __name__ == "__main__":
    # Fix B6: Add multiprocessing.freeze_support() for Windows compatibility
    multiprocessing.freeze_support()
    
    # Set multiprocessing start method for safety, especially with CUDA
    try:
        if multiprocessing.get_start_method(allow_none=True) != "spawn":
            multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError as e:
        log_error_to_stderr("Train", f"Could not set multiprocessing start method to 'spawn': {e}. Using default: {multiprocessing.get_start_method(allow_none=True)}")
    except Exception as e:
        log_error_to_stderr("Train", f"Error setting multiprocessing start_method: {e}")

    main()

]]></file>
  <file path="env_manager.py"><![CDATA[
"""
training/env_manager.py: Environment management for Shogi RL training.

This module handles environment-related concerns including:
- Game environment initialization and configuration
- Policy output mapper setup and validation
- Action space configuration and validation
- Environment seeding
- Observation space setup
"""

import sys
from typing import Any, Callable, Optional, Tuple  # Added Optional

import numpy as np  # Added for type hinting

from keisei.config_schema import AppConfig
from keisei.shogi import ShogiGame
from keisei.utils import PolicyOutputMapper

# Callable already imported via the line above


class EnvManager:
    """Manages environment setup and configuration for training runs."""

    def __init__(self, config: AppConfig, logger_func: Optional[Callable] = None):
        """
        Initialize the EnvManager.

        Args:
            config: Application configuration
            logger_func: Optional logging function for status messages
        """
        self.config = config
        self.logger_func = logger_func or (lambda msg: None)

        # Initialize environment components (will be set by setup_environment)
        self.game: Optional[ShogiGame] = None
        self.policy_output_mapper: Optional[PolicyOutputMapper] = None
        self.action_space_size: int = 0
        self.obs_space_shape: Optional[Tuple[int, int, int]] = None

        # Environment setup is now called explicitly by Trainer

    def setup_environment(self) -> Tuple[ShogiGame, PolicyOutputMapper]:
        """Initialize game environment and related components.

        Returns:
            Tuple[ShogiGame, PolicyOutputMapper]: The initialized game and policy mapper.
        """
        try:
            # Initialize the Shogi game
            self.game = ShogiGame(max_moves_per_game=self.config.env.max_moves_per_game)

            # Setup seeding if specified
            if hasattr(self.game, "seed") and self.config.env.seed is not None:
                try:
                    self.game.seed(self.config.env.seed)
                    self.logger_func(f"Environment seeded with: {self.config.env.seed}")
                except Exception as e:
                    self.logger_func(f"Warning: Failed to seed environment: {e}")

            # Setup observation space shape
            self.obs_space_shape = (self.config.env.input_channels, 9, 9)

        except (RuntimeError, ValueError, OSError) as e:
            self.logger_func(f"Error initializing ShogiGame: {e}. Aborting.")
            raise RuntimeError(f"Failed to initialize ShogiGame: {e}") from e

        # Initialize policy output mapper
        try:
            self.policy_output_mapper = PolicyOutputMapper()
            self.action_space_size = self.policy_output_mapper.get_total_actions()

            # Validate action space consistency
            self._validate_action_space()

        except (RuntimeError, ValueError) as e:
            self.logger_func(f"Error initializing PolicyOutputMapper: {e}")
            raise RuntimeError(f"Failed to initialize PolicyOutputMapper: {e}") from e

        return self.game, self.policy_output_mapper

    def _validate_action_space(self):
        """Validate that action space configuration is consistent."""
        if self.policy_output_mapper is None:
            # This should not happen if called after policy_output_mapper is initialized
            self.logger_func(
                "CRITICAL: PolicyOutputMapper not initialized before _validate_action_space."
            )
            raise ValueError("PolicyOutputMapper not initialized.")

        config_num_actions = self.config.env.num_actions_total
        mapper_num_actions = self.policy_output_mapper.get_total_actions()

        if config_num_actions != mapper_num_actions:
            error_msg = (
                f"Action space mismatch: config specifies {config_num_actions} "
                f"actions but PolicyOutputMapper provides {mapper_num_actions} actions"
            )
            self.logger_func(f"CRITICAL: {error_msg}")
            raise ValueError(error_msg)

        self.logger_func(f"Action space validated: {mapper_num_actions} total actions")

    def get_environment_info(self) -> dict:
        """Get information about the current environment configuration."""
        return {
            "game": self.game,
            "policy_mapper": self.policy_output_mapper,
            "action_space_size": self.action_space_size,
            "obs_space_shape": self.obs_space_shape,
            "input_channels": self.config.env.input_channels,
            "num_actions_total": self.config.env.num_actions_total,
            "seed": self.config.env.seed,
            "game_type": type(self.game).__name__,
            "policy_mapper_type": type(self.policy_output_mapper).__name__,
        }

    def reset_game(self):
        """Reset the game environment to initial state."""
        if not self.game:
            self.logger_func("Error: Game not initialized. Cannot reset.")
            return False
        try:
            self.game.reset()
            return True
        except Exception as e:
            self.logger_func(f"Error resetting game: {e}")
            return False

    def initialize_game_state(self) -> Optional[np.ndarray]:
        """
        Resets the game environment and returns the initial observation.
        ShogiGame.reset() is expected to return the initial observation.

        Returns:
            Optional[np.ndarray]: The initial observation from the environment, or None on error.
        """
        if not self.game:
            self.logger_func("Error: Game not initialized. Cannot get initial state.")
            return None
        try:
            # ShogiGame.reset() now returns the observation directly.
            initial_obs = self.game.reset()
            self.logger_func(
                "Game state initialized and initial observation obtained from game.reset()."
            )
            return initial_obs
        except Exception as e:
            self.logger_func(f"Error initializing game state: {e}")
            return None

    def validate_environment(self) -> bool:
        """
        Validate that the environment is properly configured and functional.

        Returns:
            bool: True if environment is valid, False otherwise
        """
        try:
            # Check game initialization
            if self.game is None:
                self.logger_func("Environment validation failed: game not initialized")
                return False

            # Check policy mapper
            if self.policy_output_mapper is None:
                self.logger_func(
                    "Environment validation failed: policy mapper not initialized"
                )
                return False

            # Check action space consistency
            if self.action_space_size <= 0:
                self.logger_func(
                    "Environment validation failed: invalid action space size"
                )
                return False

            # Test game reset functionality
            # Get initial observation, reset, then get another and compare
            # This assumes get_observation() is available and returns a comparable state.
            if not self.game:  # Should be caught by earlier check, but good practice
                self.logger_func(
                    "Environment validation failed: game not initialized for reset test."
                )
                return False

            obs1 = self.game.get_observation()
            if not self.reset_game():  # Resets the game
                self.logger_func("Environment validation failed: game reset failed")
                return False
            obs2_after_reset = self.game.get_observation()  # Get obs after reset

            # Simple comparison; for complex objects, a more robust comparison might be needed
            if not np.array_equal(obs1, obs2_after_reset):
                self.logger_func(
                    "Environment validation warning: Observation after reset differs from initial observation. This might be expected if seeding is not deterministic or initial state has randomness."
                )
            # Depending on game logic, obs1 and obs2_after_reset should ideally be the same if reset is deterministic.
            # For now, we just check if reset_game() itself succeeded.

            # Test observation space
            if self.obs_space_shape is None or len(self.obs_space_shape) != 3:
                self.logger_func(
                    "Environment validation failed: invalid observation space shape"
                )
                return False

            self.logger_func("Environment validation passed")
            return True

        except Exception as e:
            self.logger_func(f"Environment validation failed with exception: {e}")
            return False

    def get_legal_moves_count(self) -> int:
        """Get the number of legal moves in the current game state."""
        if not self.game:
            self.logger_func(
                "Error: Game not initialized. Cannot get legal moves count."
            )
            return 0
        try:
            legal_moves = self.game.get_legal_moves()
            return len(legal_moves) if legal_moves else 0
        except Exception as e:
            self.logger_func(f"Error getting legal moves count: {e}")
            return 0

    def setup_seeding(self, seed: Optional[int] = None):
        """
        Setup seeding for the environment.

        Args:
            seed: Optional seed value. If None, uses config seed.
        """
        seed_value = seed if seed is not None else self.config.env.seed

        if not self.game:
            self.logger_func("Error: Game not initialized. Cannot set seed.")
            return False

        if seed_value is not None and hasattr(self.game, "seed"):
            try:
                self.game.seed(seed_value)
                self.logger_func(f"Environment re-seeded with: {seed_value}")
                return True
            except Exception as e:
                self.logger_func(f"Error setting environment seed: {e}")
                return False
        elif seed_value is None:
            self.logger_func("No seed value provided for re-seeding.")
            return False  # Or True if no-op is considered success
        else:  # game does not have seed method
            self.logger_func(
                f"Warning: Game object does not have a 'seed' method. Cannot re-seed with {seed_value}."
            )
            return False

]]></file>
  <file path="session_manager.py"><![CDATA[
"""
training/session_manager.py: Session lifecycle management for Shogi RL training.

This module handles session-level concerns including:
- Run name generation and validation
- Directory structure creation and management
- Configuration serialization and persistence
- WandB initialization and configuration
- Session logging and reporting
"""

import os
import sys
from datetime import datetime
from typing import Any, Callable, Dict, Optional

import wandb
from keisei.config_schema import AppConfig
from keisei.utils.utils import generate_run_name
from keisei.utils.unified_logger import log_error_to_stderr, log_warning_to_stderr

from . import utils


class SessionManager:
    """Manages session-level lifecycle for training runs."""

    def __init__(self, config: AppConfig, args: Any, run_name: Optional[str] = None):
        """
        Initialize the SessionManager.

        Args:
            config: Application configuration
            args: Command-line arguments
            run_name: Optional explicit run name (overrides config and auto-generation)
        """
        self.config = config
        self.args = args

        # Determine run_name: explicit > CLI > config > auto-generate
        if run_name:
            self._run_name = run_name
        elif hasattr(args, "run_name") and args.run_name:
            self._run_name = args.run_name
        elif (
            hasattr(config, "logging")
            and hasattr(config.logging, "run_name")
            and config.logging.run_name
        ):
            self._run_name = config.logging.run_name
        else:
            self._run_name = generate_run_name(config, None)

        # Initialize paths - will be set by setup_directories()
        self._run_artifact_dir: Optional[str] = None
        self._model_dir: Optional[str] = None
        self._log_file_path: Optional[str] = None
        self._eval_log_file_path: Optional[str] = None

        # WandB state
        self._is_wandb_active: Optional[bool] = None

    @property
    def run_name(self) -> str:
        """Get the run name."""
        return self._run_name

    @property
    def run_artifact_dir(self) -> str:
        """Get the run artifact directory path."""
        if self._run_artifact_dir is None:
            raise RuntimeError(
                "Directories not yet set up. Call setup_directories() first."
            )
        return self._run_artifact_dir

    @property
    def model_dir(self) -> str:
        """Get the model directory path."""
        if self._model_dir is None:
            raise RuntimeError(
                "Directories not yet set up. Call setup_directories() first."
            )
        return self._model_dir

    @property
    def log_file_path(self) -> str:
        """Get the log file path."""
        if self._log_file_path is None:
            raise RuntimeError(
                "Directories not yet set up. Call setup_directories() first."
            )
        return self._log_file_path

    @property
    def eval_log_file_path(self) -> str:
        """Get the evaluation log file path."""
        if self._eval_log_file_path is None:
            raise RuntimeError(
                "Directories not yet set up. Call setup_directories() first."
            )
        return self._eval_log_file_path

    @property
    def is_wandb_active(self) -> bool:
        """Check if WandB is active."""
        if self._is_wandb_active is None:
            raise RuntimeError("WandB not yet initialized. Call setup_wandb() first.")
        return bool(self._is_wandb_active)

    def setup_directories(self) -> Dict[str, str]:
        """
        Set up directory structure for the training run.

        Returns:
            Dictionary containing directory paths
        """
        try:
            dirs = utils.setup_directories(self.config, self._run_name)
            self._run_artifact_dir = dirs["run_artifact_dir"]
            self._model_dir = dirs["model_dir"]
            self._log_file_path = dirs["log_file_path"]
            self._eval_log_file_path = dirs["eval_log_file_path"]
            return dirs
        except (OSError, PermissionError) as e:
            raise RuntimeError(f"Failed to setup directories: {e}") from e

    def setup_wandb(self) -> bool:
        """
        Initialize Weights & Biases logging.

        Returns:
            True if WandB is active, False otherwise
        """
        if self._run_artifact_dir is None:
            raise RuntimeError("Directories must be set up before initializing WandB.")

        try:
            self._is_wandb_active = utils.setup_wandb(
                self.config, self._run_name, self._run_artifact_dir
            )
            return bool(self._is_wandb_active)
        except Exception as e:  # Catch all exceptions for WandB setup
            log_warning_to_stderr("SessionManager", f"WandB setup failed: {e}")
            self._is_wandb_active = False
            return False

    def save_effective_config(self) -> None:
        """Save the effective configuration to a JSON file."""
        if self._run_artifact_dir is None:
            raise RuntimeError("Directories must be set up before saving config.")

        try:
            # Ensure the directory exists
            os.makedirs(self._run_artifact_dir, exist_ok=True)

            effective_config_str = utils.serialize_config(self.config)
            config_path = os.path.join(self._run_artifact_dir, "effective_config.json")
            with open(config_path, "w", encoding="utf-8") as f:
                f.write(effective_config_str)
        except (OSError, TypeError) as e:
            log_error_to_stderr("SessionManager", f"Error saving effective_config.json: {e}")
            raise RuntimeError(f"Failed to save effective config: {e}") from e

    def log_session_info(
        self,
        logger_func: Callable[[str], None],
        agent_info: Optional[Dict[str, Any]] = None,
        resumed_from_checkpoint: Optional[str] = None,
        global_timestep: int = 0,
        total_episodes_completed: int = 0,
    ) -> None:
        """
        Log comprehensive session information.

        Args:
            logger_func: Function to call for logging messages
            agent_info: Optional agent information (name, type)
            resumed_from_checkpoint: Optional checkpoint path if resumed
            global_timestep: Current global timestep
            total_episodes_completed: Total episodes completed so far
        """
        # Session title with optional WandB URL
        run_title = f"Keisei Training Run: {self._run_name}"
        if self._is_wandb_active and wandb.run and hasattr(wandb.run, "url"):
            run_title += f" (W&B: {wandb.run.url})"

        logger_func(run_title)
        logger_func(f"Run directory: {self._run_artifact_dir}")

        # Ensure directory exists before constructing paths
        if self._run_artifact_dir:
            config_path = os.path.join(self._run_artifact_dir, "effective_config.json")
            logger_func(f"Effective config saved to: {config_path}")
        else:
            logger_func("Warning: Run artifact directory not set")

        # Configuration information
        if self.config.env.seed is not None:
            logger_func(f"Random seed: {self.config.env.seed}")

        logger_func(f"Device: {self.config.env.device}")

        # Agent information
        if agent_info:
            logger_func(
                f"Agent: {agent_info.get('type', 'Unknown')} ({agent_info.get('name', 'Unknown')})"
            )

        logger_func(
            f"Total timesteps: {self.config.training.total_timesteps}, "
            f"Steps per PPO epoch: {self.config.training.steps_per_epoch}"
        )

        # Resume information
        if global_timestep > 0:
            if resumed_from_checkpoint:
                logger_func(
                    f"Resumed training from checkpoint: {resumed_from_checkpoint}"
                )
            logger_func(
                f"Resuming from timestep {global_timestep}, {total_episodes_completed} episodes completed."
            )
        else:
            logger_func("Starting fresh training.")

    def log_session_start(self) -> None:
        """Log session start event to file."""
        if self._log_file_path is None:
            raise RuntimeError(
                "Directories must be set up before logging session start."
            )

        try:
            with open(self._log_file_path, "a", encoding="utf-8") as f:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"[{timestamp}] --- SESSION START: {self._run_name} ---\n")
        except (OSError, IOError) as e:
            log_error_to_stderr("SessionManager", f"Failed to log session start: {e}")

    def finalize_session(self) -> None:
        """Finalize the training session."""
        if self._is_wandb_active and wandb.run:
            try:
                # Fix B5: Use cross-platform threading.Timer instead of POSIX signal
                import threading
                
                timeout_occurred = threading.Event()
                
                def timeout_handler():
                    timeout_occurred.set()

                # Set up timeout (10 seconds) - cross-platform compatible
                timer = threading.Timer(10.0, timeout_handler)
                timer.start()

                try:
                    # Check periodically if timeout occurred
                    import time
                    start_time = time.time()
                    while not timeout_occurred.is_set() and (time.time() - start_time) < 10:
                        try:
                            wandb.finish()
                            break  # Success, exit loop
                        except Exception:
                            time.sleep(0.1)  # Brief wait before retry
                    
                    if timeout_occurred.is_set():
                        raise TimeoutError("WandB finalization timed out")
                        
                finally:
                    timer.cancel()  # Cancel the timer

            except (KeyboardInterrupt, TimeoutError):
                log_warning_to_stderr("SessionManager", "WandB finalization interrupted or timed out")
                try:
                    # Force finish without waiting
                    wandb.finish(exit_code=1)
                except Exception:
                    pass
            except Exception as e:  # Catch all exceptions for WandB finalization
                log_warning_to_stderr("SessionManager", f"WandB finalization failed: {e}")

    def setup_seeding(self) -> None:
        """Setup random seeding based on configuration."""
        utils.setup_seeding(self.config)

    def get_session_summary(self) -> Dict[str, Any]:
        """
        Get a summary of the session configuration.

        Returns:
            Dictionary containing session summary information
        """
        return {
            "run_name": self._run_name,
            "run_artifact_dir": self._run_artifact_dir,
            "model_dir": self._model_dir,
            "log_file_path": self._log_file_path,
            "is_wandb_active": self._is_wandb_active,
            "seed": self.config.env.seed if hasattr(self.config.env, "seed") else None,
            "device": (
                self.config.env.device if hasattr(self.config.env, "device") else None
            ),
        }

]]></file>
  <file path="utils.py"><![CDATA[
"""
training/utils.py: Helper functions for setup and configuration in the Shogi RL trainer.
"""

import glob
import json
import os
import pickle
import random
import sys
from typing import Optional

import numpy as np
import torch

import wandb
from keisei.utils.unified_logger import log_error_to_stderr, log_info_to_stderr
from keisei.config_schema import AppConfig


def _validate_checkpoint(checkpoint_path: str) -> bool:
    """Validate checkpoint file integrity by attempting to load it.
    
    Args:
        checkpoint_path: Path to checkpoint file to validate
        
    Returns:
        True if checkpoint loads successfully, False otherwise
    """
    try:
        # Attempt minimal load to check file integrity
        torch.load(checkpoint_path, map_location='cpu')
        return True
    except (OSError, RuntimeError, EOFError, pickle.UnpicklingError) as e:
        log_error_to_stderr("TrainingUtils", f"Corrupted checkpoint {checkpoint_path}: {e}")
        return False


def find_latest_checkpoint(model_dir_path: str) -> Optional[str]:
    try:
        checkpoints = glob.glob(os.path.join(model_dir_path, "*.pth"))
        if not checkpoints:
            checkpoints = glob.glob(os.path.join(model_dir_path, "*.pt"))
        if not checkpoints:
            return None
        
        # Sort checkpoints by modification time (newest first)
        checkpoints.sort(key=os.path.getmtime, reverse=True)
        
        # Find the first valid (non-corrupted) checkpoint
        for checkpoint_path in checkpoints:
            if _validate_checkpoint(checkpoint_path):
                return checkpoint_path
        
        # If we get here, all checkpoints are corrupted
        log_error_to_stderr("TrainingUtils", "All checkpoint files in directory are corrupted or unreadable")
        return None
        
    except OSError as e:
        log_error_to_stderr("TrainingUtils", f"Error in find_latest_checkpoint: {e}")
        return None


def serialize_config(config: AppConfig) -> str:
    """Serialize AppConfig to JSON string using Pydantic's built-in serialization.
    
    Args:
        config: AppConfig instance to serialize
        
    Returns:
        JSON string representation of the configuration
    """
    return config.model_dump_json(indent=4)


def setup_directories(config, run_name):
    model_dir = config.logging.model_dir
    log_file = config.logging.log_file
    run_artifact_dir = os.path.join(model_dir, run_name)
    model_dir_path = run_artifact_dir
    log_file_path = os.path.join(run_artifact_dir, os.path.basename(log_file))
    eval_log_file_path = os.path.join(run_artifact_dir, "rich_periodic_eval_log.txt")
    os.makedirs(run_artifact_dir, exist_ok=True)
    return {
        "run_artifact_dir": run_artifact_dir,
        "model_dir": model_dir_path,
        "log_file_path": log_file_path,
        "eval_log_file_path": eval_log_file_path,
    }


def setup_seeding(config):
    seed = config.env.seed
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)


def setup_wandb(config, run_name, run_artifact_dir):
    wandb_cfg = config.wandb
    is_active = wandb_cfg.enabled
    if is_active:
        try:
            config_dict_for_wandb = (
                json.loads(serialize_config(config)) if serialize_config(config) else {}
            )
            wandb.init(
                project=wandb_cfg.project,
                entity=wandb_cfg.entity,
                name=run_name,
                config=config_dict_for_wandb,
                mode="online" if wandb_cfg.enabled else "disabled",
                dir=run_artifact_dir,
                resume="allow",
                id=run_name,
            )
        except (TypeError, ValueError, OSError) as e:
            log_error_to_stderr("TrainingUtils", f"Error initializing W&B: {e}. W&B logging disabled.")
            is_active = False
    if not is_active:
        log_info_to_stderr("TrainingUtils", "Weights & Biases logging is disabled or failed to initialize.")
    return is_active


def apply_wandb_sweep_config():
    """Apply W&B sweep configuration to override parameters.

    Returns:
        Dict[str, Any]: Dictionary of configuration overrides extracted from sweep parameters.
                       Empty dict if no W&B sweep is active.
    """
    if wandb.run is None:
        return {}

    sweep_config = wandb.config
    log_info_to_stderr("TrainingUtils", f"Running W&B sweep with config: {dict(sweep_config)}")

    # Map W&B sweep parameters to config paths
    sweep_param_mapping = {
        "learning_rate": "training.learning_rate",
        "gamma": "training.gamma",
        "clip_epsilon": "training.clip_epsilon",
        "ppo_epochs": "training.ppo_epochs",
        "minibatch_size": "training.minibatch_size",
        "value_loss_coeff": "training.value_loss_coeff",
        "entropy_coef": "training.entropy_coef",
        "tower_depth": "training.tower_depth",
        "tower_width": "training.tower_width",
        "se_ratio": "training.se_ratio",
        "steps_per_epoch": "training.steps_per_epoch",
        "gradient_clip_max_norm": "training.gradient_clip_max_norm",
        "lambda_gae": "training.lambda_gae",
    }

    # Apply sweep parameters as overrides
    sweep_overrides = {"wandb.enabled": True}  # Force enable W&B for sweeps
    for sweep_key, config_path in sweep_param_mapping.items():
        if hasattr(sweep_config, sweep_key):
            sweep_overrides[config_path] = getattr(sweep_config, sweep_key)

    return sweep_overrides


def build_cli_overrides(args) -> dict:
    """
    Build configuration overrides from command line arguments.
    
    This function centralizes the logic for converting CLI arguments
    to configuration overrides, eliminating duplication between training scripts.
    
    Args:
        args: Parsed command line arguments from argparse
        
    Returns:
        dict: Configuration overrides in dot notation format
    """
    cli_overrides = {}
    
    # Core overrides supported by both training scripts
    if hasattr(args, 'seed') and args.seed is not None:
        cli_overrides["env.seed"] = args.seed
    if hasattr(args, 'device') and args.device is not None:
        cli_overrides["env.device"] = args.device
    if hasattr(args, 'total_timesteps') and args.total_timesteps is not None:
        cli_overrides["training.total_timesteps"] = args.total_timesteps
    
    # Extended overrides only available in main training script
    if hasattr(args, 'savedir') and args.savedir is not None:
        cli_overrides["logging.model_dir"] = args.savedir
    if hasattr(args, 'render_every') and args.render_every is not None:
        cli_overrides["training.render_every_steps"] = args.render_every
    if hasattr(args, 'wandb_enabled') and args.wandb_enabled:
        cli_overrides["wandb.enabled"] = True
    
    return cli_overrides

]]></file>
  <file path="training_loop_manager.py"><![CDATA[
# keisei/training/training_loop_manager.py
"""
Manages the main training loop execution, previously part of the Trainer class.
"""
import time
from typing import TYPE_CHECKING, Any, Dict, Optional, cast

import torch.nn as nn
from keisei.utils.unified_logger import log_info_to_stderr

# Constants
STEP_MANAGER_NOT_AVAILABLE_MSG = "StepManager is not available"

if TYPE_CHECKING:
    from typing import Callable  # Added Callable

    from keisei.config_schema import AppConfig
    from keisei.core.experience_buffer import ExperienceBuffer
    from keisei.core.ppo_agent import PPOAgent
    from keisei.training.callbacks import Callback
    from keisei.training.display import TrainingDisplay
    from keisei.training.parallel import ParallelManager
    from keisei.training.step_manager import (  # Added StepResult
        EpisodeState,
        StepManager,
        StepResult,
    )
    from keisei.training.trainer import Trainer  # Forward reference


class TrainingLoopManager:
    """
    Manages the primary iteration logic of the training loop.
    """

    def __init__(
        self,
        trainer: "Trainer",
        # Components are accessed via trainer instance
    ):
        self.trainer = trainer
        self.config = trainer.config  # Convenience access
        self.agent = trainer.agent  # Convenience access
        self.buffer = trainer.experience_buffer  # Convenience access
        self.step_manager = trainer.step_manager  # Convenience access
        self.display = trainer.display  # Convenience access
        self.callbacks = trainer.callbacks  # Convenience access

        self.current_epoch: int = 0
        self.episode_state: Optional["EpisodeState"] = None  # Set by Trainer before run

        # For SPS calculation and display update throttling
        self.last_time_for_sps: float = 0.0
        self.steps_since_last_time_for_sps: int = 0
        self.last_display_update_time: float = 0.0

        # Initialize parallel manager if enabled
        self.parallel_manager: Optional["ParallelManager"] = None
        if self.config.parallel.enabled:
            from keisei.training.parallel import ParallelManager

            # Build config dictionaries for parallel manager
            env_config = self._build_env_config()
            model_config = self._build_model_config()

            self.parallel_manager = ParallelManager(
                env_config=env_config,
                model_config=model_config,
                parallel_config=self.config.parallel.dict(),
                device=self.config.env.device,
            )

    def set_initial_episode_state(self, initial_episode_state: "EpisodeState"):
        """Sets the initial episode state, typically provided by the Trainer."""
        self.episode_state = initial_episode_state

    def run(self):
        """
        Executes the main training loop.
        This method contains the core iteration logic.
        """
        log_both = self.trainer.log_both
        if not log_both:
            # This should be set by Trainer.run_training_loop before calling this
            raise RuntimeError(
                "Trainer's log_both callback is not set before running TrainingLoopManager."
            )
        if self.episode_state is None:
            raise RuntimeError(
                "Initial episode state not set in TrainingLoopManager before run."
            )

        self.last_time_for_sps = time.time()
        self.steps_since_last_time_for_sps = 0
        self.last_display_update_time = time.time()

        # Start parallel workers if parallel training is enabled
        if self.parallel_manager and self.config.parallel.enabled:
            if self.trainer.agent and self.trainer.agent.model:
                log_both(f"Starting {self.config.parallel.num_workers} parallel workers...")
                if self.parallel_manager.start_workers(self.trainer.agent.model):
                    log_both("Parallel workers started successfully")
                else:
                    log_both("Failed to start parallel workers, falling back to sequential training")
                    self.parallel_manager = None
            else:
                log_both("Cannot start parallel workers: model not available")
                self.parallel_manager = None

        try:
            while self.trainer.global_timestep < self.config.training.total_timesteps:
                self.current_epoch += 1

                self._run_epoch(log_both)

                if self.trainer.global_timestep >= self.config.training.total_timesteps:
                    log_both(
                        f"Target timesteps ({self.config.training.total_timesteps}) reached during epoch {self.current_epoch}."
                    )
                    break

                if self.episode_state and self.episode_state.current_obs is not None:
                    self.trainer.perform_ppo_update(
                        self.episode_state.current_obs, log_both
                    )
                else:
                    log_both(
                        "[WARNING] Skipping PPO update due to missing current_obs in episode_state. "
                        f"(Timestep: {self.trainer.global_timestep})",
                        also_to_wandb=True,
                    )

                # Execute step callbacks using centralized callback manager with error handling
                self.trainer.callback_manager.execute_step_callbacks(self.trainer)

        except KeyboardInterrupt:
            log_both(
                "Training interrupted by user (KeyboardInterrupt in TrainingLoopManager).",
                also_to_wandb=True,
            )
            raise
        except (RuntimeError, ValueError, AttributeError) as e:
            log_message = f"Training error in TrainingLoopManager.run: {e}"
            if hasattr(self.trainer, "logger") and self.trainer.logger:
                self.trainer.logger.log(log_message)
            else:
                log_info_to_stderr("TrainingLoopManager", log_message)
            log_both(f"Training error in training loop: {e}", also_to_wandb=True)
            raise

    def _run_epoch(self, log_both):
        """
        Runs a single epoch, collecting experiences until the buffer is full or total timesteps are met.
        Uses parallel collection if enabled, otherwise falls back to sequential collection.
        """
        num_steps_collected_this_epoch = 0

        # Check if parallel training is enabled
        if self.parallel_manager and self.config.parallel.enabled:
            # Parallel experience collection
            num_steps_collected_this_epoch = self._run_epoch_parallel(log_both)
        else:
            # Sequential experience collection (existing logic)
            num_steps_collected_this_epoch = self._run_epoch_sequential(log_both)

        # Update metrics regardless of collection mode
        self.trainer.metrics_manager.pending_progress_updates.setdefault(
            "steps_collected_this_epoch", num_steps_collected_this_epoch
        )

    def _run_epoch_parallel(self, log_both):
        """
        Parallel experience collection using worker processes.
        """
        if not self.parallel_manager:
            log_both("ParallelManager not available, falling back to sequential mode.")
            return self._run_epoch_sequential(log_both)

        num_steps_collected = 0
        collection_attempts = 0
        max_collection_attempts = 50  # Prevent infinite loops

        log_both(
            f"Starting parallel experience collection for epoch {self.current_epoch}"
        )

        while (
            num_steps_collected < self.config.training.steps_per_epoch
            and self.trainer.global_timestep < self.config.training.total_timesteps
            and collection_attempts < max_collection_attempts
        ):

            collection_attempts += 1

            # Synchronize model with workers if needed
            if (
                self.agent
                and self.agent.model
                and self.parallel_manager.sync_model_if_needed(
                    cast(nn.Module, self.agent.model), self.trainer.global_timestep
                )
            ):
                log_both(
                    f"Model synchronized with workers at step {self.trainer.global_timestep}"
                )

            # Collect experiences from workers
            try:
                if self.buffer:
                    experiences_collected = self.parallel_manager.collect_experiences(
                        self.buffer
                    )

                    if experiences_collected > 0:
                        num_steps_collected += experiences_collected
                        self.trainer.metrics_manager.increment_timestep_by(
                            experiences_collected
                        )
                        # Fix B11: Update SPS calculation counter for parallel mode
                        self.steps_since_last_time_for_sps += experiences_collected

                        log_both(
                            f"Collected {experiences_collected} experiences from workers "
                            f"(total this epoch: {num_steps_collected})",
                            also_to_wandb=False,
                        )

                        # Update display periodically
                        if num_steps_collected % 100 == 0:  # Every 100 steps
                            self._update_display_progress(num_steps_collected)
                    else:
                        # No experiences collected this round, brief wait
                        time.sleep(0.01)  # 10ms wait
                else:
                    log_both("Experience buffer not available for parallel collection")
                    break

            except (
                RuntimeError,
                ValueError,
                AttributeError,
                ConnectionError,
                TimeoutError,
            ) as e:
                log_both(
                    f"Error collecting parallel experiences: {e}. "
                    f"Attempt {collection_attempts}/{max_collection_attempts}",
                    also_to_wandb=True,
                )
                if collection_attempts >= max_collection_attempts:
                    log_both(
                        "Max collection attempts reached. Falling back to sequential mode.",
                        also_to_wandb=True,
                    )
                    return self._run_epoch_sequential(log_both)

        log_both(
            f"Parallel epoch {self.current_epoch} completed. "
            f"Collected {num_steps_collected} experiences in {collection_attempts} attempts."
        )

        return num_steps_collected

    def _run_epoch_sequential(self, log_both: "Callable"):
        """
        Sequential experience collection.
        Refactored for clarity and reduced complexity.
        """
        num_steps_collected_this_epoch = 0
        while num_steps_collected_this_epoch < self.config.training.steps_per_epoch:
            should_continue = self._process_step_and_handle_episode(log_both)
            if not should_continue:
                break

            num_steps_collected_this_epoch += 1
            self._handle_display_updates()

        return num_steps_collected_this_epoch

    def _handle_successful_step(
        self,
        episode_state: "EpisodeState",
        step_result: "StepResult",  # Corrected type hint
        log_both: "Callable",  # Corrected type hint
    ) -> "EpisodeState":
        """Handles the logic after a successful step, including episode end."""
        if self.step_manager is None:
            raise RuntimeError(STEP_MANAGER_NOT_AVAILABLE_MSG)

        updated_episode_state = self.step_manager.update_episode_state(
            episode_state, step_result
        )

        if step_result.done:
            current_cumulative_stats = {
                "black_wins": self.trainer.metrics_manager.black_wins,
                "white_wins": self.trainer.metrics_manager.white_wins,
                "draws": self.trainer.metrics_manager.draws,
            }

            new_episode_state_after_end, episode_winner_color = (
                self.step_manager.handle_episode_end(
                    updated_episode_state,
                    step_result,
                    current_cumulative_stats,
                    self.trainer.metrics_manager.total_episodes_completed,
                    log_both,
                )
            )

            if episode_winner_color == "black":
                self.trainer.metrics_manager.black_wins += 1
            elif episode_winner_color == "white":
                self.trainer.metrics_manager.white_wins += 1
            elif episode_winner_color is None:
                self.trainer.metrics_manager.draws += 1

            self.trainer.metrics_manager.total_episodes_completed += 1

            self._log_episode_metrics(updated_episode_state)
            return new_episode_state_after_end
        else:
            return updated_episode_state

    def _process_step_and_handle_episode(self, log_both: "Callable") -> bool:
        """Processes a single step and handles episode completion if necessary.
        Returns True if the loop should continue, False if a critical error occurred or max timesteps reached.
        """
        if self.trainer.global_timestep >= self.config.training.total_timesteps:
            return False  # Stop epoch

        if self.episode_state is None:
            log_both("[ERROR] Episode state is None. Resetting.", also_to_wandb=True)
            if self.step_manager is None:
                raise RuntimeError(STEP_MANAGER_NOT_AVAILABLE_MSG)
            self.episode_state = self.step_manager.reset_episode()
            if self.episode_state is None:
                raise RuntimeError("Failed to reset episode_state.")
            return True  # Continue epoch

        if self.step_manager is None:
            raise RuntimeError(STEP_MANAGER_NOT_AVAILABLE_MSG)

        step_result = self.step_manager.execute_step(
            episode_state=self.episode_state,
            global_timestep=self.trainer.global_timestep,
            logger_func=log_both,
        )

        if not step_result.success:
            log_both(
                f"[WARNING] Step failed at {self.trainer.global_timestep}. Resetting.",
                also_to_wandb=True,
            )
            if self.step_manager is None:
                raise RuntimeError(STEP_MANAGER_NOT_AVAILABLE_MSG)
            self.episode_state = self.step_manager.reset_episode()
            return True  # Continue epoch

        self.episode_state = self._handle_successful_step(
            self.episode_state, step_result, log_both
        )

        self.trainer.metrics_manager.increment_timestep()
        self.steps_since_last_time_for_sps += 1
        return True  # Continue epoch

    def _log_episode_metrics(self, episode_state: "EpisodeState"):
        """Logs metrics at the end of an episode."""
        ep_len = episode_state.episode_length
        ep_rew = episode_state.episode_reward
        ep_metrics_str = f"L:{ep_len} R:{ep_rew:.2f}"

        total_games = (
            self.trainer.metrics_manager.black_wins
            + self.trainer.metrics_manager.white_wins
            + self.trainer.metrics_manager.draws
        )
        bw_rate = (
            self.trainer.metrics_manager.black_wins / total_games
            if total_games > 0
            else 0.0
        )
        ww_rate = (
            self.trainer.metrics_manager.white_wins / total_games
            if total_games > 0
            else 0.0
        )
        d_rate = (
            self.trainer.metrics_manager.draws / total_games if total_games > 0 else 0.0
        )

        self.trainer.metrics_manager.pending_progress_updates.update(
            {
                "ep_metrics": ep_metrics_str,
                "black_wins_cum": self.trainer.metrics_manager.black_wins,
                "white_wins_cum": self.trainer.metrics_manager.white_wins,
                "draws_cum": self.trainer.metrics_manager.draws,
                "black_win_rate": bw_rate,
                "white_win_rate": ww_rate,
                "draw_rate": d_rate,
                "total_episodes": self.trainer.metrics_manager.total_episodes_completed,
            }
        )

    def _update_display_if_needed(self, extra_updates: Optional[dict] = None):
        """
        Shared helper method to update display with current progress.

        Args:
            extra_updates: Optional dictionary of additional updates to include
        """
        current_time = time.time()
        display_update_interval = getattr(
            self.config.training, "rich_display_update_interval_seconds", 0.2
        )

        if (current_time - self.last_display_update_time) > display_update_interval:
            time_delta_sps = current_time - self.last_time_for_sps
            current_speed = (
                self.steps_since_last_time_for_sps / time_delta_sps
                if time_delta_sps > 0
                else 0.0
            )

            self.trainer.metrics_manager.pending_progress_updates.setdefault(
                "current_epoch", self.current_epoch
            )

            # Add any extra updates provided
            if extra_updates:
                for key, value in extra_updates.items():
                    self.trainer.metrics_manager.pending_progress_updates.setdefault(key, value)

            if hasattr(self.display, "update_progress") and callable(
                self.display.update_progress
            ):
                self.display.update_progress(
                    self.trainer,
                    current_speed,
                    self.trainer.metrics_manager.pending_progress_updates,
                )
            self.trainer.metrics_manager.pending_progress_updates.clear()

            self.last_time_for_sps = current_time
            self.steps_since_last_time_for_sps = 0
            self.last_display_update_time = current_time

    def _handle_display_updates(self):
        """Handles periodic display updates based on time and step intervals."""
        if self.trainer.global_timestep % self.config.training.render_every_steps == 0:
            if hasattr(self.display, "update_log_panel") and callable(
                self.display.update_log_panel
            ):
                self.display.update_log_panel(self.trainer)

        self._update_display_if_needed()

    def _build_env_config(self) -> Dict[str, Any]:
        """Build environment configuration dictionary for parallel workers."""
        return {
            "device": "cpu",  # Workers typically use CPU for environment simulation
            "input_channels": self.config.env.input_channels,
            "num_actions_total": self.config.env.num_actions_total,
            "seed": self.config.env.seed,
            "input_features": self.config.training.input_features,
        }

    def _build_model_config(self) -> Dict[str, Any]:
        """Build model configuration dictionary for parallel workers."""
        return {
            "model_type": self.config.training.model_type,
            "tower_depth": self.config.training.tower_depth,
            "tower_width": self.config.training.tower_width,
            "se_ratio": self.config.training.se_ratio,
            "obs_shape": (self.config.env.input_channels, 9, 9),  # Standard Shogi board
            "num_actions": self.config.env.num_actions_total,
        }

    def _update_display_progress(self, num_steps_collected):
        """Update display with current progress during parallel collection."""
        extra_updates = {"parallel_steps_collected": num_steps_collected}
        self._update_display_if_needed(extra_updates)

]]></file>
  <file path="trainer.py"><![CDATA[
"""
trainer.py: Contains the Trainer class for managing the Shogi RL training loop (refactored).
"""

from datetime import datetime
from typing import Any, Callable, Dict, Optional

import torch

import wandb
from keisei.config_schema import AppConfig
from keisei.core.actor_critic_protocol import ActorCriticProtocol
from keisei.core.experience_buffer import ExperienceBuffer
from keisei.core.ppo_agent import PPOAgent
from keisei.evaluation.evaluate import execute_full_evaluation_run
from keisei.utils import TrainingLogger

from .callback_manager import CallbackManager
from .compatibility_mixin import CompatibilityMixin
from .display_manager import DisplayManager
from .env_manager import EnvManager
from .metrics_manager import MetricsManager
from .model_manager import ModelManager
from .session_manager import SessionManager
from .setup_manager import SetupManager
from .step_manager import EpisodeState, StepManager
from .training_loop_manager import TrainingLoopManager


class Trainer(CompatibilityMixin):
    """
    Manages the training process for the PPO Shogi agent.
    This class orchestrates setup, training loop, evaluation, and logging.
    """

    def __init__(self, config: AppConfig, args: Any):
        """
        Initializes the Trainer with configuration and command-line arguments.

        Args:
            config: An AppConfig Pydantic object containing all configuration.
            args: Parsed command-line arguments.
        """
        self.config = config
        self.args = args

        # Core instance attributes
        self.model: Optional[ActorCriticProtocol] = None
        self.agent: Optional[PPOAgent] = None
        self.experience_buffer: Optional[ExperienceBuffer] = None
        self.step_manager: Optional[StepManager] = None
        self.game = None
        self.policy_output_mapper = None
        self.action_space_size = 0
        self.obs_space_shape = ()
        self.log_both: Optional[Callable] = None
        self.execute_full_evaluation_run: Optional[Callable] = None
        self.resumed_from_checkpoint = False

        # Initialize device
        self.device = torch.device(config.env.device)

        # Initialize session manager and setup session infrastructure
        self.session_manager = SessionManager(config, args)
        self.session_manager.setup_directories()
        self.session_manager.setup_wandb()
        self.session_manager.save_effective_config()
        self.session_manager.setup_seeding()

        # Access session properties
        self.run_name = self.session_manager.run_name
        self.run_artifact_dir = self.session_manager.run_artifact_dir
        self.model_dir = self.session_manager.model_dir
        self.log_file_path = self.session_manager.log_file_path
        self.eval_log_file_path = self.session_manager.eval_log_file_path
        self.is_train_wandb_active = self.session_manager.is_wandb_active

        # Initialize managers
        self.display_manager = DisplayManager(config, self.log_file_path)
        self.rich_console = self.display_manager.get_console()
        self.rich_log_messages = self.display_manager.get_log_messages()
        self.logger = TrainingLogger(self.log_file_path, self.rich_console)
        self.model_manager = ModelManager(config, args, self.device, self.logger.log)
        self.env_manager = EnvManager(config, self.logger.log)
        self.metrics_manager = MetricsManager()
        self.callback_manager = CallbackManager(config, self.model_dir)
        self.setup_manager = SetupManager(config, self.device)

        # Setup components using SetupManager
        self._initialize_components()

        # Setup display and callbacks
        self.display = self.display_manager.setup_display(self)
        self.callbacks = self.callback_manager.setup_default_callbacks()

        # Initialize TrainingLoopManager
        self.training_loop_manager = TrainingLoopManager(trainer=self)

    def _initialize_components(self):
        """Initialize all training components using SetupManager."""
        # Setup game components
        (
            self.game,
            self.policy_output_mapper,
            self.action_space_size,
            self.obs_space_shape,
        ) = self.setup_manager.setup_game_components(
            self.env_manager, self.rich_console
        )

        # Setup training components
        self.model, self.agent, self.experience_buffer = (
            self.setup_manager.setup_training_components(self.model_manager)
        )

        # Setup step manager
        self.step_manager = self.setup_manager.setup_step_manager(
            self.game, self.agent, self.policy_output_mapper, self.experience_buffer
        )

        # Handle checkpoint resume
        self.resumed_from_checkpoint = self.setup_manager.handle_checkpoint_resume(
            self.model_manager,
            self.agent,
            self.model_dir,
            self.args.resume,
            self.metrics_manager,
            self.logger,
        )

    def _initialize_game_state(self, log_both) -> EpisodeState:
        """Initialize the game state for training using EnvManager."""
        try:
            self.env_manager.reset_game()
            if not self.step_manager:
                raise RuntimeError("StepManager not initialized")
            
            # Type narrowing: assert that step_manager is not None
            assert self.step_manager is not None, "StepManager should be initialized at this point"
            return self.step_manager.reset_episode()
        except (RuntimeError, ValueError, OSError) as e:
            log_both(
                f"CRITICAL: Error during initial game.reset(): {e}. Aborting.",
                also_to_wandb=True,
            )
            if self.is_train_wandb_active and wandb.run:
                wandb.finish(exit_code=1)
            raise RuntimeError(f"Game initialization error: {e}") from e

    def perform_ppo_update(self, current_obs_np, log_both):
        """Perform a PPO update."""
        if not self.agent or not self.experience_buffer:
            log_both(
                "[ERROR] PPO update called but agent or experience buffer is not initialized.",
                also_to_wandb=True,
            )
            return

        # Type narrowing: assert that agent and experience_buffer are not None
        assert self.agent is not None, "Agent should be initialized at this point"
        assert self.experience_buffer is not None, "Experience buffer should be initialized at this point"

        with torch.no_grad():
            last_value_pred_for_gae = self.agent.get_value(current_obs_np)

        self.experience_buffer.compute_advantages_and_returns(last_value_pred_for_gae)
        learn_metrics = self.agent.learn(self.experience_buffer)
        self.experience_buffer.clear()

        # Format PPO metrics for display using MetricsManager
        ppo_metrics_display = self.metrics_manager.format_ppo_metrics(learn_metrics)
        self.metrics_manager.update_progress_metrics("ppo_metrics", ppo_metrics_display)

        # Format detailed metrics for logging
        ppo_metrics_log = self.metrics_manager.format_ppo_metrics_for_logging(
            learn_metrics
        )

        log_both(
            f"PPO Update @ ts {self.metrics_manager.global_timestep+1}. Metrics: {ppo_metrics_log}",
            also_to_wandb=True,
            wandb_data=learn_metrics,
        )

    def _log_run_info(self, log_both):
        """Log run information at the start of training using SetupManager."""
        self.setup_manager.log_run_info(
            self.session_manager,
            self.model_manager,
            self.agent,
            self.metrics_manager,
            log_both,
        )

    def _finalize_training(self, log_both):
        """Finalize training and save final model and checkpoint via ModelManager."""
        log_both(
            f"Training loop finished at timestep {self.metrics_manager.global_timestep}. Total episodes: {self.metrics_manager.total_episodes_completed}.",
            also_to_wandb=True,
        )

        if not self.agent:
            log_both(
                "[ERROR] Finalize training: Agent not initialized. Cannot save model or checkpoint.",
                also_to_wandb=True,
            )
            if (
                self.is_train_wandb_active and wandb.run
            ):  # Ensure WandB is finalized if active
                self.session_manager.finalize_session()
                log_both("Weights & Biases run finished due to error.")
            return

        game_stats = self.metrics_manager.get_final_stats()

        if self.metrics_manager.global_timestep >= self.config.training.total_timesteps:
            log_both(
                "Training successfully completed all timesteps. Saving final model.",
                also_to_wandb=True,
            )
            success, final_model_path = self.model_manager.save_final_model(
                agent=self.agent,
                model_dir=self.model_dir,
                global_timestep=self.metrics_manager.global_timestep,
                total_episodes_completed=self.metrics_manager.total_episodes_completed,
                game_stats=game_stats,
                run_name=self.run_name,
                is_wandb_active=self.is_train_wandb_active,
            )
            if success and final_model_path:
                log_both(
                    f"Final model processing (save & artifact) successful: {final_model_path}",
                    also_to_wandb=True,
                )
            else:
                log_both(
                    f"[ERROR] Failed to save/artifact final model for timestep {self.metrics_manager.global_timestep}.",
                    also_to_wandb=True,
                )

            # WandB finishing is handled by SessionManager or after all save attempts
        else:
            log_both(
                f"[WARNING] Training interrupted at timestep {self.metrics_manager.global_timestep} (before {self.config.training.total_timesteps} total).",
                also_to_wandb=True,
            )

        # Always attempt to save a final checkpoint
        log_both(
            f"Attempting to save final checkpoint at timestep {self.metrics_manager.global_timestep}.",
            also_to_wandb=False,
        )
        ckpt_success, final_ckpt_path = self.model_manager.save_final_checkpoint(
            agent=self.agent,
            model_dir=self.model_dir,
            global_timestep=self.metrics_manager.global_timestep,
            total_episodes_completed=self.metrics_manager.total_episodes_completed,
            game_stats=game_stats,
            run_name=self.run_name,
            is_wandb_active=self.is_train_wandb_active,
        )
        if ckpt_success and final_ckpt_path:
            log_both(
                f"Final checkpoint processing (save & artifact) successful: {final_ckpt_path}",
                also_to_wandb=True,
            )
        elif (
            self.metrics_manager.global_timestep > 0
        ):  # Only log error if a checkpoint was expected
            log_both(
                f"[ERROR] Failed to save/artifact final checkpoint for timestep {self.metrics_manager.global_timestep}.",
                also_to_wandb=True,
            )

        if self.is_train_wandb_active and wandb.run:
            self.session_manager.finalize_session()
            log_both("Weights & Biases run finished.")

        # Finalize display and save console output
        self.display_manager.finalize_display(self.run_name, self.run_artifact_dir)

    def run_training_loop(self):
        """Executes the main training loop by delegating to TrainingLoopManager."""
        self.session_manager.log_session_start()

        with TrainingLogger(
            self.log_file_path,
            rich_console=self.rich_console,
            rich_log_panel=self.rich_log_messages,
        ) as logger:

            def log_both_impl(
                message: str,
                also_to_wandb: bool = False,
                wandb_data: Optional[Dict] = None,
                log_level: str = "info",
            ):
                logger.log(message)
                if self.is_train_wandb_active and also_to_wandb and wandb.run:
                    log_payload = {"train_message": message}
                    if wandb_data:
                        log_payload.update(wandb_data)
                    wandb.log(log_payload, step=self.metrics_manager.global_timestep)

            self.log_both = log_both_impl
            self.execute_full_evaluation_run = execute_full_evaluation_run

            session_start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            self.log_both(
                f"--- SESSION START: {self.run_name} at {session_start_time} ---"
            )

            # Setup run information logging
            self._log_run_info(self.log_both)

            # Log checkpoint resume status
            if self.resumed_from_checkpoint:
                self.log_both(
                    f"Resumed training from checkpoint: {self.resumed_from_checkpoint}"
                )

            initial_episode_state = self._initialize_game_state(self.log_both)
            self.training_loop_manager.set_initial_episode_state(initial_episode_state)

            # Start the Rich Live display as a context manager
            with self.display.start():
                try:
                    self.training_loop_manager.run()
                except KeyboardInterrupt:
                    self.log_both(
                        "Trainer caught KeyboardInterrupt from TrainingLoopManager. Finalizing.",
                        also_to_wandb=True,
                    )
                except (RuntimeError, ValueError, AttributeError, ImportError) as e:
                    self.log_both(
                        f"Trainer caught unhandled exception from TrainingLoopManager: {e}. Finalizing.",
                        also_to_wandb=True,
                    )
                finally:
                    self._finalize_training(self.log_both)

    def _handle_checkpoint_resume(self):
        """
        Handle checkpoint resume for backward compatibility.

        This method delegates to SetupManager for consistency with the refactored architecture.
        """
        if not self.agent:
            raise RuntimeError("Agent not initialized before _handle_checkpoint_resume")

        # Delegate to SetupManager
        result = self.setup_manager.handle_checkpoint_resume(
            self.model_manager,
            self.agent,
            self.model_dir,
            self.args.resume,
            self.metrics_manager,
            self.logger,
        )

        # Update trainer's state to match the result
        self.resumed_from_checkpoint = result

        return result

]]></file>
  <file path="metrics_manager.py"><![CDATA[
"""
metrics_manager.py: Manages training statistics, metrics tracking, and formatting.
"""

import json
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple

from keisei.shogi.shogi_core_definitions import Color


@dataclass
class TrainingStats:
    """Container for training statistics."""

    global_timestep: int = 0
    total_episodes_completed: int = 0
    black_wins: int = 0
    white_wins: int = 0
    draws: int = 0


class MetricsManager:
    """
    Manages training statistics, PPO metrics formatting, and progress tracking.

    Responsibilities:
    - Game outcome statistics (wins/losses/draws)
    - PPO metrics processing and formatting
    - Progress update management
    - Rate calculations and reporting
    """

    def __init__(self):
        """Initialize metrics manager with zero statistics."""
        self.stats = TrainingStats()
        self.pending_progress_updates: Dict[str, Any] = {}

    # === Statistics Management ===

    def update_episode_stats(self, winner_color: Optional[Color]) -> Dict[str, float]:
        """
        Update episode statistics based on game outcome.

        Args:
            winner_color: Color of the winner, or None for draw

        Returns:
            Dictionary with updated win rates
        """
        self.stats.total_episodes_completed += 1

        if winner_color == Color.BLACK:
            self.stats.black_wins += 1
        elif winner_color == Color.WHITE:
            self.stats.white_wins += 1
        else:
            self.stats.draws += 1

        return self.get_win_rates_dict()

    def get_win_rates(self) -> Tuple[float, float, float]:
        """
        Calculate win rates as percentages.

        Returns:
            Tuple of (black_rate, white_rate, draw_rate) as percentages
        """
        total = self.stats.total_episodes_completed
        if total == 0:
            return (0.0, 0.0, 0.0)

        black_rate = (self.stats.black_wins / total) * 100
        white_rate = (self.stats.white_wins / total) * 100
        draw_rate = (self.stats.draws / total) * 100

        return (black_rate, white_rate, draw_rate)

    def get_win_rates_dict(self) -> Dict[str, float]:
        """Get win rates as a dictionary for logging."""
        black_rate, white_rate, draw_rate = self.get_win_rates()
        return {
            "win_rate_black": black_rate,
            "win_rate_white": white_rate,
            "win_rate_draw": draw_rate,
        }

    def format_episode_metrics(self, episode_length: int, episode_reward: float) -> str:
        """
        Format episode completion metrics for display.

        Args:
            episode_length: Number of steps in the episode
            episode_reward: Total reward for the episode

        Returns:
            Formatted string with episode metrics
        """
        black_rate, white_rate, draw_rate = self.get_win_rates()

        return (
            f"Ep {self.stats.total_episodes_completed}: "
            f"Len={episode_length}, R={episode_reward:.3f}, "
            f"B={black_rate:.1f}%, W={white_rate:.1f}%, D={draw_rate:.1f}%"
        )

    # === PPO Metrics Management ===

    def format_ppo_metrics(self, learn_metrics: Dict[str, float]) -> str:
        """
        Format PPO learning metrics for display.

        Args:
            learn_metrics: Dictionary of PPO metrics from agent.learn()

        Returns:
            Formatted string with key PPO metrics
        """
        ppo_metrics_parts = []

        if "ppo/learning_rate" in learn_metrics:
            ppo_metrics_parts.append(f"LR:{learn_metrics['ppo/learning_rate']:.2e}")
        if "ppo/kl_divergence_approx" in learn_metrics:
            ppo_metrics_parts.append(
                f"KL:{learn_metrics['ppo/kl_divergence_approx']:.4f}"
            )
        if "ppo/policy_loss" in learn_metrics:
            ppo_metrics_parts.append(f"PolL:{learn_metrics['ppo/policy_loss']:.4f}")
        if "ppo/value_loss" in learn_metrics:
            ppo_metrics_parts.append(f"ValL:{learn_metrics['ppo/value_loss']:.4f}")
        if "ppo/entropy" in learn_metrics:
            ppo_metrics_parts.append(f"Ent:{learn_metrics['ppo/entropy']:.4f}")

        return " ".join(ppo_metrics_parts)

    def format_ppo_metrics_for_logging(self, learn_metrics: Dict[str, float]) -> str:
        """
        Format PPO metrics for detailed logging (JSON format).

        Args:
            learn_metrics: Dictionary of PPO metrics

        Returns:
            JSON-formatted string of metrics
        """
        formatted_metrics = {k: f"{v:.4f}" for k, v in learn_metrics.items()}
        return json.dumps(formatted_metrics)

    # === Progress Updates Management ===

    def update_progress_metrics(self, key: str, value: Any) -> None:
        """
        Store a progress update for later display.

        Args:
            key: Update identifier (e.g., 'ppo_metrics', 'speed')
            value: Update value
        """
        self.pending_progress_updates[key] = value

    def get_progress_updates(self) -> Dict[str, Any]:
        """Get current pending progress updates."""
        return self.pending_progress_updates.copy()

    def clear_progress_updates(self) -> None:
        """Clear pending progress updates after they've been displayed."""
        self.pending_progress_updates.clear()

    # === State Management ===

    def get_final_stats(self) -> Dict[str, int]:
        """
        Get final game statistics for saving with model.

        Returns:
            Dictionary with final statistics
        """
        return {
            "black_wins": self.stats.black_wins,
            "white_wins": self.stats.white_wins,
            "draws": self.stats.draws,
            "total_episodes_completed": self.stats.total_episodes_completed,
            "global_timestep": self.stats.global_timestep,
        }

    def restore_from_checkpoint(self, checkpoint_data: Dict[str, Any]) -> None:
        """
        Restore statistics from checkpoint data.

        Args:
            checkpoint_data: Checkpoint dictionary with saved statistics
        """
        self.stats.global_timestep = checkpoint_data.get("global_timestep", 0)
        self.stats.total_episodes_completed = checkpoint_data.get(
            "total_episodes_completed", 0
        )
        self.stats.black_wins = checkpoint_data.get("black_wins", 0)
        self.stats.white_wins = checkpoint_data.get("white_wins", 0)
        self.stats.draws = checkpoint_data.get("draws", 0)

    def increment_timestep(self) -> None:
        """Increment the global timestep counter."""
        self.stats.global_timestep += 1

    def increment_timestep_by(self, amount: int) -> None:
        """
        Increment the global timestep counter by a specific amount.

        Args:
            amount: Number of timesteps to add
        """
        if amount < 0:
            raise ValueError("Timestep increment amount must be non-negative")
        self.stats.global_timestep += amount

    # === Properties for Backward Compatibility ===

    @property
    def global_timestep(self) -> int:
        """Current global timestep."""
        return self.stats.global_timestep

    @global_timestep.setter
    def global_timestep(self, value: int) -> None:
        """Set global timestep."""
        self.stats.global_timestep = value

    @property
    def total_episodes_completed(self) -> int:
        """Total episodes completed."""
        return self.stats.total_episodes_completed

    @total_episodes_completed.setter
    def total_episodes_completed(self, value: int) -> None:
        """Set total episodes completed."""
        self.stats.total_episodes_completed = value

    @property
    def black_wins(self) -> int:
        """Number of black wins."""
        return self.stats.black_wins

    @black_wins.setter
    def black_wins(self, value: int) -> None:
        """Set black wins."""
        self.stats.black_wins = value

    @property
    def white_wins(self) -> int:
        """Number of white wins."""
        return self.stats.white_wins

    @white_wins.setter
    def white_wins(self, value: int) -> None:
        """Set white wins."""
        self.stats.white_wins = value

    @property
    def draws(self) -> int:
        """Number of draws."""
        return self.stats.draws

    @draws.setter
    def draws(self, value: int) -> None:
        """Set draws."""
        self.stats.draws = value

]]></file>
  <file path="display_manager.py"><![CDATA[
"""
training/display_manager.py: Manages Rich UI display components and logging.
"""

import sys
from typing import Any, List, Optional

from rich.console import Console, Text
from rich.live import Live

from . import display
from keisei.utils.unified_logger import log_error_to_stderr


class DisplayManager:
    """Manages Rich console display, logging, and UI components."""

    def __init__(self, config: Any, log_file_path: str):
        """
        Initialize the DisplayManager.

        Args:
            config: Training configuration
            log_file_path: Path to the log file
        """
        self.config = config
        self.log_file_path = log_file_path

        # Initialize Rich console components
        self.rich_console = Console(file=sys.stderr, record=True)
        self.rich_log_messages: List[Text] = []

        # Display will be initialized when needed
        self.display: Optional[display.TrainingDisplay] = None

    def setup_display(self, trainer) -> display.TrainingDisplay:
        """
        Setup the training display.

        Args:
            trainer: The trainer instance

        Returns:
            The configured TrainingDisplay
        """
        self.display = display.TrainingDisplay(self.config, trainer, self.rich_console)
        return self.display

    def get_console(self) -> Console:
        """
        Get the Rich console instance.

        Returns:
            The Rich console
        """
        return self.rich_console

    def get_log_messages(self) -> List[Text]:
        """
        Get the log messages list.

        Returns:
            List of Rich Text log messages
        """
        return self.rich_log_messages

    def add_log_message(self, message: str) -> None:
        """
        Add a log message to the Rich log panel.

        Args:
            message: The message to add
        """
        rich_message = Text(message)
        self.rich_log_messages.append(rich_message)

    def update_progress(self, trainer, speed: float, pending_updates: dict) -> None:
        """
        Update the progress display.

        Args:
            trainer: The trainer instance
            speed: Training speed in steps/second
            pending_updates: Dictionary of pending progress updates
        """
        if self.display:
            self.display.update_progress(trainer, speed, pending_updates)

    def update_log_panel(self, trainer) -> None:
        """
        Update the log panel display.

        Args:
            trainer: The trainer instance
        """
        if self.display:
            self.display.update_log_panel(trainer)

    def start_live_display(self) -> Optional[Live]:
        """
        Start the Rich Live display context manager.

        Returns:
            Live context manager if display is available, None otherwise
        """
        if self.display:
            return self.display.start()
        return None

    def save_console_output(self, output_dir: str) -> bool:
        """
        Save the Rich console output to HTML.

        Args:
            output_dir: Directory to save the output

        Returns:
            True if successful, False otherwise
        """
        try:
            import os

            console_log_path = os.path.join(output_dir, "full_console_output_rich.html")
            self.rich_console.save_html(console_log_path)
            log_error_to_stderr("DisplayManager", f"Full Rich console output saved to {console_log_path}")
            return True
        except OSError as e:
            log_error_to_stderr("DisplayManager", f"Error saving Rich console log: {e}")
            return False

    def print_rule(self, title: str, style: str = "bold green") -> None:
        """
        Print a rule with title to the console.

        Args:
            title: The title for the rule
            style: Style for the rule
        """
        self.rich_console.rule(f"[{style}]{title}[/{style}]")

    def print_message(self, message: str, style: str = "bold green") -> None:
        """
        Print a styled message to the console.

        Args:
            message: The message to print
            style: Style for the message
        """
        self.rich_console.print(f"[{style}]{message}[/{style}]")

    def finalize_display(self, run_name: str, run_artifact_dir: str) -> None:
        """
        Finalize the display and save console output.

        Args:
            run_name: Name of the training run
            run_artifact_dir: Directory for run artifacts
        """
        # Save console output
        self.save_console_output(run_artifact_dir)

        # Final messages
        self.print_rule("Run Finished")
        self.print_message(f"Run '{run_name}' processing finished.")
        self.print_message(f"Output and logs are in: {run_artifact_dir}")

]]></file>
  <file path="model_manager.py"><![CDATA[
"""
training/model_manager.py: Model lifecycle management for Shogi RL training.

This module handles model-related concerns including:
- Model configuration and factory instantiation
- Mixed precision training setup
- Checkpoint loading and resuming
- Model artifact creation for WandB
- Final model saving and persistence
"""

import os
import shutil
import time
from typing import Any, Dict, List, Optional, Tuple

import torch
from torch.cuda.amp import GradScaler

import wandb
from keisei.config_schema import AppConfig
from keisei.core.actor_critic_protocol import ActorCriticProtocol
from keisei.core.ppo_agent import PPOAgent
from keisei.shogi import features
from keisei.training.models import model_factory

from . import utils


class ModelManager:
    """Manages model lifecycle for training runs."""

    def __init__(
        self, config: AppConfig, args: Any, device: torch.device, logger_func=None
    ):
        """
        Initialize the ModelManager.

        Args:
            config: Application configuration
            args: Command-line arguments
            device: PyTorch device for model operations
            logger_func: Optional logging function for status messages
        """
        self.config = config
        self.args = args
        self.device = device
        self.logger_func = logger_func or (lambda msg: None)

        # Initialize scaler early to satisfy type checker
        self.scaler: Optional[GradScaler] = None

        # Model configuration from args or config
        self.input_features = (
            getattr(args, "input_features", None) or config.training.input_features
        )
        self.model_type = getattr(args, "model", None) or config.training.model_type
        self.tower_depth = (
            getattr(args, "tower_depth", None) or config.training.tower_depth
        )
        self.tower_width = (
            getattr(args, "tower_width", None) or config.training.tower_width
        )
        self.se_ratio = getattr(args, "se_ratio", None) or config.training.se_ratio

        # Initialize feature spec and observation shape
        self._setup_feature_spec()

        # Initialize mixed precision
        self._setup_mixed_precision()

        # Model will be created by explicit call to create_model()
        self.model: Optional[ActorCriticProtocol] = None

        # Track checkpoint resume status
        self.resumed_from_checkpoint: Optional[str] = None
        self.checkpoint_data: Optional[Dict[str, Any]] = None

    def _setup_feature_spec(self):
        """Setup feature specification and observation shape."""
        self.feature_spec = features.FEATURE_SPECS[self.input_features]
        self.obs_shape = (self.feature_spec.num_planes, 9, 9)

    def _setup_mixed_precision(self):
        """Setup mixed precision training if enabled and supported."""
        self.use_mixed_precision = (
            self.config.training.mixed_precision and self.device.type == "cuda"
        )

        if self.use_mixed_precision:
            self.scaler = GradScaler()
            self.logger_func("Mixed precision training enabled (CUDA).")
        elif self.config.training.mixed_precision and self.device.type != "cuda":
            self.logger_func(
                "Mixed precision training requested but CUDA is not available/selected. "
                "Proceeding without mixed precision."
            )
            self.use_mixed_precision = False
            self.scaler = None
        else:
            self.scaler = None

    def create_model(self) -> ActorCriticProtocol:
        """Create the model using the model factory and sets it to self.model.
        Raises RuntimeError if model creation fails.
        """
        # Call model_factory once and assign to a temporary variable
        created_model = model_factory(
            model_type=self.model_type,
            obs_shape=self.obs_shape,
            num_actions=self.config.env.num_actions_total,
            tower_depth=self.tower_depth,
            tower_width=self.tower_width,
            se_ratio=self.se_ratio if self.se_ratio > 0 else None,
        )

        if not created_model:  # Check if model_factory returned a valid model
            raise RuntimeError("Model factory returned None, failed to create model.")

        # Assign to self.model after moving to device
        self.model = created_model.to(self.device)

        if (
            self.model is None
        ):  # Should not happen if .to() raises on error or created_model was None
            raise RuntimeError("Failed to create model or move model to device.")

        return self.model

    # create_agent method removed as Trainer will instantiate the agent

    def handle_checkpoint_resume(
        self,
        agent: PPOAgent,
        model_dir: str,
        resume_path_override: Optional[str] = None,
    ) -> bool:
        """
        Handle resuming from checkpoint if specified or auto-detected.

        Args:
            agent: PPO agent to load checkpoint into
            model_dir: Directory to search for checkpoints

        Returns:
            bool: True if resumed from checkpoint, False otherwise
        """
        actual_resume_path = (
            resume_path_override
            if resume_path_override is not None
            else self.args.resume
        )

        if actual_resume_path == "latest" or actual_resume_path is None:
            return self._handle_latest_checkpoint_resume(agent, model_dir)
        elif actual_resume_path:
            return self._handle_specific_checkpoint_resume(agent, actual_resume_path)
        else:
            self._reset_checkpoint_state()
            return False

    def _handle_latest_checkpoint_resume(self, agent: PPOAgent, model_dir: str) -> bool:
        """Handle resuming from the latest checkpoint."""
        latest_ckpt = self._find_latest_checkpoint(model_dir)

        if latest_ckpt:
            self.checkpoint_data = agent.load_model(latest_ckpt)
            self.resumed_from_checkpoint = latest_ckpt
            self.logger_func(f"Resumed from latest checkpoint: {latest_ckpt}")
            return True
        else:
            self._reset_checkpoint_state()
            self.logger_func("No checkpoint found to resume from (searched latest).")
            return False

    def _handle_specific_checkpoint_resume(
        self, agent: PPOAgent, resume_path: str
    ) -> bool:
        """Handle resuming from a specific checkpoint path."""
        self.logger_func(f"Checking if {resume_path} exists...")
        if os.path.exists(resume_path):
            self.logger_func(f"Path exists! Loading model from {resume_path}")
            self.checkpoint_data = agent.load_model(resume_path)
            self.resumed_from_checkpoint = resume_path
            self.logger_func(f"Resumed from specified checkpoint: {resume_path}")
            return True
        else:
            self.logger_func(f"Specified resume checkpoint not found: {resume_path}")
            self._reset_checkpoint_state()
            return False

    def _find_latest_checkpoint(self, model_dir: str) -> Optional[str]:
        """Find the latest checkpoint in model_dir or parent directory."""
        # Try to find latest checkpoint in the run's model_dir
        latest_ckpt = utils.find_latest_checkpoint(model_dir)

        # If not found, try the parent directory (savedir)
        if not latest_ckpt and model_dir:
            latest_ckpt = self._search_parent_directory(model_dir)

        return latest_ckpt

    def _search_parent_directory(self, model_dir: str) -> Optional[str]:
        """Search for checkpoint in parent directory and copy if found."""
        parent_dir_path = os.path.dirname(model_dir.rstrip(os.sep))
        if parent_dir_path and parent_dir_path != model_dir:
            parent_ckpt = utils.find_latest_checkpoint(parent_dir_path)
            if parent_ckpt:
                # Copy the checkpoint into the run's model_dir for consistency
                dest_ckpt = os.path.join(model_dir, os.path.basename(parent_ckpt))
                shutil.copy2(parent_ckpt, dest_ckpt)
                return dest_ckpt
        return None

    def _reset_checkpoint_state(self) -> None:
        """Reset checkpoint-related state variables."""
        self.resumed_from_checkpoint = None
        self.checkpoint_data = None

    def create_model_artifact(
        self,
        model_path: str,
        artifact_name: str,
        run_name: str,
        is_wandb_active: bool,
        artifact_type: str = "model",
        description: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        aliases: Optional[List[str]] = None,
    ) -> bool:
        """
        Create and upload a W&B artifact for a model checkpoint.

        Args:
            model_path: Path to the model file to upload
            artifact_name: Name for the artifact (without run prefix)
            run_name: Current run name for artifact prefixing
            is_wandb_active: Whether WandB is currently active
            artifact_type: Type of artifact (default: "model")
            description: Optional description for the artifact
            metadata: Optional metadata dict to attach to the artifact
            aliases: Optional list of aliases (e.g., ["latest", "best"])

        Returns:
            bool: True if artifact was created successfully, False otherwise
        """
        if not (is_wandb_active and wandb.run):
            return False

        if not os.path.exists(model_path):
            self.logger_func(
                f"Warning: Model file {model_path} does not exist, skipping artifact creation."
            )
            return False

        try:
            # Create artifact with run name prefix for uniqueness
            full_artifact_name = f"{run_name}-{artifact_name}"
            artifact = wandb.Artifact(
                name=full_artifact_name,
                type=artifact_type,
                description=description or f"Model checkpoint from run {run_name}",
                metadata=metadata or {},
            )

            # Add the model file
            artifact.add_file(model_path)

            # Log the artifact with retry logic for network failures
            self._log_artifact_with_retry(artifact, aliases, model_path)

            aliases_str = f" with aliases {aliases}" if aliases else ""
            self.logger_func(
                f"Model artifact '{full_artifact_name}' created and uploaded{aliases_str}"
            )

            return True

        except KeyboardInterrupt:
            self.logger_func(f"W&B artifact upload interrupted for {model_path}")
            return False
        except (OSError, RuntimeError, TypeError, ValueError) as e:
            self.logger_func(f"Error creating W&B artifact for {model_path}: {e}")
            return False

    def _log_artifact_with_retry(
        self,
        artifact,
        aliases: Optional[List[str]],
        model_path: str,
        max_retries: int = 3,
        backoff_factor: float = 2.0,
    ) -> None:
        """
        Log WandB artifact with retry logic for network failures.

        Args:
            artifact: WandB artifact to log
            aliases: Optional list of aliases for the artifact
            model_path: Path to model file (for error logging)
            max_retries: Maximum number of retry attempts (default: 3)
            backoff_factor: Multiplier for exponential backoff (default: 2.0)

        Raises:
            Exception: Re-raises the last exception if all retries fail
        """
        for attempt in range(max_retries):
            try:
                wandb.log_artifact(artifact, aliases=aliases)
                return  # Success, exit retry loop
            except (ConnectionError, TimeoutError, RuntimeError) as e:
                if attempt == max_retries - 1:
                    # Last attempt failed, re-raise the exception
                    raise e

                # Calculate backoff delay with exponential growth
                delay = backoff_factor ** attempt
                self.logger_func(
                    f"WandB artifact upload attempt {attempt + 1} failed for {model_path}: {e}. "
                    f"Retrying in {delay:.1f} seconds..."
                )
                time.sleep(delay)

    def save_final_model(
        self,
        agent: PPOAgent,
        model_dir: str,
        global_timestep: int,
        total_episodes_completed: int,
        game_stats: Dict[str, int],
        run_name: str,
        is_wandb_active: bool,
    ) -> Tuple[bool, Optional[str]]:
        """
        Save the final trained model and create associated artifacts.

        Args:
            agent: PPO agent to save
            model_dir: Directory to save model in
            global_timestep: Current training timestep
            total_episodes_completed: Total episodes completed
            game_stats: Dictionary with black_wins, white_wins, draws
            run_name: Current run name
            is_wandb_active: Whether WandB is active

        Returns:
            Tuple[bool, Optional[str]]: (success, model_path)
        """
        final_model_path = os.path.join(model_dir, "final_model.pth")

        try:
            agent.save_model(
                final_model_path,
                global_timestep,
                total_episodes_completed,
            )
            self.logger_func(f"Final model saved to {final_model_path}")

            # Create W&B artifact for final model
            final_metadata = {
                "training_timesteps": global_timestep,
                "total_episodes": total_episodes_completed,
                "black_wins": game_stats.get("black_wins", 0),
                "white_wins": game_stats.get("white_wins", 0),
                "draws": game_stats.get("draws", 0),
                "training_completed": True,
                "model_type": getattr(self.config.training, "model_type", "resnet"),
                "feature_set": getattr(self.config.env, "feature_set", "core"),
            }

            self.create_model_artifact(
                model_path=final_model_path,
                artifact_name="final-model",
                run_name=run_name,
                is_wandb_active=is_wandb_active,
                description=f"Final trained model after {global_timestep} timesteps",
                metadata=final_metadata,
                aliases=["latest", "final"],
            )

            return True, final_model_path

        except (OSError, RuntimeError) as e:
            self.logger_func(f"Error saving final model {final_model_path}: {e}")
            return False, None

    def save_checkpoint(
        self,
        agent: PPOAgent,
        model_dir: str,
        timestep: int,
        episode_count: int,
        stats: Dict[str, Any],
        run_name: str,
        is_wandb_active: bool,
        checkpoint_name_prefix: str = "checkpoint_ts",
    ) -> Tuple[bool, Optional[str]]:
        """
        Save a model checkpoint periodically.

        Args:
            agent: PPO agent to save.
            model_dir: Directory to save checkpoint in.
            timestep: Current training timestep.
            episode_count: Total episodes completed.
            stats: Dictionary with game statistics (e.g., black_wins, white_wins, draws).
            run_name: Current run name for artifact naming.
            is_wandb_active: Whether WandB is active.
            checkpoint_name_prefix: Prefix for the checkpoint filename.

        Returns:
            Tuple[bool, Optional[str]]: (success, checkpoint_path)
        """
        if timestep <= 0:
            self.logger_func("Skipping checkpoint save for timestep <= 0.")
            return False, None

        checkpoint_filename = os.path.join(
            model_dir, f"{checkpoint_name_prefix}{timestep}.pth"
        )

        # Don't save if checkpoint already exists (e.g. if called multiple times for same step)
        if os.path.exists(checkpoint_filename):
            self.logger_func(
                f"Checkpoint {checkpoint_filename} already exists. Skipping save."
            )
            return True, checkpoint_filename

        try:
            os.makedirs(model_dir, exist_ok=True)  # Ensure model_dir exists
            agent.save_model(
                checkpoint_filename,
                timestep,
                episode_count,
                stats_to_save=stats,
            )
            self.logger_func(f"Checkpoint saved to {checkpoint_filename}")

            # Create W&B artifact for this checkpoint
            checkpoint_metadata = {
                "training_timesteps": timestep,
                "total_episodes": episode_count,
                **stats,  # Merge game stats
                "checkpoint_type": "periodic",
                "model_type": self.model_type,
                "feature_set": self.input_features,  # Assuming input_features is the feature_set
            }

            artifact_created = self.create_model_artifact(
                model_path=checkpoint_filename,
                artifact_name=f"checkpoint-ts{timestep}",
                run_name=run_name,
                is_wandb_active=is_wandb_active,
                description=f"Periodic checkpoint at timestep {timestep}",
                metadata=checkpoint_metadata,
                aliases=[f"ts-{timestep}"],  # Add a timestep specific alias
            )
            if not artifact_created and is_wandb_active:
                self.logger_func(
                    f"Warning: Failed to create WandB artifact for {checkpoint_filename}"
                )

            return True, checkpoint_filename

        except (OSError, RuntimeError) as e:
            self.logger_func(f"Error saving checkpoint {checkpoint_filename}: {e}")
            return False, None

    def save_final_checkpoint(
        self,
        agent: PPOAgent,
        model_dir: str,
        global_timestep: int,
        total_episodes_completed: int,
        game_stats: Dict[str, int],
        run_name: str,
        is_wandb_active: bool,
    ) -> Tuple[bool, Optional[str]]:
        """
        Save a final checkpoint with game statistics.

        Args:
            agent: PPO agent to save
            model_dir: Directory to save checkpoint in
            global_timestep: Current training timestep
            total_episodes_completed: Total episodes completed
            game_stats: Dictionary with black_wins, white_wins, draws
            run_name: Current run name
            is_wandb_active: Whether WandB is active

        Returns:
            Tuple[bool, Optional[str]]: (success, checkpoint_path)
        """
        if global_timestep <= 0:
            return False, None

        checkpoint_filename = os.path.join(
            model_dir, f"checkpoint_ts{global_timestep}.pth"
        )

        # Don't save if checkpoint already exists
        if os.path.exists(checkpoint_filename):
            return True, checkpoint_filename

        try:
            agent.save_model(
                checkpoint_filename,
                global_timestep,
                total_episodes_completed,
                stats_to_save=game_stats,
            )
            self.logger_func(f"Final checkpoint saved to {checkpoint_filename}")

            # Create W&B artifact for final checkpoint
            checkpoint_metadata = {
                "training_timesteps": global_timestep,
                "total_episodes": total_episodes_completed,
                "black_wins": game_stats.get("black_wins", 0),
                "white_wins": game_stats.get("white_wins", 0),
                "draws": game_stats.get("draws", 0),
                "checkpoint_type": "final",
                "model_type": getattr(self.config.training, "model_type", "resnet"),
                "feature_set": getattr(self.config.env, "feature_set", "core"),
            }

            self.create_model_artifact(
                model_path=checkpoint_filename,
                artifact_name="final-checkpoint",
                run_name=run_name,
                is_wandb_active=is_wandb_active,
                description=f"Final checkpoint at timestep {global_timestep}",
                metadata=checkpoint_metadata,
                aliases=["latest-checkpoint"],
            )

            return True, checkpoint_filename

        except (OSError, RuntimeError) as e:
            self.logger_func(
                f"Error saving final checkpoint {checkpoint_filename}: {e}"
            )
            return False, None

    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the current model configuration."""
        return {
            "model_type": self.model_type,
            "input_features": self.input_features,
            "tower_depth": self.tower_depth,
            "tower_width": self.tower_width,
            "se_ratio": self.se_ratio,
            "obs_shape": self.obs_shape,
            "num_planes": self.feature_spec.num_planes,
            "use_mixed_precision": self.use_mixed_precision,
            "device": str(self.device),
        }

]]></file>
  <file path="step_manager.py"><![CDATA[
"""
step_manager.py: Manages individual training steps and episode lifecycle.

This module encapsulates the logic for executing single training steps,
handling episode boundaries, and managing the interaction between the agent
and the environment during training.
"""

import time  # Added import
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np  # Added import
import torch  # Added import

from keisei.config_schema import AppConfig
from keisei.core.experience_buffer import ExperienceBuffer
from keisei.core.ppo_agent import PPOAgent
from keisei.shogi import Color, ShogiGame  # Added Color import
from keisei.utils import PolicyOutputMapper, format_move_with_description_enhanced


@dataclass
class EpisodeState:
    """Represents the current state of a training episode."""

    current_obs: np.ndarray
    current_obs_tensor: torch.Tensor
    episode_reward: float
    episode_length: int


@dataclass
class StepResult:
    """Result of executing a single training step."""

    next_obs: np.ndarray
    next_obs_tensor: torch.Tensor
    reward: float
    done: bool
    info: Dict[str, Any]
    selected_move: Optional[Tuple]
    policy_index: int
    log_prob: float
    value_pred: float
    success: bool = True
    error_message: Optional[str] = None


class StepManager:
    """
    Manages individual training step execution and episode lifecycle.

    This class encapsulates the logic for:
    - Executing single training steps
    - Handling episode boundaries and resets
    - Managing demo mode interactions
    - Error handling and recovery during steps
    """

    def __init__(
        self,
        config: AppConfig,
        game: ShogiGame,
        agent: PPOAgent,
        policy_mapper: PolicyOutputMapper,
        experience_buffer: ExperienceBuffer,
    ):
        """
        Initialize the StepManager.

        Args:
            config: Application configuration
            game: Shogi game environment
            agent: PPO agent for action selection
            policy_mapper: Maps between game moves and policy outputs
            experience_buffer: Buffer for storing training experiences
        """
        self.config = config
        self.game = game
        self.agent = agent
        self.policy_mapper = policy_mapper
        self.experience_buffer = experience_buffer
        self.device = torch.device(config.env.device)

    def execute_step(
        self,
        episode_state: EpisodeState,
        global_timestep: int,
        logger_func: Callable[[str, bool, Optional[Dict], str], None],
    ) -> StepResult:
        """
        Execute a single training step.

        Args:
            episode_state: Current state of the episode
            global_timestep: Current global training timestep
            logger_func: Function for logging messages

        Returns:
            StepResult containing the outcome of the step
        """
        try:
            # Get legal moves for current position
            legal_shogi_moves = self.game.get_legal_moves()

            # Check for no legal moves condition (terminal state)
            if not legal_shogi_moves:
                error_msg = (
                    f"No legal moves available at timestep {global_timestep}. "
                    f"Game should be in terminal state (checkmate/stalemate)."
                )
                logger_func(
                    f"TERMINAL: {error_msg} Resetting episode.",
                    True,  # also_to_wandb
                    None,  # wandb_data
                    "info",  # log_level
                )

                # Reset game and return failure result
                reset_obs = self.game.reset()
                reset_tensor = torch.tensor(
                    reset_obs,
                    dtype=torch.float32,
                    device=self.device,
                ).unsqueeze(0)

                return StepResult(
                    next_obs=reset_obs,
                    next_obs_tensor=reset_tensor,
                    reward=0.0,
                    done=True,  # Terminal state
                    info={"terminal_reason": "no_legal_moves"},
                    selected_move=None,
                    policy_index=0,
                    log_prob=0.0,
                    value_pred=0.0,
                    success=False,
                    error_message=error_msg,
                )

            legal_mask_tensor = self.policy_mapper.get_legal_mask(
                legal_shogi_moves, device=self.device
            )

            # Prepare demo mode info if needed
            piece_info_for_demo = None
            if self.config.demo.enable_demo_mode and legal_shogi_moves:
                piece_info_for_demo = self._prepare_demo_info(legal_shogi_moves)

            # Agent action selection
            selected_shogi_move, policy_index, log_prob, value_pred = (
                self.agent.select_action(
                    episode_state.current_obs, legal_mask_tensor, is_training=True
                )
            )

            # Check if agent failed to select a move
            if selected_shogi_move is None:
                error_msg = (
                    f"Agent failed to select a move at timestep {global_timestep}"
                )
                logger_func(
                    f"CRITICAL: {error_msg}. Resetting episode.",
                    True,  # also_to_wandb
                    None,  # wandb_data
                    "error",  # log_level
                )

                # Reset game and return failure result
                reset_obs = self.game.reset()
                reset_tensor = torch.tensor(
                    reset_obs,
                    dtype=torch.float32,
                    device=self.device,
                ).unsqueeze(0)

                return StepResult(
                    next_obs=reset_obs,
                    next_obs_tensor=reset_tensor,
                    reward=0.0,
                    done=False,
                    info={},
                    selected_move=None,
                    policy_index=0,
                    log_prob=0.0,
                    value_pred=0.0,
                    success=False,
                    error_message=error_msg,
                )

            # Handle demo mode logging and delay
            if self.config.demo.enable_demo_mode:
                self._handle_demo_mode(
                    selected_shogi_move,
                    episode_state.episode_length,
                    piece_info_for_demo,
                    logger_func,
                )

            # Execute the move in the environment
            move_result = self.game.make_move(selected_shogi_move)
            if not (isinstance(move_result, tuple) and len(move_result) == 4):
                raise ValueError(f"Invalid move result: {type(move_result)}")

            next_obs_np, reward, done, info = move_result

            # Add experience to buffer
            self.experience_buffer.add(
                episode_state.current_obs_tensor.squeeze(0),
                policy_index,
                reward,
                log_prob,
                value_pred,
                done,
                legal_mask_tensor,
            )

            # Create next observation tensor
            next_obs_tensor = torch.tensor(
                next_obs_np,
                dtype=torch.float32,
                device=self.device,
            ).unsqueeze(0)

            return StepResult(
                next_obs=next_obs_np,
                next_obs_tensor=next_obs_tensor,
                reward=reward,
                done=done,
                info=info,
                selected_move=selected_shogi_move,
                policy_index=policy_index,
                log_prob=log_prob,
                value_pred=value_pred,
                success=True,
            )

        except ValueError as e:
            error_msg = f"Error during training step: {e}"
            logger_func(
                f"CRITICAL: {error_msg}. Resetting episode.",
                True,  # also_to_wandb
                None,  # wandb_data
                "error",  # log_level
            )

            # Reset game and return failure result
            try:
                reset_obs = self.game.reset()
                reset_tensor = torch.tensor(
                    reset_obs,
                    dtype=torch.float32,
                    device=self.device,
                ).unsqueeze(0)

                return StepResult(
                    next_obs=reset_obs,
                    next_obs_tensor=reset_tensor,
                    reward=0.0,
                    done=False,
                    info={},
                    selected_move=None,
                    policy_index=0,
                    log_prob=0.0,
                    value_pred=0.0,
                    success=False,
                    error_message=error_msg,
                )
            except Exception as reset_error:
                # If reset also fails, return a minimal failure result
                return StepResult(
                    next_obs=episode_state.current_obs,
                    next_obs_tensor=episode_state.current_obs_tensor,
                    reward=0.0,
                    done=True,  # Force episode end
                    info={},
                    selected_move=None,
                    policy_index=0,
                    log_prob=0.0,
                    value_pred=0.0,
                    success=False,
                    error_message=f"{error_msg}; Reset also failed: {reset_error}",
                )

    def handle_episode_end(
        self,
        episode_state: EpisodeState,
        step_result: StepResult,
        game_stats: Dict[str, int],  # Read-only, not modified
        total_episodes_completed: int,
        logger_func: Callable[..., None],
    ) -> Tuple[EpisodeState, Optional[str]]:  # Return tuple
        """
        Handle the end of an episode, prepare for the next one, and log results.

        Args:
            episode_state: Current episode state
            step_result: Result from the final step of the episode
            game_stats: Running game statistics (e.g., black_wins, white_wins, draws) - READ ONLY (not modified)
            total_episodes_completed: Total episodes completed so far (before this one)
            logger_func: Function for logging messages

        Returns:
            Tuple[New EpisodeState for the next episode, final_winner_color (str or None)]
        """
        final_winner_color, reason_from_info = self._determine_winner_and_reason(
            step_result.info
        )
        game_outcome_message = self._format_game_outcome_message(
            final_winner_color, reason_from_info
        )

        # Fix B2: Don't modify game_stats in place to avoid double counting
        # Create a temporary copy for win rate calculations only
        temp_game_stats = game_stats.copy()
        if final_winner_color == "black":
            temp_game_stats["black_wins"] += 1
        elif final_winner_color == "white":
            temp_game_stats["white_wins"] += 1
        elif final_winner_color is None:  # Draw
            temp_game_stats["draws"] += 1

        # Calculate win rates for logging using the temporary game_stats
        updated_total_games = (
            temp_game_stats["black_wins"] + temp_game_stats["white_wins"] + temp_game_stats["draws"]
        )

        updated_black_win_rate = (
            temp_game_stats["black_wins"] / updated_total_games
            if updated_total_games > 0
            else 0.0
        )
        updated_white_win_rate = (
            temp_game_stats["white_wins"] / updated_total_games
            if updated_total_games > 0
            else 0.0
        )
        updated_draw_rate = (
            temp_game_stats["draws"] / updated_total_games
            if updated_total_games > 0
            else 0.0
        )

        # Log episode completion
        logger_func(
            f"Episode {total_episodes_completed + 1} finished. Length: {episode_state.episode_length}, "
            f"Reward: {episode_state.episode_reward:.2f}. {game_outcome_message}",
            also_to_wandb=True,
            wandb_data={
                "episode_reward": episode_state.episode_reward,
                "episode_length": episode_state.episode_length,
                "game_outcome": final_winner_color,
                "game_reason": reason_from_info,
                "black_wins_total": temp_game_stats["black_wins"],  # Use updated totals for logging
                "white_wins_total": temp_game_stats["white_wins"],  # Use updated totals for logging
                "draws_total": temp_game_stats["draws"],  # Use updated totals for logging
                "black_win_rate": updated_black_win_rate,  # Corrected key
                "white_win_rate": updated_white_win_rate,  # Corrected key
                "draw_rate": updated_draw_rate,  # Corrected key
            },
            log_level="info",
        )

        # Reset game for next episode
        try:
            reset_result = self.game.reset()
            if not isinstance(reset_result, np.ndarray):
                raise RuntimeError("Game reset failed after episode end")

            reset_obs_tensor = torch.tensor(
                reset_result,
                dtype=torch.float32,
                device=self.device,
            ).unsqueeze(0)

            new_episode_state = EpisodeState(
                current_obs=reset_result,
                current_obs_tensor=reset_obs_tensor,
                episode_reward=0.0,
                episode_length=0,
            )
            return new_episode_state, final_winner_color

        except (RuntimeError, ValueError, OSError) as e:
            logger_func(
                f"CRITICAL: Game reset failed after episode end: {e}",
                True,
                None,
                "error",
            )
            # Return current state to allow caller to handle the error
            return (
                episode_state,
                final_winner_color,
            )  # Return winner color even on reset failure

    def reset_episode(self) -> EpisodeState:
        """
        Reset the game and create a new episode state.

        Returns:
            New EpisodeState for a fresh episode
        """
        reset_obs = self.game.reset()
        reset_tensor = torch.tensor(
            reset_obs,
            dtype=torch.float32,
            device=self.device,
        ).unsqueeze(0)

        return EpisodeState(
            current_obs=reset_obs,
            current_obs_tensor=reset_tensor,
            episode_reward=0.0,
            episode_length=0,
        )

    def update_episode_state(
        self, episode_state: EpisodeState, step_result: StepResult
    ) -> EpisodeState:
        """
        Update episode state with the results of a step.

        Args:
            episode_state: Current episode state
            step_result: Result from the executed step

        Returns:
            Updated EpisodeState
        """
        return EpisodeState(
            current_obs=step_result.next_obs,
            current_obs_tensor=step_result.next_obs_tensor,
            episode_reward=episode_state.episode_reward + step_result.reward,
            episode_length=episode_state.episode_length + 1,
        )

    def _prepare_demo_info(self, legal_shogi_moves) -> Optional[Any]:
        """
        Prepare piece information for demo mode display.

        Args:
            legal_shogi_moves: List of legal moves

        Returns:
            Piece information for the first legal move, or None if unavailable
        """
        if not legal_shogi_moves or legal_shogi_moves[0] is None:
            return None

        try:
            sample_move = legal_shogi_moves[0]
            if (
                len(sample_move) == 5
                and sample_move[0] is not None
                and sample_move[1] is not None
            ):
                from_r, from_c = sample_move[0], sample_move[1]
                return self.game.get_piece(from_r, from_c)
        except (AttributeError, IndexError, ValueError):
            pass  # Silently ignore errors in demo mode preparation

        return None

    def _handle_demo_mode(
        self,
        selected_move: Tuple,
        episode_length: int,
        piece_info_for_demo: Optional[Any],
        logger_func: Callable[[str, bool, Optional[Dict], str], None],
    ) -> None:
        """
        Handle demo mode logging and delay.

        Args:
            selected_move: The move selected by the agent
            episode_length: Current episode length
            piece_info_for_demo: Piece information for demo display
            logger_func: Function for logging messages
        """
        # Get current player name
        current_player_name = (
            getattr(
                self.game.current_player,
                "name",
                str(self.game.current_player),
            )
            if hasattr(self.game, "current_player")
            else "Unknown"
        )

        # Format move description
        move_str = format_move_with_description_enhanced(
            selected_move,
            self.policy_mapper,
            piece_info_for_demo,
        )

        # Log the move
        logger_func(
            f"Move {episode_length + 1}: {current_player_name} played {move_str}",
            False,  # also_to_wandb
            None,  # wandb_data
            "info",  # log_level
        )

        # Add delay for easier observation
        demo_delay = self.config.demo.demo_mode_delay
        if demo_delay > 0:
            time.sleep(demo_delay)

    def _determine_winner_and_reason(
        self, step_info: Optional[Dict[str, Any]]
    ) -> Tuple[Optional[str], str]:
        """Helper to determine winner and reason from step_info and game state."""
        winner_from_info = None
        reason_from_info = "Unknown"

        if step_info:
            winner_from_info = step_info.get("winner")
            reason_from_info = step_info.get("reason", "Unknown")

        final_winner_color = winner_from_info

        if reason_from_info == "Tsumi" and winner_from_info is None:
            if hasattr(self.game, "winner") and self.game.winner is not None:
                game_winner_enum = self.game.winner
                if game_winner_enum == Color.BLACK:
                    final_winner_color = "black"
                elif game_winner_enum == Color.WHITE:
                    final_winner_color = "white"

        return final_winner_color, reason_from_info

    def _format_game_outcome_message(self, winner: Optional[str], reason: str) -> str:
        """Helper to format the game outcome message."""
        if winner == "black":
            return f"Black wins by {reason}."
        if winner == "white":
            return f"White wins by {reason}."
        if winner is None:
            return f"Draw by {reason}."
        return f"Game ended: {winner} by {reason}."  # Should ideally not be reached with current logic

]]></file>
  <file path="callbacks.py"><![CDATA[
"""
training/callbacks.py: Periodic task callbacks for the Shogi RL trainer.
"""

import os
from abc import ABC
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .trainer import Trainer


class Callback(ABC):
    def on_step_end(self, trainer: "Trainer"):
        # Base implementation - subclasses override this method
        pass


class CheckpointCallback(Callback):
    def __init__(self, interval: int, model_dir: str):
        self.interval = interval
        self.model_dir = model_dir

    def on_step_end(self, trainer: "Trainer"):
        if (trainer.global_timestep + 1) % self.interval == 0:
            if not trainer.agent:
                if trainer.log_both:
                    trainer.log_both(
                        "[ERROR] CheckpointCallback: Agent not initialized, cannot save checkpoint.",
                        also_to_wandb=True,
                    )
                return

            game_stats = {
                "black_wins": trainer.black_wins,
                "white_wins": trainer.white_wins,
                "draws": trainer.draws,
            }

            # Use the consolidated save_checkpoint method from ModelManager
            success, ckpt_save_path = trainer.model_manager.save_checkpoint(
                agent=trainer.agent,
                model_dir=self.model_dir,  # model_dir is part of CheckpointCallback's state
                timestep=trainer.global_timestep + 1,
                episode_count=trainer.total_episodes_completed,
                stats=game_stats,
                run_name=trainer.run_name,
                is_wandb_active=trainer.is_train_wandb_active,
            )

            if success:
                if trainer.log_both and ckpt_save_path:
                    trainer.log_both(
                        f"Checkpoint saved via ModelManager to {ckpt_save_path}",
                        also_to_wandb=True,
                    )
            else:
                if trainer.log_both:
                    trainer.log_both(
                        f"[ERROR] CheckpointCallback: Failed to save checkpoint via ModelManager for timestep {trainer.global_timestep + 1}.",
                        also_to_wandb=True,
                    )


class EvaluationCallback(Callback):
    def __init__(self, eval_cfg, interval: int):
        self.eval_cfg = eval_cfg
        self.interval = interval

    def on_step_end(self, trainer: "Trainer"):
        if not getattr(self.eval_cfg, "enable_periodic_evaluation", False):
            return
        if (trainer.global_timestep + 1) % self.interval == 0:
            if not trainer.agent:
                if trainer.log_both:
                    trainer.log_both(
                        "[ERROR] EvaluationCallback: Agent not initialized, cannot run evaluation.",
                        also_to_wandb=True,
                    )
                return

            # Type narrowing: assert that agent is not None
            assert trainer.agent is not None, "Agent should be initialized at this point"

            # Ensure the model to be evaluated exists and is accessible
            # The Trainer now owns self.model directly.
            # PPOAgent also has a self.model attribute.
            # For evaluation, we typically want the agent's current model state.
            current_model = trainer.agent.model
            if not current_model:
                if trainer.log_both:
                    trainer.log_both(
                        "[ERROR] EvaluationCallback: Agent's model not found.",
                        also_to_wandb=True,
                    )
                return

            eval_ckpt_path = os.path.join(
                trainer.model_dir, f"eval_checkpoint_ts{trainer.global_timestep+1}.pth"
            )

            # Save the current agent state for evaluation
            trainer.agent.save_model(
                eval_ckpt_path,
                trainer.global_timestep + 1,
                trainer.total_episodes_completed,
            )

            if trainer.log_both is not None:
                trainer.log_both(
                    f"Starting periodic evaluation at timestep {trainer.global_timestep + 1}...",
                    also_to_wandb=True,
                )

            current_model.eval()  # Set the agent's model to eval mode
            if trainer.execute_full_evaluation_run is not None:
                eval_results = trainer.execute_full_evaluation_run(
                    agent_checkpoint_path=eval_ckpt_path,  # Use the saved checkpoint for eval
                    opponent_type=getattr(self.eval_cfg, "opponent_type", "random"),
                    opponent_checkpoint_path=getattr(
                        self.eval_cfg, "opponent_checkpoint_path", None
                    ),
                    num_games=getattr(self.eval_cfg, "num_games", 20),
                    max_moves_per_game=getattr(
                        self.eval_cfg,
                        "max_moves_per_game",
                        trainer.config.env.max_moves_per_game,
                    ),
                    device_str=trainer.config.env.device,
                    log_file_path_eval=getattr(self.eval_cfg, "log_file_path_eval", ""),
                    policy_mapper=trainer.policy_output_mapper,
                    seed=trainer.config.env.seed,
                    wandb_log_eval=getattr(self.eval_cfg, "wandb_log_eval", False),
                    wandb_project_eval=getattr(
                        self.eval_cfg, "wandb_project_eval", None
                    ),
                    wandb_entity_eval=getattr(self.eval_cfg, "wandb_entity_eval", None),
                    wandb_run_name_eval=f"periodic_eval_{trainer.run_name}_ts{trainer.global_timestep+1}",
                    wandb_group=trainer.run_name,
                    wandb_reinit=True,
                    logger_also_stdout=False,
                )
                current_model.train()  # Set model back to train mode
                if trainer.log_both is not None:
                    trainer.log_both(
                        f"Periodic evaluation finished. Results: {eval_results}",
                        also_to_wandb=True,
                        wandb_data=(
                            dict(eval_results)
                            if isinstance(eval_results, dict)
                            else {"eval_summary": str(eval_results)}
                        ),
                    )
            else:  # if execute_full_evaluation_run is None
                current_model.train()  # Ensure model is set back to train mode

]]></file>
  <file path="__init__.py"><![CDATA[
# keisei/training/__init__.py

]]></file>
  <file path="parallel/parallel_manager.py"><![CDATA[
"""
Parallel training manager for coordinating multiple self-play workers.

This module manages the parallel experience collection system, including
worker processes, communication, and model synchronization.
"""

import logging
import time
from typing import Any, Dict, List

import torch
import torch.nn as nn

from keisei.core.experience_buffer import ExperienceBuffer
from keisei.training.parallel.communication import WorkerCommunicator
from keisei.training.parallel.model_sync import ModelSynchronizer
from keisei.training.parallel.self_play_worker import SelfPlayWorker

logger = logging.getLogger(__name__)


class ParallelManager:
    """
    Manages parallel experience collection using multiple worker processes.

    Coordinates worker processes, handles communication, and synchronizes
    model weights between the main training process and workers.
    """

    def __init__(
        self,
        env_config: Dict,
        model_config: Dict,
        parallel_config: Dict,
        device: str = "cuda",
    ):
        """
        Initialize parallel training manager.

        Args:
            env_config: Environment configuration
            model_config: Model configuration
            parallel_config: Parallel training configuration
            device: Device for main process (typically GPU)
        """
        self.env_config = env_config
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.device = torch.device(device)

        # Parallel configuration
        self.num_workers = parallel_config["num_workers"]
        self.batch_size = parallel_config["batch_size"]
        self.enabled = parallel_config["enabled"]

        # Initialize components
        self.communicator = WorkerCommunicator(
            num_workers=self.num_workers,
            max_queue_size=parallel_config["max_queue_size"],
            timeout=parallel_config["timeout_seconds"],
        )

        self.model_sync = ModelSynchronizer(
            sync_interval=parallel_config["sync_interval"],
            compression_enabled=parallel_config["compression_enabled"],
        )

        # Worker processes
        self.workers: List[SelfPlayWorker] = []
        self.worker_stats: Dict[int, Dict] = {}

        # State tracking
        self.total_steps_collected = 0
        self.total_batches_received = 0
        self.last_sync_time = time.time()
        self.is_running = False

        logger.info("ParallelManager initialized with %d workers", self.num_workers)

    def start_workers(self, initial_model: nn.Module) -> bool:
        """
        Start all worker processes.

        Args:
            initial_model: Initial model to distribute to workers

        Returns:
            True if all workers started successfully
        """
        if not self.enabled:
            logger.info("Parallel collection disabled, skipping worker startup")
            return True

        try:
            # Create and start worker processes
            for worker_id in range(self.num_workers):
                worker = SelfPlayWorker(
                    worker_id=worker_id,
                    env_config=self.env_config,
                    model_config=self.model_config,
                    parallel_config=self.parallel_config,
                    experience_queue=self.communicator.experience_queues[worker_id],
                    model_queue=self.communicator.model_queues[worker_id],
                    control_queue=self.communicator.control_queues[worker_id],
                    seed_offset=self.parallel_config["worker_seed_offset"],
                )

                worker.start()
                self.workers.append(worker)

                logger.info("Started worker %d (PID: %d)", worker_id, worker.pid)

            # Send initial model to all workers
            self._sync_model_to_workers(initial_model)

            self.is_running = True
            logger.info("All %d workers started successfully", self.num_workers)
            return True

        except (OSError, RuntimeError, ValueError) as e:
            logger.error("Failed to start workers: %s", str(e))
            self.stop_workers()
            return False

    def collect_experiences(self, experience_buffer: ExperienceBuffer) -> int:
        """
        Collect experiences from all workers and add to main buffer.

        Args:
            experience_buffer: Main experience buffer to fill

        Returns:
            Number of experiences collected
        """
        if not self.enabled or not self.is_running:
            return 0

        experiences_collected = 0

        try:
            # Collect from all workers
            worker_batches = self.communicator.collect_experiences()

            for worker_id, batch_data in worker_batches:
                # batch_data is a dictionary with experiences and metadata
                worker_experiences = batch_data["experiences"]
                experience_buffer.add_from_worker_batch(worker_experiences)

                batch_size = batch_data.get("batch_size", 0)
                experiences_collected += batch_size
                self.total_batches_received += 1

                # Update worker stats
                self.worker_stats[worker_id] = {
                    "steps_collected": batch_data.get("steps_collected", 0),
                    "games_played": batch_data.get("games_played", 0),
                    "last_batch_time": batch_data.get("timestamp", time.time()),
                    "last_batch_size": batch_size,
                }

            self.total_steps_collected += experiences_collected

            if experiences_collected > 0:
                logger.debug(
                    "Collected %d experiences from workers", experiences_collected
                )

            return experiences_collected

        except (RuntimeError, OSError, ValueError) as e:
            logger.error("Failed to collect experiences: %s", str(e))
            return 0

    def sync_model_if_needed(self, model: nn.Module, current_step: int) -> bool:
        """
        Synchronize model with workers if needed.

        Args:
            model: Current model to synchronize
            current_step: Current training step

        Returns:
            True if synchronization occurred
        """
        if not self.enabled or not self.is_running:
            return False

        if self.model_sync.should_sync(current_step):
            return self._sync_model_to_workers(model, current_step)

        return False

    def _sync_model_to_workers(self, model: nn.Module, current_step: int = 0) -> bool:
        """
        Synchronize model weights to all workers.

        Args:
            model: Model to synchronize
            current_step: Current training step

        Returns:
            True if synchronization succeeded
        """
        try:
            # Send model weights to workers
            self.communicator.send_model_weights(
                model.state_dict(),
                compression_enabled=self.parallel_config["compression_enabled"],
            )

            # Mark sync completed
            self.model_sync.mark_sync_completed(current_step)
            self.last_sync_time = time.time()

            logger.debug("Model synchronized to workers at step %d", current_step)
            return True

        except (RuntimeError, OSError, ValueError) as e:
            logger.error("Model synchronization failed: %s", str(e))
            return False

    def stop_workers(self) -> None:
        """Stop all worker processes gracefully."""
        if not self.workers:
            return

        logger.info("Stopping parallel workers...")

        try:
            # Send stop command to all workers
            self.communicator.send_control_command("stop")

            # Wait for workers to finish
            for worker in self.workers:
                if worker.is_alive():
                    worker.join(timeout=5.0)  # 5 second timeout

                    if worker.is_alive():
                        logger.warning("Force terminating worker %d", worker.pid)
                        worker.terminate()
                        worker.join(timeout=2.0)

            # Clean up communication
            self.communicator.cleanup()

            self.workers.clear()
            self.is_running = False

            logger.info("All workers stopped")

        except (OSError, RuntimeError, ValueError) as e:
            logger.error("Error stopping workers: %s", str(e))

    def get_parallel_stats(self) -> Dict[str, Any]:
        """
        Get comprehensive parallel training statistics.

        Returns:
            Dictionary with parallel training stats
        """
        queue_info = self.communicator.get_queue_info() if self.is_running else {}
        sync_stats = self.model_sync.get_sync_stats()

        return {
            "enabled": self.enabled,
            "running": self.is_running,
            "num_workers": self.num_workers,
            "total_steps_collected": self.total_steps_collected,
            "total_batches_received": self.total_batches_received,
            "last_sync_time": self.last_sync_time,
            "worker_stats": self.worker_stats.copy(),
            "queue_info": queue_info,
            "sync_stats": sync_stats,
            "collection_rate": self._calculate_collection_rate(),
        }

    def _calculate_collection_rate(self) -> float:
        """Calculate current experience collection rate."""
        if not self.worker_stats:
            return 0.0

        # Simple rate calculation based on recent activity
        current_time = time.time()
        recent_steps = 0

        for stats in self.worker_stats.values():
            last_batch_time = stats.get("last_batch_time", 0)
            if current_time - last_batch_time < 60:  # Last minute
                recent_steps += stats.get("last_batch_size", 0)

        return recent_steps / 60.0  # Steps per second

    def reset_workers(self) -> None:
        """Reset all worker environments."""
        if self.is_running:
            self.communicator.send_control_command("reset")
            logger.info("Reset command sent to all workers")

    def pause_workers(self, duration: float = 1.0) -> None:
        """
        Pause all workers for specified duration.

        Args:
            duration: Pause duration in seconds
        """
        if self.is_running:
            self.communicator.send_control_command("pause", data={"duration": duration})
            logger.info("Pause command sent to all workers (duration=%fs)", duration)

    def is_healthy(self) -> bool:
        """
        Check if parallel system is healthy.

        Returns:
            True if system appears to be functioning normally
        """
        if not self.enabled:
            return True  # Disabled system is considered healthy

        if not self.is_running:
            return False

        # Check if workers are alive
        alive_workers = sum(1 for worker in self.workers if worker.is_alive())
        if alive_workers < self.num_workers:
            logger.warning("Only %d/%d workers alive", alive_workers, self.num_workers)
            return False

        # Check recent activity
        current_time = time.time()
        recent_activity = any(
            current_time - stats.get("last_batch_time", 0) < 30
            for stats in self.worker_stats.values()
        )

        return recent_activity

    def __enter__(self):
        """Context manager entry."""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - clean up resources."""
        self.stop_workers()

]]></file>
  <file path="parallel/communication.py"><![CDATA[
"""
Communication utilities for parallel training workers.

Provides queue-based communication between main process and worker processes,
including experience collection and model synchronization.
"""

import gzip
import logging
import multiprocessing as mp
import queue
import time
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import torch

logger = logging.getLogger(__name__)


class WorkerCommunicator:
    """
    Manages communication between main training process and worker processes.

    Uses multiprocessing queues for efficient data exchange:
    - Experience queue: Workers -> Main (collected experiences)
    - Model queue: Main -> Workers (updated model weights)
    - Control queue: Main -> Workers (control commands)
    """

    def __init__(
        self, num_workers: int, max_queue_size: int = 1000, timeout: float = 10.0
    ):
        """
        Initialize communication channels.

        Args:
            num_workers: Number of worker processes
            max_queue_size: Maximum size of communication queues
            timeout: Timeout for queue operations in seconds
        """
        self.num_workers = num_workers
        self.timeout = timeout

        # Create queues for each worker
        self.experience_queues: List[mp.Queue] = [
            mp.Queue(maxsize=max_queue_size) for _ in range(num_workers)
        ]
        self.model_queues: List[mp.Queue] = [
            mp.Queue(maxsize=10)
            for _ in range(num_workers)  # Smaller for model weights
        ]
        self.control_queues: List[mp.Queue] = [
            mp.Queue(maxsize=10) for _ in range(num_workers)  # Control commands
        ]

        # Shared memory for efficient model weight transfer
        self._shared_model_data: Optional[Dict[str, Any]] = None

        logger.info("Initialized worker communication for %d workers", num_workers)

    def send_model_weights(
        self,
        model_state_dict: Dict[str, torch.Tensor],
        compression_enabled: bool = True,
    ) -> None:
        """
        Send updated model weights to all workers.

        Args:
            model_state_dict: PyTorch model state dictionary
            compression_enabled: Whether to compress weights for transmission
        """
        try:
            # Prepare model data for transmission
            model_data = self._prepare_model_data(model_state_dict, compression_enabled)

            # Send to all workers
            for i, model_queue in enumerate(self.model_queues):
                try:
                    model_queue.put(model_data, timeout=self.timeout)
                except queue.Full:
                    logger.warning(
                        "Model queue for worker %d is full, skipping update", i
                    )

        except (RuntimeError, OSError, ValueError) as e:
            logger.error("Failed to send model weights: %s", str(e))

    def collect_experiences(self) -> List[Tuple[int, Dict[str, Any]]]:
        """
        Collect experiences from all workers.

        Returns:
            List of (worker_id, experiences) tuples
        """
        collected_experiences = []

        for worker_id, exp_queue in enumerate(self.experience_queues):
            try:
                # Non-blocking collection
                while True:
                    try:
                        batch_data = exp_queue.get_nowait()
                        collected_experiences.append((worker_id, batch_data))
                    except queue.Empty:
                        break

            except (RuntimeError, OSError, queue.Empty) as e:
                logger.error(
                    "Failed to collect experiences from worker %d: %s",
                    worker_id,
                    str(e),
                )

        return collected_experiences

    def send_control_command(
        self,
        command: str,
        data: Optional[Dict] = None,
        worker_ids: Optional[List[int]] = None,
    ) -> None:
        """
        Send control command to workers.

        Args:
            command: Command string ('stop', 'reset', 'pause', etc.)
            data: Optional command data
            worker_ids: Specific workers to send to (None = all workers)
        """
        command_msg = {"command": command, "data": data, "timestamp": time.time()}

        target_workers = (
            worker_ids if worker_ids is not None else range(self.num_workers)
        )

        for worker_id in target_workers:
            try:
                self.control_queues[worker_id].put(command_msg, timeout=self.timeout)
            except queue.Full:
                logger.warning("Control queue for worker %d is full", worker_id)
            except (RuntimeError, OSError) as e:
                logger.error(
                    "Failed to send control command to worker %d: %s", worker_id, str(e)
                )

    def _prepare_model_data(
        self, state_dict: Dict[str, torch.Tensor], compress: bool
    ) -> Dict[str, Any]:
        """
        Prepare model state dict for transmission with actual compression.

        Args:
            state_dict: PyTorch model state dictionary
            compress: Whether to compress the data

        Returns:
            Prepared model data for queue transmission
        """
        # Convert tensors to numpy arrays for transmission
        model_data = {}
        total_original_size = 0
        total_compressed_size = 0
        
        for key, tensor in state_dict.items():
            np_array = tensor.cpu().numpy()
            
            if compress:
                try:
                    # Apply gzip compression
                    array_bytes = np_array.tobytes()
                    compressed_bytes = gzip.compress(array_bytes, compresslevel=6)
                    
                    total_original_size += len(array_bytes)
                    total_compressed_size += len(compressed_bytes)
                    
                    model_data[key] = {
                        "data": compressed_bytes,
                        "shape": np_array.shape,
                        "dtype": str(np_array.dtype),
                        "compressed": True,
                    }
                except Exception as e:
                    logger.warning("Compression failed for %s, using uncompressed: %s", key, str(e))
                    # Fallback to uncompressed
                    model_data[key] = {
                        "data": np_array,
                        "shape": np_array.shape,
                        "dtype": str(np_array.dtype),
                        "compressed": False,
                    }
                    total_original_size += np_array.nbytes
                    total_compressed_size += np_array.nbytes
            else:
                model_data[key] = {
                    "data": np_array,
                    "shape": np_array.shape,
                    "dtype": str(np_array.dtype),
                    "compressed": False,
                }
                total_original_size += np_array.nbytes
                total_compressed_size += np_array.nbytes

        compression_ratio = total_original_size / total_compressed_size if total_compressed_size > 0 else 1.0

        return {
            "model_data": model_data,
            "timestamp": time.time(),
            "compressed": compress,
            "compression_ratio": compression_ratio,
            "total_original_size": total_original_size,
            "total_compressed_size": total_compressed_size,
        }

    def get_queue_info(self) -> Dict[str, List[int]]:
        """
        Get current queue sizes for monitoring.

        Returns:
            Dictionary with queue sizes for each worker
        """
        return {
            "experience_queue_sizes": [q.qsize() for q in self.experience_queues],
            "model_queue_sizes": [q.qsize() for q in self.model_queues],
            "control_queue_sizes": [q.qsize() for q in self.control_queues],
        }

    def cleanup(self) -> None:
        """Clean up communication resources."""
        logger.info("Cleaning up worker communication resources")

        # Send stop command to all workers
        self.send_control_command("stop")

        # Clear all queues
        for queues in [self.experience_queues, self.model_queues, self.control_queues]:
            for q in queues:
                try:
                    while not q.empty():
                        q.get_nowait()
                except queue.Empty:
                    pass
                q.close()

        logger.info("Worker communication cleanup complete")

]]></file>
  <file path="parallel/model_sync.py"><![CDATA[
"""
Model synchronization utilities for parallel training.

Handles efficient synchronization of model weights between the main training
process and worker processes, including compression and versioning.
"""

import gzip
import logging
import pickle
import time
from typing import Any, Dict

import numpy as np
import torch
import torch.nn as nn

logger = logging.getLogger(__name__)


class ModelSynchronizer:
    """
    Manages synchronization of model weights between main process and workers.

    Provides efficient model weight distribution with optional compression
    and tracks synchronization intervals.
    """

    def __init__(self, sync_interval: int = 100, compression_enabled: bool = True):
        """
        Initialize model synchronizer.

        Args:
            sync_interval: Number of steps between synchronizations
            compression_enabled: Whether to compress model weights for transmission
        """
        self.sync_interval = sync_interval
        self.compression_enabled = compression_enabled
        self.last_sync_step = 0
        self.sync_count = 0

        logger.info(
            "Model synchronizer initialized (interval=%d, compression=%s)",
            sync_interval,
            compression_enabled,
        )

    def should_sync(self, current_step: int) -> bool:
        """
        Check if model should be synchronized at current step.

        Args:
            current_step: Current training step

        Returns:
            True if synchronization should occur
        """
        return current_step - self.last_sync_step >= self.sync_interval

    def prepare_model_for_sync(self, model: nn.Module) -> Dict[str, Any]:
        """
        Prepare model state dict for efficient transmission.

        Args:
            model: PyTorch model to synchronize

        Returns:
            Prepared model data for transmission
        """
        state_dict = model.state_dict()

        # Convert to CPU and numpy for efficient transmission
        prepared_data = {}
        total_params = 0

        for key, tensor in state_dict.items():
            cpu_tensor = tensor.cpu()
            np_array = cpu_tensor.numpy()
            total_params += np_array.size

            if self.compression_enabled:
                prepared_data[key] = self._compress_array(np_array)
            else:
                prepared_data[key] = {
                    "data": np_array,
                    "shape": np_array.shape,
                    "dtype": str(np_array.dtype),
                }

        sync_metadata = {
            "sync_count": self.sync_count,
            "timestamp": time.time(),
            "total_parameters": total_params,
            "compressed": self.compression_enabled,
            "model_keys": list(state_dict.keys()),
        }

        return {"model_data": prepared_data, "metadata": sync_metadata}

    def restore_model_from_sync(
        self, sync_data: Dict[str, Any], model: nn.Module
    ) -> bool:
        """
        Restore model from synchronized data.

        Args:
            sync_data: Data received from synchronization
            model: Model to update

        Returns:
            True if restoration was successful
        """
        try:
            model_data = sync_data["model_data"]
            metadata = sync_data["metadata"]

            # Reconstruct state dict
            state_dict = {}
            for key, data in model_data.items():
                if metadata["compressed"]:
                    state_dict[key] = torch.from_numpy(self._decompress_array(data))
                else:
                    np_array = data["data"]
                    state_dict[key] = torch.from_numpy(np_array)

            # Load into model
            model.load_state_dict(state_dict)

            logger.debug(
                "Model synchronized (sync_count=%d, " "params=%d)",
                metadata["sync_count"],
                metadata["total_parameters"],
            )

            return True

        except (RuntimeError, ValueError, TypeError) as e:
            logger.error("Failed to restore model from sync data: %s", str(e))
            return False

    def mark_sync_completed(self, current_step: int) -> None:
        """
        Mark synchronization as completed for the current step.

        Args:
            current_step: Current training step
        """
        self.last_sync_step = current_step
        self.sync_count += 1
        logger.debug(
            "Model sync completed at step %d (count=%d)", current_step, self.sync_count
        )

    def _compress_array(self, array: np.ndarray) -> Dict[str, Any]:
        """
        Compress numpy array for efficient transmission using gzip compression.

        Args:
            array: Numpy array to compress

        Returns:
            Compressed array data with actual compression
        """
        try:
            # Convert to bytes
            array_bytes = array.tobytes()
            original_size = len(array_bytes)
            
            # Apply gzip compression
            compressed_bytes = gzip.compress(array_bytes, compresslevel=6)
            compressed_size = len(compressed_bytes)
            
            compression_ratio = original_size / compressed_size if compressed_size > 0 else 1.0
            
            return {
                "data": compressed_bytes,
                "shape": array.shape,
                "dtype": str(array.dtype),
                "compressed": True,
                "compression_ratio": compression_ratio,
                "original_size": original_size,
                "compressed_size": compressed_size,
            }
        except Exception as e:
            logger.warning("Compression failed, using uncompressed data: %s", str(e))
            # Fallback to uncompressed
            return {
                "data": array,
                "shape": array.shape,
                "dtype": str(array.dtype),
                "compressed": False,
                "compression_ratio": 1.0,
            }

    def _decompress_array(self, compressed_data: Dict[str, Any]) -> np.ndarray:
        """
        Decompress array data.

        Args:
            compressed_data: Compressed array data

        Returns:
            Decompressed numpy array
        """
        try:
            if compressed_data.get("compressed", False):
                # Decompress gzip data
                compressed_bytes = compressed_data["data"]
                decompressed_bytes = gzip.decompress(compressed_bytes)
                
                # Reconstruct numpy array
                dtype = np.dtype(compressed_data["dtype"])
                shape = compressed_data["shape"]
                array = np.frombuffer(decompressed_bytes, dtype=dtype).reshape(shape)
                return array
            else:
                # Data is not compressed, return as-is
                return compressed_data["data"]
        except Exception as e:
            logger.error("Decompression failed: %s", str(e))
            # Try to return raw data as fallback
            return compressed_data["data"]

    def get_sync_stats(self) -> Dict[str, Any]:
        """
        Get synchronization statistics.

        Returns:
            Dictionary with sync statistics
        """
        return {
            "sync_count": self.sync_count,
            "last_sync_step": self.last_sync_step,
            "sync_interval": self.sync_interval,
            "compression_enabled": self.compression_enabled,
            "average_sync_rate": (
                self.sync_count / max(1, self.last_sync_step)
                if self.last_sync_step > 0
                else 0
            ),
        }

]]></file>
  <file path="parallel/self_play_worker.py"><![CDATA[
"""
Self-play worker process for parallel experience collection.

This module implements worker processes that run self-play games independently
and collect experiences to send back to the main training process.
"""

import logging
import multiprocessing as mp
import queue
import time
from typing import Any, Dict, List, Optional

import numpy as np
import torch

from keisei.core.actor_critic_protocol import ActorCriticProtocol
from keisei.core.experience_buffer import Experience
from keisei.shogi.shogi_game import ShogiGame
from keisei.utils.utils import PolicyOutputMapper

logger = logging.getLogger(__name__)


class SelfPlayWorker(mp.Process):
    """
    Worker process for parallel self-play experience collection.

    Each worker runs an independent environment and model to collect experiences,
    which are then sent back to the main training process via queues.
    """

    def __init__(
        self,
        worker_id: int,
        env_config: Dict[str, Any],
        model_config: Dict[str, Any],
        parallel_config: Dict[str, Any],
        experience_queue: mp.Queue,
        model_queue: mp.Queue,
        control_queue: mp.Queue,
        seed_offset: int = 1000,
    ):
        """
        Initialize self-play worker.

        Args:
            worker_id: Unique identifier for this worker
            env_config: Environment configuration
            model_config: Model configuration
            parallel_config: Parallel training configuration
            experience_queue: Queue to send experiences to main process
            model_queue: Queue to receive model updates from main process
            control_queue: Queue to receive control commands
            seed_offset: Offset for random seed to ensure diversity
        """
        super().__init__()
        self.worker_id = worker_id
        self.env_config = env_config
        self.model_config = model_config
        self.parallel_config = parallel_config

        # Communication queues
        self.experience_queue = experience_queue
        self.model_queue = model_queue
        self.control_queue = control_queue

        # Worker state
        self.seed = env_config["seed"] + seed_offset + worker_id
        self.running = True
        self.steps_collected = 0
        self.games_played = 0

        # Will be initialized in run()
        self.game: Optional[ShogiGame] = None
        self.model: Optional[ActorCriticProtocol] = None
        self.policy_mapper: Optional[PolicyOutputMapper] = None
        self.device = torch.device("cpu")  # Workers use CPU

        # Internal state tracking
        self._current_obs: Optional[np.ndarray] = None

        logger.info("Worker %d initialized with seed %d", worker_id, self.seed)

    def run(self) -> None:
        """Main worker loop - runs in the worker process."""
        try:
            self._setup_worker()
            self._worker_loop()
        except (ValueError, RuntimeError, ImportError, OSError) as e:
            logger.error("Worker %d failed: %s", self.worker_id, str(e))
        finally:
            self._cleanup_worker()

    def _setup_worker(self) -> None:
        """Initialize worker environment and model."""
        try:
            # Set random seeds for reproducibility and diversity
            np.random.seed(self.seed)
            torch.manual_seed(self.seed)

            # Initialize environment
            self.game = ShogiGame()

            # Initialize PolicyOutputMapper for proper action space mapping
            self.policy_mapper = PolicyOutputMapper()

            # Initialize a model architecture for the worker using model_factory
            # This will be replaced when the first model update is received
            from keisei.training.models import model_factory

            input_channels = self.model_config.get("input_channels", 46)
            num_actions = self.model_config.get(
                "num_actions", 13527
            )  # Use correct action space size

            self.model = model_factory(
                model_type=self.model_config.get("model_type", "resnet"),
                obs_shape=(input_channels, 9, 9),
                num_actions=num_actions,
                tower_depth=self.model_config.get("tower_depth", 9),
                tower_width=self.model_config.get("tower_width", 256),
                se_ratio=self.model_config.get("se_ratio", 0.25),
            )
            if self.model is None:
                raise RuntimeError("Failed to create model using model_factory")
            self.model.to(self.device)
            self.model.eval()  # Set to evaluation mode for inference

            logger.info("Worker %d setup complete", self.worker_id)
        except (ValueError, RuntimeError, ImportError) as e:
            logger.error("Worker %d setup failed: %s", self.worker_id, str(e))
            raise

    def _worker_loop(self) -> None:
        """Main worker experience collection loop."""
        batch_experiences = []
        batch_size = self.parallel_config["batch_size"]

        while self.running:
            # Check for control commands
            self._check_control_commands()

            # Check for model updates
            self._check_model_updates()

            if not self.running:
                break

            # Skip experience collection if model not ready
            if self.model is None:
                time.sleep(0.1)
                continue

            # Collect one experience
            experience = self._collect_single_experience()
            if experience is not None:
                batch_experiences.append(experience)
                self.steps_collected += 1

                # Send batch when full
                if len(batch_experiences) >= batch_size:
                    self._send_experience_batch(batch_experiences)
                    batch_experiences = []

        # Send any remaining experiences
        if batch_experiences:
            self._send_experience_batch(batch_experiences)

    def _collect_single_experience(self) -> Optional[Experience]:
        """
        Collect a single experience from the environment.

        Returns:
            Experience object or None if collection failed
        """
        try:
            if self.game is None or self.model is None:
                return None

            # Reset game if needed
            if self._current_obs is None:
                self._current_obs = self.game.reset()

            # Get current observation
            obs_tensor = torch.from_numpy(self._current_obs).float().to(self.device)

            # Get model predictions
            with torch.no_grad():
                action_logits, value = self.model.forward(obs_tensor.unsqueeze(0))
                action_probs = torch.softmax(action_logits, dim=-1)

                # Get legal moves and create action mask using PolicyOutputMapper
                legal_moves = self.game.get_legal_moves()
                if self.policy_mapper is not None:
                    legal_mask = self.policy_mapper.get_legal_mask(
                        legal_moves, self.device
                    )
                else:
                    logger.error(
                        "Worker %d: PolicyOutputMapper not initialized", self.worker_id
                    )
                    return None

                # Mask illegal actions
                masked_probs = action_probs.squeeze(0) * legal_mask.float()
                masked_probs = masked_probs / (masked_probs.sum() + 1e-8)

                # Sample action
                action_dist = torch.distributions.Categorical(masked_probs)
                action = action_dist.sample()
                log_prob = action_dist.log_prob(action)

            # Convert action index to move using PolicyOutputMapper
            if self.policy_mapper is not None:
                try:
                    selected_move = self.policy_mapper.policy_index_to_shogi_move(
                        int(action.item())
                    )
                except (IndexError, ValueError) as e:
                    logger.error(
                        "Worker %d invalid action %d: %s",
                        self.worker_id,
                        int(action.item()),
                        str(e),
                    )
                    return None
            else:
                logger.error(
                    "Worker %d: PolicyOutputMapper not initialized", self.worker_id
                )
                return None

            # Take action in environment
            next_obs, reward, done, _ = self.game.make_move(selected_move)

            # Create experience
            experience = Experience(
                obs=obs_tensor.cpu(),  # Move back to CPU for transmission
                action=int(action.item()),
                reward=float(reward),
                log_prob=log_prob.item(),
                value=value.squeeze().item(),
                done=bool(done),
                legal_mask=legal_mask.cpu(),
            )

            # Update current observation
            if done:
                self._current_obs = None  # Force environment reset
                self.games_played += 1
            else:
                # next_obs should be numpy array from make_move
                if isinstance(next_obs, np.ndarray):
                    self._current_obs = next_obs
                else:
                    # Fallback: get fresh observation
                    self._current_obs = self.game.get_observation()

            return experience

        except (ValueError, RuntimeError, TypeError) as e:
            logger.error(
                "Worker %d experience collection failed: %s", self.worker_id, str(e)
            )
            return None

    def _send_experience_batch(self, experiences: List[Experience]) -> None:
        """
        Send batch of experiences to main process.

        Args:
            experiences: List of Experience objects to send
        """
        try:
            # Convert experiences to batched tensor format for efficient IPC
            batched_tensors = self._experiences_to_batch(experiences)

            # Add worker metadata
            batch_message = {
                "worker_id": self.worker_id,
                "experiences": batched_tensors,  # Now sending batched tensors instead of individual objects
                "batch_size": len(experiences),
                "timestamp": time.time(),
                "steps_collected": self.steps_collected,
                "games_played": self.games_played,
            }

            # Send to main process
            timeout = self.parallel_config.get("timeout_seconds", 10.0)
            self.experience_queue.put(batch_message, timeout=timeout)

            logger.debug(
                "Worker %d sent batch of %d experiences as tensors", 
                self.worker_id, len(experiences)
            )

        except queue.Full:
            logger.warning(
                "Worker %d experience queue full, dropping batch", self.worker_id
            )
        except (ValueError, RuntimeError, OSError) as e:
            logger.error(
                "Worker %d failed to send experiences: %s", self.worker_id, str(e)
            )

    def _experiences_to_batch(
        self, experiences: List[Experience]
    ) -> Dict[str, torch.Tensor]:
        """
        Convert list of experiences to batched tensor format.

        Args:
            experiences: List of Experience objects

        Returns:
            Dictionary with batched tensors
        """
        obs_list = [exp.obs for exp in experiences]

        return {
            "obs": torch.stack(obs_list, dim=0),
            "actions": torch.tensor(
                [exp.action for exp in experiences], dtype=torch.int64
            ),
            "rewards": torch.tensor(
                [exp.reward for exp in experiences], dtype=torch.float32
            ),
            "log_probs": torch.tensor(
                [exp.log_prob for exp in experiences], dtype=torch.float32
            ),
            "values": torch.tensor(
                [exp.value for exp in experiences], dtype=torch.float32
            ),
            "dones": torch.tensor([exp.done for exp in experiences], dtype=torch.bool),
            "legal_masks": torch.stack([exp.legal_mask for exp in experiences], dim=0),
        }

    def _check_control_commands(self) -> None:
        """Check for control commands from main process."""
        try:
            while True:
                try:
                    command_msg = self.control_queue.get_nowait()
                    self._handle_control_command(command_msg)
                except queue.Empty:
                    break
        except (ValueError, RuntimeError, OSError) as e:
            logger.error(
                "Worker %d control command check failed: %s", self.worker_id, str(e)
            )

    def _check_model_updates(self) -> None:
        """Check for model updates from main process."""
        try:
            while True:
                try:
                    model_data = self.model_queue.get_nowait()
                    self._update_model(model_data)
                except queue.Empty:
                    break
        except (ValueError, RuntimeError, OSError) as e:
            logger.error(
                "Worker %d model update check failed: %s", self.worker_id, str(e)
            )

    def _handle_control_command(self, command_msg: Dict) -> None:
        """
        Handle control command from main process.

        Args:
            command_msg: Command message dictionary
        """
        command = command_msg.get("command")

        if command == "stop":
            logger.info("Worker %d received stop command", self.worker_id)
            self.running = False
        elif command == "reset":
            logger.info("Worker %d received reset command", self.worker_id)
            if self.game:
                self._current_obs = None  # Force environment reset
        elif command == "pause":
            # Simple pause implementation - could be more sophisticated
            pause_duration = command_msg.get("data", {}).get("duration", 1.0)
            time.sleep(pause_duration)
        else:
            logger.warning(
                "Worker %d received unknown command: %s", self.worker_id, command
            )

    def _update_model(self, model_data: Dict) -> None:
        """
        Update worker model with new weights from main process.

        Args:
            model_data: Model weight data from main process
        """
        try:
            if not self.model:
                logger.warning(
                    "Worker %d received model update but model not initialized",
                    self.worker_id,
                )
                return

            # Extract model weights
            weights = model_data["model_data"]

            # Reconstruct state dict
            state_dict = {}
            for key, data in weights.items():
                if isinstance(data, dict) and "data" in data:
                    state_dict[key] = torch.from_numpy(data["data"])
                else:
                    state_dict[key] = torch.from_numpy(data)

            # Load weights
            self.model.load_state_dict(state_dict)

            logger.debug("Worker %d model updated", self.worker_id)

        except (ValueError, RuntimeError, TypeError) as e:
            logger.error("Worker %d model update failed: %s", self.worker_id, str(e))

    def _cleanup_worker(self) -> None:
        """Clean up worker resources."""
        if self.game:
            # Close game resources if needed
            pass

        logger.info(
            "Worker %d cleanup complete (steps=%d, games=%d)",
            self.worker_id,
            self.steps_collected,
            self.games_played,
        )

    def get_worker_stats(self) -> Dict[str, Any]:
        """
        Get worker statistics.

        Returns:
            Dictionary with worker statistics
        """
        return {
            "worker_id": self.worker_id,
            "steps_collected": self.steps_collected,
            "games_played": self.games_played,
            "running": self.running,
            "seed": self.seed,
        }

]]></file>
  <file path="parallel/__init__.py"><![CDATA[
"""
Parallel training system for Keisei Shogi.

This package provides utilities for parallel experience collection using
multiprocessing workers that run self-play games independently.

Main Components:
- ParallelManager: Coordinates multiple worker processes
- SelfPlayWorker: Individual worker process for self-play games
- WorkerCommunicator: Queue-based communication between processes
- ModelSynchronizer: Efficient model weight synchronization

Example Usage:
    from keisei.training.parallel import ParallelManager
    from keisei.core.neural_network import ActorCritic

    model = ActorCritic(input_dim, hidden_dim, action_dim)
    config = {'num_workers': 4, 'games_per_worker': 10, ...}

    manager = ParallelManager(model, config)
    manager.start_workers()

    # Training loop
    experiences = manager.collect_experiences()
    # ... process experiences and update model ...
    manager.sync_model_to_workers()

    manager.shutdown()
"""

from .communication import WorkerCommunicator
from .model_sync import ModelSynchronizer
from .parallel_manager import ParallelManager
from .self_play_worker import SelfPlayWorker

__all__ = [
    "ParallelManager",
    "SelfPlayWorker",
    "ModelSynchronizer",
    "WorkerCommunicator",
]

__version__ = "1.0.0"

]]></file>
  <file path="models/resnet_tower.py"><![CDATA[
"""
resnet_tower.py: ActorCriticResTower model for Keisei Shogi with SE block support.
"""

from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

from keisei.core.base_actor_critic import BaseActorCriticModel


class SqueezeExcitation(nn.Module):
    def __init__(self, channels: int, se_ratio: float = 0.25):
        super().__init__()
        hidden = max(1, int(channels * se_ratio))
        self.fc1 = nn.Conv2d(channels, hidden, 1)
        self.fc2 = nn.Conv2d(hidden, channels, 1)

    def forward(self, x):
        s = F.adaptive_avg_pool2d(x, 1)
        s = F.relu(self.fc1(s))
        s = torch.sigmoid(self.fc2(s))
        return x * s


class ResidualBlock(nn.Module):
    def __init__(self, channels: int, se_ratio: Optional[float] = None):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.se = SqueezeExcitation(channels, se_ratio) if se_ratio else None

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.se:
            out = self.se(out)  # pylint: disable=not-callable
        out += x
        return F.relu(out)


class ActorCriticResTower(BaseActorCriticModel):
    def __init__(
        self,
        input_channels: int,
        num_actions_total: int,
        tower_depth: int = 9,
        tower_width: int = 256,
        se_ratio: Optional[float] = None,
    ):
        super().__init__()
        self.stem = nn.Conv2d(input_channels, tower_width, 3, padding=1)
        self.bn_stem = nn.BatchNorm2d(tower_width)
        self.res_blocks = nn.Sequential(
            *[ResidualBlock(tower_width, se_ratio) for _ in range(tower_depth)]
        )
        # Slim policy head: 2 planes, then flatten, then linear
        self.policy_head = nn.Sequential(
            nn.Conv2d(tower_width, 2, 1),
            nn.BatchNorm2d(2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(2 * 9 * 9, num_actions_total),
        )
        # Slim value head: 2 planes, then flatten, then linear
        self.value_head = nn.Sequential(
            nn.Conv2d(tower_width, 2, 1),
            nn.BatchNorm2d(2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(2 * 9 * 9, 1),
        )

    def forward(self, x):
        x = F.relu(self.bn_stem(self.stem(x)))
        x = self.res_blocks(x)
        policy = self.policy_head(x)
        value = self.value_head(x).squeeze(-1)
        return policy, value

]]></file>
  <file path="models/__init__.py"><![CDATA[
from keisei.core.actor_critic_protocol import ActorCriticProtocol

from .resnet_tower import ActorCriticResTower


def model_factory(
    model_type, obs_shape, num_actions, tower_depth, tower_width, se_ratio, **kwargs
) -> ActorCriticProtocol:
    if model_type == "resnet":
        return ActorCriticResTower(
            input_channels=obs_shape[0],
            num_actions_total=num_actions,
            tower_depth=tower_depth,
            tower_width=tower_width,
            se_ratio=se_ratio,
            **kwargs,
        )
    # Add dummy/test models for testing
    elif model_type in ["dummy", "testmodel", "resumemodel"]:
        # Use a simple version of ActorCriticResTower or a mock for testing
        # For now, let's use ActorCriticResTower with minimal fixed params
        # Ensure these params are sensible for a minimal test model
        return ActorCriticResTower(
            input_channels=obs_shape[0],  # Should come from feature_spec.num_planes
            num_actions_total=num_actions,
            tower_depth=1,  # Minimal depth
            tower_width=16,  # Minimal width
            se_ratio=None,  # No SE block for simplicity
            **kwargs,
        )
    raise ValueError(f"Unknown model_type: {model_type}")

]]></file>
</codebase>
